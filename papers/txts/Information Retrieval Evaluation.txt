




on Retrieval Evaluation



rmation Concepts,

ieval, and Services

of North Carolina, Chapel Hill

ion Concepts, Retrieval, and Services is edited by Gary Marchionini of

na. The series will publish 50- to 100-page publications on topics

ce and applications of technology to information discovery, production,

The scope will largely follow the purview of premier information and

uch as ASIST, ACM SIGIR, ACM/IEEE JCDL, and ACM CIKM.

t are limited to: data models, indexing theory and algorithms,

tecture, information economics, privacy and identity, scholarly

nd webometrics, personal information management, human

braries, archives and preservation, cultural informatics, information

, relevance feedback, recommendation systems, question answering,

retrieval, text summarization, multimedia retrieval, multilingual

h.

ation

M) Processes in Organizations: Theoretical Foundations and

E. D. Koenig

At the Conﬂuence of Search and Database Technologies

Wilber

m Books to Cyberspace Identities


-Oriented Data System

hien-Yi Hou, Christopher A. Lee, Richard Marciano, Antoine de

eder, Sheau-Yen Chen, Lucas Gilbert, Paul Tooby, Bing Zhu

o, What, Where, When, and Why

van

val

Design and Integration of Information Spaces

onic Book

onary Perspective on Concepts, Models, and Architectures

ico

actions via Web Analytics

Quantitative Web Research for the Social Sciences


ference

ltimedia Information Systems: Creation, Reﬁnement, Use in


ypool

blication may be reproduced, stored in a retrieval system, or transmitted in

mechanical, photocopy, recording, or any other except for brief quotations in

ission of the publisher.

rback

k

01105ICR019

laypool Publishers series

FORMATION CONCEPTS, RETRIEVAL, AND SERVICES

University of North Carolina, Chapel Hill

n Concepts, Retrieval, and Services

7-9468


on Retrieval Evaluation

d Technology

S ON INFORMATION CONCEPTS, RETRIEVAL, AND

cLaypool

n

publishers

&amp;


y

g

g

unity has been extremely fortunate to have such a well-grounded

riod when most of the human language technologies were just

oal of explaining where these evaluation methodologies came from

dapt to the vastly changed environment in the search engine world

scussion of the early evaluation of information retrieval systems,

in the early 1960s, continuing with the Lancaster “user” study for

various test collection investigations by the SMART project and

er is on the how and the why of the various methodologies devel-

he more recent “batch” evaluations, examining the methodologies

on campaigns such as TREC, NTCIR (emphasis on Asian lan-

opean languages), INEX (emphasis on semi-structured data), etc.

w and why, and in particular on the evolving of the older evaluation

ormation access techniques. This includes how the test collection

w the metrics were changed to better reﬂect operational environ-

valuation issues in user studies–the interactive part of information

earch log studies mainly done by the commercial search engines.

tudies, how the high-level issues of experimental design affect the

mation retrieval, Cranﬁeld paradigm, TREC


. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi

rly History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

ests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

RS evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

ystem and early test collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

ve Systems Laboratory at Case Western University . . . . . . . . . . . . 20

the “Ideal” Test Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

k in metrics up to 1992 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

Since 1992 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

luations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

hoc tests (1992-1999) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

g the ad hoc collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

of the ad hoc collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

EC ad hoc metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

etrieval tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

l from “noisy” text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

l of non-English documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

ge corpus, web retrieval, and enterprise searching . . . . . . . . . . . . . . 38

-speciﬁc retrieval tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

the limits of the Cranﬁeld model . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

n campaigns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

n metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

using, building and evaluating test collections . . . . . . . . . . . . . . . . 51

isting collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52


b data collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

on in TREC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62

ractive evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

on using log data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

how to design an experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

in evaluation of information retrieval . . . . . . . . . . . . . . . . . . . . . 79

some future challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107


g

ne person, and I am grateful to all the writers that came before me for

detail and careful documentation of their methodologies and results. I

any discussions with members of the information retrieval community

ated evaluation.The Retrieval Group at NIST (especially Ian Soboroff

ckley patiently listened to me and then told me where I was wrong,

n for getting the details correct in this lecture. Emily Morse of NIST

hapter 3 and assured me that I had properly documented an area that

Stephen Robertson for several critical items. He helped to gather the

Library and got permission for me to scan this for public consumption.

der reports, plus some that Doug Oard was able to borrow from the

me the material I needed. Stephen was also a preliminary reviewer of

y and I am grateful for his helpful comments.

ial thanks to Ellen Voorhees, not only for many good discussions, but

eview of Chapter 2, making sure that I did get it right and that the

were properly recorded.

rateful to the reviewers of the entire lecture, Mark Smucker and John

ny little errors, but highlighted areas that needed more work or better

time they took to read it, to write helpful comments, and to help me

ortant issues.



tion and Early History

ION

can be evaluated for many reasons, such as which search engine to

ial system to buy, or how to improve the user-friendliness of a system

bout any of these types of evaluation, but rather about the measure-

doing in retrieval, and about how to develop testing that will enable

nderstand what is happening inside the system. The emphasis in the

re that the methodologies used for testing actually measure what the

sured, and that any biases in evaluation can be recognized.

ses the early evaluation of information retrieval systems, starting with

ly 1960s, continuing with the Lancaster “user” study for MEDLARS,

st collection investigations by the SMART project and by groups in

s chapter is on the how and the why of the various methodologies

f the methodologies is the measures of effectiveness, i.e., the metrics

here is some discussion of these metrics, readers are generally referred

ns elsewhere. It should be noted that many of the older references for

online. Readers particularly interested in the early history should also

vers the more recent “batch” evaluations, examining the methodolo-

evaluation campaigns such as TREC, NTCIR (emphasis on Asian

on European languages), INEX (emphasis on semi-structured data),

on the how and why, and in particular on the evolving of the older

handle new information access techniques. This includes how the test

diﬁed and how the metrics were changed to better reﬂect operational

o contains some advice on how to build a test collection. Most of the

re also available online.

at evaluation issues in user studies–the interactive part of information

with a short review of evaluation in early user studies, such as those

ediaries, followed by a discussion of the evaluation experiences in the

e ﬁnal section is a look at recent user studies, including the search log

ommercial search engines. Here the goal is to show, via case studies,

xperimental design affect the ﬁnal evaluations.


D TESTS

ed by Cyril Cleverdon) is often cited as a “standard” for informa-

t exactly is this paradigm, where did it come from, and what are

o read Cleverdon’s own account of the Cranﬁeld tests and their

eech for the 1991 SIGIR award [53].)

parate tests conducted by Cleverdon, Librarian of the College of

and his staff. The ﬁrst, Cranﬁeld I running from 1958 to 1962

d to test four manual indexing (classiﬁcation) methods. It is hard

access methods that existed at that time–text was not available

uld only be found by “word-of-mouth” or specialist librarians, who

massive indexes to publications. Examples of these systems still

ubject Index, or the Engineering Index.

rease in the volume of scientiﬁc papers after World War II and

ese indexes to keep current. But these indexes were very expensive

ntention as to which type of indexing system to use. An edito-

on of these various competing indexing (classiﬁcation) systems:

n of all experimental results is essential in rating the efﬁciency of

age old controversies that arose from the conventional concepts of

mechanized searching systems of the future.”

ial as encouragement, and after presenting a paper in Detroit in

a proposal to the National Science Foundation to create such an

ded and work started in April of 1958. The proposal summarized

be considered, including the number and type of documents which

systems, the indexer’s subject knowledge of the documents and

em, and the type of question to be searched. It also proposed to

ch as the time cost to prepare the index and to locate required

of producing the required answer while minimizing the irrelevant

orts to be indexed were from the ﬁeld of aerodynamics (obviously

ng systems tested were an alphabetical subject catalogue, a faceted

al Decimal Classiﬁcation and the Uniterm system of co-ordinate

nant types of manual indexing schemes in vogue. There were 3

experience in the subject matter. Experience using the different

by careful rotation among the various systems throughout the


g was an additional control). It took two years of work to ﬁnish this

ifﬁculties encountered along the way (such as major indexer fatigue).

e of Cranﬁeld 1 was working within a somewhat familiar paradigm,

ions for the second stage (the searching). One previous test, known

had been done in 1953, but never fully documented [76]. In this case

h using one of two indexing methods for 15,000 documents. About 93

earched by each team using their index.The end result was that there

e teams as to the relevance of the documents and each team generated

ir results!

oid this relevance trap, but also wanted to make sure the results would

stimated that as many as 1,600 questions would be needed, with full

ur different indexes.This seemed impossible and therefore he decided

nown-item searching, i.e., ﬁnding the one document (which he called

was guaranteed to be relevant for a given question.

evaluation mirror a true operational setting, in particular that of a

en document in the 18,000 indexed documents. Cleverdon carefully

sking authors of documents in his indexed collection to select some of

me a question that could be satisfactorily answered by that document”.

uestions, which were then subsetted into random batches for various

heck, he submitted the ﬁrst batch of 400 questions to a panel who

d typical user questions (only one question was discarded).

equired using each index to manually search for the documents,record-

ccess (or failure) of the search.The results–the search failed an average

gniﬁcant differences among the indexing systems. All of the failures

error”, which did not signiﬁcantly differ across the indexers.

med inconclusive on the surface, Cleverdon was able to discover the

of the huge amount of data that was examined in the failure analysis.

indexing system used, but rather the actual content descriptors that

Were the descriptors multiple terms or single terms and how many

ore descriptors (exhaustive indexing) lead to better recall (generally),

n (called “relevance” up to 1964). What about weighting the indexing

actic indexing involving a method of showing relationships between

to select content descriptors for indexing, and an increasing interest

erdon to continue his investigations in the Cranﬁeld 2 (1962-1966)


Cleverdon and Jean Aitchinson at Western Reserve University [3]

used in similar testing.The creation of the test questions by the use

ork well, avoiding the need for relevance judging (and its inherent

eas in his design for Cranﬁeld 2, deciding to aim for about 1200

felt that it was critical to ﬁrst build the test collection (documents,

ts), and then do the experiments on the indexing and searching.

ully model his users in building this test collection.The documents

urally search, the questions needed to reﬂect ones they might ask,

ded to mirror the type of judgments researchers would make for

search process.

ocument method to gather questions, but modiﬁed it in order to

for a given question rather than just the one source document.The

962 on the subject of high speed aerodynamics and the theory of

eir authors, along with a listing of up to 10 papers that were cited

tructions were also sent to these authors.

n the form of a search question, which was the reason for the

ading to the paper,and also give not more than three supplemental

ourse of the work, and which were, or might have been, put to an

of the submitted list of papers which had been cited as references,

estions given.The assessment is to be based on the following scale

a complete answer to the question.

egree of relevance, the lack of which either would have made the

would have resulted in a considerable amount of extra work.

useful, either as general background to the work or as suggesting

rtain aspects of the work.

l interest, for example, those that have been included from an

est

ms returned, with an average of 3.5 questions per form. The docu-

ing the 173 source documents with their cited documents (those


ted based on their grammatical correctness and on a minimal number

authors. Five graduate students spent the summer of 1963 making

gments for these 361 questions against all 1400 documents. These

d to the question authors for a ﬁnal decision, based on the ﬁve graded

sly. Judgments were returned for 279 of the questions, although for

21 of them were used in testing (compound questions were removed

collection was built and experimentation could begin. Note that the

amine in more depth the various properties of index methodologies.

indexing systems as in Cranﬁeld 1, Cleverdon and his team wanted to

x variations for each document and then perform manual experiments

case 33 different index types (see Figure 1.1 for the complete set).The

owever, would come from the results of searching using these various

was as follows: each experiment was deﬁned by a series of rules that

possible combinations of variables would be used. The searchers then

using coded cards stored in what was known as the “Beehive”. So

ould involve one speciﬁc type of index and a series of precision device

co-ordination (the Boolean “anding” of terms or concepts during the

ion basis. For “run 1” of that experiment, each question was indexed

ng investigated, all index terms from that question were “anded”, and

riterion were manually retrieved. This resulted in a single score. The

un 2” which had one less index term used, and so on until only one

hing.

phasize the searching; in general only different co-ordination levels

as the indexing, which was done manually using three basic types of

ms,simple concepts,and controlled terms.On top of this basic structure

alled recall devices, such as the use of the (stemmed) word forms, and

ierarchies from a thesaurus. There were also precision devices such as

o-ordination level searching.

rst indexed at the simple concept level, i.e., “terms which in isolation

as retrieval handles were given the necessary context; such terms as

etc.”. These simple concepts were manually assigned weights based

ment: 9/10 for the main general theme of the document, 7/8 for a

5/6 for a minor subsidiary theme. The simple concepts could then be

, with weights assigned to these terms based on the concept weight


I-3

I-2

I-1

I-6

I-8

I-7

I-5

II-11

II-10

III-1

III-2

I-9

IV-3

IV-4

III-3

IV-2

III-4

III-5

III-6

IV-1

II-15

II-9

II-13

II-8

II-12

II-5

II-7

II-3

II-14

II-4

II-6

II-2

II-1

Single terms. 

Word forms

Single terms. 

Synonyms

Single terms. 

Natural Language

Single terms. 

Synonyms, word forms, quasi-synonyms

Single terms. 

Hierarchy second stage

Single terms. 

Hierarchy first stage

Single terms. 

Synonyms.    Quasy-synonyms

Simple concepts.  

Hierarchical and alphabetical selection

Simple concepts. 

Alphabetical second second stage selection

Controlled terms. 

Basic terms

Controlled terms. 

Narrower terms

Single terms.  

Hierarchy third stage

Abstracts.  

Natural language

Abstracts. 

Word forms

Controlled terms. 

Broader terms

Titles. 

Word forms

Controlled terms. 

Related terms

Controlled terms. 

Narrower and broader and terms

Controlled terms. 

Narrower, broader and related terms

Titles. 

Natural language

Simple concepts. 

Complete combination

Simple concepts. 

Alphabetical first stage selection

Simple concepts. 

Complete species and superordinate

Somple concepts. 

Hierarchical selection

Simple concepts. 

Complete species

Simple concepts. 

Selected species and superordinate

Simple concepts. 

Selected coordinate and collateral

Simple concepts. 

Selected species

Simple concepts. 

Complete collateral

Simple concepts. 

Superordinate

Simple concepts. 

Selected coordinate

Simple concepts. 

Synonyms

Simple concepts. 

Natural language

ER OF EFFECTIVENESS BASED ON NORMALISED

ALL FOR 33 CRANFIELD INDEX LANGUAGES 

ERAGE OF NUMBERS)

dexing used in Cranﬁeld 2.


the details of this indexing, including tables showing that there were

he 1400 documents, with an average postings per document of 31.3

ts), 25.2 single terms with weights 7/10 and 12.9 single terms with

els of exhaustivity of indexing. The manual document indexing was

ut as a contrast,and as a way of creating two more levels of exhaustivity,

lus the abstracts were “automatically” indexed. The details of this are

r, it is likely that any of the terms that had been declared single terms

ained in the abstracts/titles were considered to the automatic indexes

ccurrences of the same term. This gave an average of 7 single terms

ms to the abstracts plus titles. Figure 1.1 gives the 33 types of index

mance ranking of the various schemes using types of scoring that will

discussion previously about metrics, centering around the well-known

. Cleverdon decided to use the “Recall Ratio” deﬁned as a/(a + c),

(a + b). These had been used by Perry [128] and called the Recall

tor, respectively. Other names previously used for the recall ratio were

with the precision ratio known as the relevance ratio, the pertinency

see [147] for more on the history of metrics). Cleverdon liked both

cision and the fact that they directly described a user’s experience and

ore complex formulas such as those suggested by Bourne, Farradane,

for each of the 221 questions the recall and precision ratios measured

an experiment. Using the example experiment described earlier, each

d generate a single recall and a precision point, e.g., co-ordinating 5

% precision, 4 terms gives 40% recall at 12% precision, and using only

% precision.These could be plotted on a recall/precision curve looking

with each point representing a single experiment as opposed to one

about how to average these points across the full set of questions.

ually worked with the grand total ﬁgures of the relevant and retrieved

, sum up the total number of relevant retrieved and the total number

l questions and then divide by the number of questions. This today

method and was the simplest to calculate (remember they were not

was aware of the problems with this method; in that, questions with

wed the results, and therefore he did some experimentation with per

cro-averaging).


c

d

c + d

a + c

b + d

a + b + c + d = N

at there was too much to do; several subsets of the collection were

ﬁeld 200 was created using 42 questions on aerodynamics, along

nts (but not the source documents for these questions). This then

eriments done on different subsets could not be directly compared;

s of relevant/non-relevant documents for the 42 questions in the

measure, the ratio of number of relevant documents to the total

tion,was deﬁned as (a + c)/(a + b + c + d) and used,along with

h measured the experiments ability to avoid retrieving non-relevant

this huge set of experiments? Figure 1.1 copied from Figure 8.1T

ness of 33 different “index languages”. The scoring is based on a

hapter 5 in [51] for details) coming from the SMART project and

c (see Section 1.4). The top seven indexing languages used only

esults found using the word forms (stems) of these single terms.

on to these results on the ﬁrst page of his conclusions [51].

g and seemingly inexplicable conclusion that arises from the project

guages are superior to any other type. ....This conclusion is so

hat it is bound to throw considerable doubt on the methods which

ults, and our ﬁrst reaction was to doubt the evidence. A complete

screpancies, and unless one is prepared to say that the whole test

t the results are completely distorted, then there is no other course

sults which seem to offend against every canon on which we were

at furor from the community and arguments over the Cranﬁeld

, 171]. These mostly centered on the use of source documents to

real questions) and on the deﬁnitions of relevancy. Whereas some

ejection of the experimental conclusions, many were reasonable

digm (although the general consensus was that the experimental

antoutcomesfromCranﬁeld2fortheﬁeldofinformationretrieval.

y that using the actual terms in a document, as opposed to any type

r alteration of these terms (such as the simple concepts), resulted

. Whereas the single terms at the top seven ranks were the result


ove from experiments in indexing to more detailed experiments in

outcome was the Cranﬁeld paradigm for evaluation. Today this is

use of a static test collection of documents, questions, and relevance

d recall and precision metrics. But there are two other subtle compo-

tent on.The ﬁrst was the careful modeling of the task being tested by

ection of scientiﬁc documents, his selection of “users” (via the source

d heavily use this collection, and his careful deﬁnition of relevance

r users might judge documents were critical pieces of the Cranﬁeld

ent was his strict separation of the building of the test collection from

s was done to avoid problems in earlier experiments, and one can only

ave been so clear if he had not followed this principle. Both of these

forgotten today, but need to be further examined in light of the various

nﬁeld paradigm.

RS EVALUATION

Cranﬁeld work with the other major study that took place during 1966

Medical Literature Analysis and Retrieval System) evaluation [115].By

approximately 700,000 indexed citations online for medicine by 1966,

being indexed using a huge (7000 categories at that time) controlled

edical Subject Headings, or MeSH). The citations were growing at

he National Library of Medicine offered a search service using search

ons.

Lancaster to do an evaluation of this search service,with the following

ments,to ﬁnd out how well the service was meeting those requirements,

erformance, and to suggest improvements. The evaluation was also

on,with documents,requests/questions,indexing,search formulations

d Cleverdon was an advisor.

e measured were the coverage of MEDLARS, the recall and precision

times, the format of the results, and the amount of effort needed by

these factors depended on many of the variables that Cleverdon had

ch as inter-indexer performance, the necessary level of exhaustivity of

f the indexing language (the MeSH thesaurus). But because this was

peciﬁc interest in the users, such as what were their requirements with

what were the best modes of interaction between the users and search

the effect of the response times?


period,to put in the types of requests that would be representative

me from different types of organizations,and to have close enough

so that interactions could be carefully studied.Twenty-one groups

ademic organizations, ﬁve U.S. government health organizations,

ve clinical organizations such as hospitals, and two U.S. regulatory

icipate and 410 requests were collected between August 1966 and

ual requesters (users) did not know about the testing until their

h time they were asked if they were willing to be part of this test.

ion needs.These requests were then searched in a normal manner

these were test requests) and the citations that were found were

entire documents).

procedure, these users were asked to ﬁll in two types of forms.

” assessment of a subset of the documents that had been returned.

re in this subset, randomly selected from the full set of documents

r. For each document the user was asked ﬁrst if they knew of this

o pick one of three relevance categories: “of major value to me in

“of minor value”,and “of no value”.They were also asked to explain

owed the computation of a precision ratio, in which the number

as divided by the number in the judged subset (not the number

). Because these documents had been picked at random from the

hat this precision ratio could be extrapolated as the precision for

been examined. There was also a novelty ratio, calculated as the

d that had NOT been previously known to the user.

call, and it would have been impossible to judge all of the MED-

The recall was estimated by building what was known as the “recall”

to ﬁll in a second type of form after they submitted their request.

ist all known relevant documents that had been published since

S citation service had started), and to list relevancy (presumably

documents. A quick glance at the full results shown in Appendix

listed well less that 10 known documents, with a likely median of

eport what this number actually is). For about 80% of the searches,

using manual searching (usually by NLM staff) using non-NLM

ditional documents were also submitted to the user for relevance

er citations that had been returned).The recall ratio could then be


ge at about 58% recall and 50% precision.These averages cover a huge

s the requests. Figure 1.2 reproduces page 129 in the Lancaster report

results, broken down by the number of documents in the recall base.

m, due to the huge variation in performance across the requests. (The

aster called the performance guarantees, i.e., what MEDLARS could

guarantee of performance for curve A, 80% for curve B, and 75% for

contains detailed failure analysis for these requests. For each request

nt documents that had been missed by the system (recall failures), and

found by the system but judged non-relevant (the precision failures).

full text of each document, the indexing record for that document,

anual search formulation, and the reasons the user had given for his

ing examination led to detailed conclusions for each request, and his

neralize based on these conclusions.

olved the performance levels, which were lower than expected. MED-

age of 175 citations per search at 58% recall and 50% precision. To

85-90% recall (and 20-25% precision), Lancaster estimated that 500

o be found and examined by the users. He polled a small sample of

f those 8 users were happy with less than the maximum recall; his

RS was therefore to allow users to specify a high recall search rather

l by always getting more citations.

s a test of an operational system,with the goals of ﬁnding the problems

So whereas a test collection was built, and recall/precision was the

mphasis was on understanding the nature of the problems rather than

arameters. His deﬁnition of relevance was more similar to utility, i.e.,

minor value to the user.Lancaster’s method of measuring recall within

ique; essentially a variation of today’s known item retrieval methods.

priate user groups, and getting “natural” requests from the users was

f its results being accepted. And ﬁnally the methods of failure analysis,

to detail in these analyses is a lesson for today’s researchers.

SYSTEM AND EARLY TEST COLLECTIONS

ART system at Harvard University in 1961, moving with it to Cornell

. His initial interests were in the indexing structures used by manual

estions [119] that simply using the words of a document for indexing

e Harvard SMART framework was built to allow insertion of different


1 - 2

3 - 5

6 - 1

11- 20

21÷

129

30

40

A

B

C

50

60

70

80

90

100

PRECISION RATIO

No. of documents in 

recall base

EDLARS test searches.


R”reports to NSF. A discussion of the research is beyond the scope of

ibutions to evaluation, especially in metrics and new test collections,

aga of evaluation in information retrieval.

uation contribution was in metrics, ﬁrst started at Harvard by Joe

nell by Mike Keen (borrowed from the Cranﬁeld project in 1966 and

ed that the Cranﬁeld and MEDLARS experiments were all working

oints, e.g., each experiment in Cranﬁeld or each request in Medlars

sion number. But the SMART experiments produced ranked or semi-

aking it impossible to use the single-point recall/precision metrics.

two sets of metrics called rank recall and log precision and normalized

[102], both of which measured the differences between the actual

nt documents and their “ideal” positions. Keen [107, 146] discussed

for experiments, with the following deﬁnitions.

−

�n

i=1 ri − �n

i=1 i

n(N − n)

= 1 −

�n

i=1 log ri − �n

i=1 log i

log N!

(N−n)!n!

ant documents,

ments in collection,

vant document,

ion for the ith relevant item.

rther methodology for the recall/precision curves since he agreed with

it was important to provide separate recall and precision performances.

lly to recall/precision curves, but with problems as to where to make

the cutoff points. So for example one could plot the actual recall and

at a ﬁxed set of document cutoffs, e.g., 1,2,5,10,15 …documents

“Pseudo-Cranﬁeld” method. The problem with this method is that

s of relevant documents so these individual results did not average

uld pick a ﬁxed recall, e.g., 10%, 20%, … 100% and plot the precision

eries have actual precision measurements at these exact recall points so

pecially in the days of small collections (with small numbers of relevant

ral types of interpolations proposed, including using the precision of

t precision at that recall point, the one with the lowest precision at that

o use the highest precision, the “Semi-Cranﬁeld” method illustrated

This ﬁgure (copied from Figure 17 in [107]) also shows the averages


1.0

0.8

0.6

0.4

0.2

0

0.2

0.4

0.6

0.8

1.0

Curve for Individual Request Q 268 

Cran -1, Thesaurus-2, using Recall 

levels cut-off - “Semi-Cranfield” Method

a)

Ranks of Five Relevant are 1,2,4,6,13

0.5

0.4

0.3

0.2

0.1

0

0.2

0.4

0.6

0.8

1.0

Cran-1, Abstracts, Thesaurus - 3, Macro 

Evaluation of 42 Requests

b)     Average Curve to Compare Cut-offs

nt methods of recall/precision interpolation


with the frequent situation of two relevant documents adjacent in

olated method used today, where the interpolated precision at a given

ecision obtained at any recall point greater than or equal to that recall-

nce differences for the various recall-level methods were slight even in

discussion at Cornell in the mid 1960s about the micro vs. macro

olved in favor of using macro averaging now that a computer could

nally there was work [39, 138, 139] with the generality measure (the

he collection that were relevant) since SMART had three versions of

e was the full collection of 1400 documents (Cran-1),the subcollection

an-2), and a second subcollection of 425 documents built at Cornell.

about $104 in 1965 (about $600 in today’s dollar); very few runs were

nd therefore generality was an issue. As a ﬁnal note on SMART’s

t is important to realize that whereas there was discussion of these

hers, and certainly some disagreements, once these were resolved, the

d without many metrics “ﬁghts” and this has been a blessing for the

ution was the idea of building a framework that allows for “easy” ex-

e at Harvard was updated to one at Cornell [146, 200] that allowed

projects with minimal recoding simply by changing of the parameter

ous today, but the framework and the availability of several test collec-

val experimentation a routine process at a time when experimentation

nology ﬁelds was still a difﬁcult endeavor.

of the SMART group to evaluation was a very large set of new test

ions were built according to the Cranﬁeld paradigm, with each collec-

rigorous speciﬁcations. Usually these speciﬁcations were dictated by a

on.Tables 1.2 and 1.3 lists the collections used by SMART, with all of

ose marked with an asterisk. The dates are approximate, based mostly

n an experiment, and the various counts, etc. are taken from different

discrepancies. The number of queries shown is the number of queries

h the number of relevant being an average per query. The lengths of

es are (mostly) after stopword removal and stemming, and any large

me from counting unique terms vs. counting all terms, not removing

temmers (this was unfortunately seldom documented). What follows

ach collection was built, both for historical purposes and to illustrate

n construction.


ns in 1965. For example he used a “null thesaurus” consisting of

, a manually-constructed thesaurus of 600 concepts in computer

word stems, hierarchical arrangements of those concepts, statisti-

[108, 142, 145]. Note that these experiments were investigating

xperiments, with the major exceptions being that these searches

ce the document analysis was done, and that the returned results

ot single points per experiment. The 17 initial requests (queries)

ilt by three project staff, two of which had extensive knowledge

of how it would actually perform). These three people also made

ing at all the abstracts. The collection was extended using similar

by one non-staff person to become the IRE-3 collection.

n, the ADI collection, was a set of short papers from the 1963

Documentation Institute, keypunched at Harvard. The requests

nical staff not familiar with the system,and once again these people

t the full collection. The reasons for building the ADI collection

ne, but in addition there were titles, abstracts, and full texts, and

h document lengths [106, 146]. Note that unlike the Cranﬁeld

collections made no effort to get “real” user requests; however,

the requests were not derived from the documents and therefore

cuments that had caused Cleverdon so much pain [108].

n of abstracts was also keypunched at Harvard, allowing compar-

he use of a very large collection compared to the two earlier ones.

2 collection (200 documents), with stemmers and using the the-

d. As mentioned before, the cost of the runs allowed few runs with

y in 1969/1970 a “slimmed down” version of Cranﬁeld with 424

uilt by ﬁrst eliminating the source documents, then the documents

y, and ﬁnally creatively ﬁnding ways of reducing the documents to

s as possible [116].

periments with these three collections, Salton moved on to tackle

was the continued strong criticisms of these evaluations based

nd on the unreliability of the relevance judgments. These issues

ments and a new test collection, the ISPRA collection, was built

m [117, 146]. The ISPRA documents were 1268 abstracts in the

science from American Documentation and several other journals

.The queries this time were very carefully built by 8 library science

rson was asked to construct 6 requests that might actually be asked


1398

225

compare indexing methods

200

42

Cranﬁeld subset of 42 questions

424

155

Cornell “reduced” subset

780

34

indexing/dictionary experiments

82

35

doc length experiments

1268

48

multiple relevance judgements

1095/468

48

CLIR English/German

273

18

comparison to Lancaster

450

30

“corrected” Medlars

1033

30

larger medical collection

853

30

speciﬁc medical domain

425

83

full text articles

11429

93

indexing experiments

12684

84

indexing, Boolean

3204

52

additional metadata

1460

76

co-citations

Table 1.3: SMART test collections

h

Quest.length

#relevant

comments

9.2

7.2

#unique terms in abstract

8

4.7

no source docs

8

6.4

ﬁxed # of docs, no source docs

12

17.4

8

4.9

text, abstracts, title lengths

longer

17.8

relevant by author

longer

14.2/13.6

relevant English/German

9.3

4.8/11.1

precision/recall bases

10.1

9.2

10.1

23.2

10?

30

16.0

8.7

7.2

22.4

15.6

33.0

10.8

15.3

28.3

49.8


red relevant only “if it is directly stated in the abstract as printed,

the printed abstract, that the document contains information on

Relevance judgments were made by the original query author and

The average agreement between these two was found to be only

eriments in TREC, it was shown that the performance ranking

s, such as words vs. stems and thesaurus vs. stems did not change

ment set that was used [117, 146].

also used to test cross-language retrieval [137, 146]. The English

he initial ISPRA collection; additionally there were 468 German

ame 48 queries were used, with translations to German done by

evance assessments against the German abstracts were done by a

he need to show that the SMART system, e.g., a ranking system

ms in the documents and queries, was equivalent to the manually-

s [116]. Proving this required showing that the SMART system

ections; this issue drove many of the clustering theses during those

on of the ﬁrst Medlars collection [143, 146]. The collection was

18 of the queries that had been used in that study, along with 273

were not in English or not easily available). The problem came in

caster had measured his recall and precision by using two separate

precision base. Additionally he had computed a single recall and

parison of a ranked list to a single point was one problem, but this

problems and the ﬁnal solutions (see [143, 146]) show both the

oper testing methods possessed by Salton and Keen.

n was built in 1970 [140, 141] in order to do a better comparison.

ster requests were added to the original 18 queries, and documents

nt from all of the 30 requests were checked against the Science

additional citations (per request) that were in MEDLARS.These

RT collection and a medical student reviewed them for relevance.

MEDLARS requests and had a 69% agreement with the original

ent based on the earlier ISPRA study. The third and ﬁnal version

expansion by adding a total of 583 relevant documents from the

ophthalmology collection was built from “scratch” using 30 real

et that was retrieved for these requests. The relevance assessments

al student.The goal of this second medical collection was to allow


build a collection of longer documents that were not in a scientiﬁc

were taken from an old print tape [116] and converted to SMART

ing a useful query set. After several different tries, 83 questions were

ces: the New York Times News of the Week news questions (44), the

fairs Test for Colleges (27), the TIME Current Affair Test (7) and

. These questions all involved news issues in 1963 and were selected

from these sources. Once again, one person made all of the relevance

ll documents).

om this period were imported.The NPL collection came from Britain

ction on the British collections. The INSPEC collection came from

he topics had been built in several versions by information science

e documents were abstracts in electrical engineering from the INSPEC

ollections was heavily used, particularly the Cranﬁeld424, TIME, and

e [144]). There were no more collections built until a very energetic

lt two much more complex collections in 1982 [69].These collections,

e constructed speciﬁcally to test the use of multiple concept types as

bliographic citations/co-citations.

s covered all articles in issues of the CACM from 1958 to 1979. The

abstracts for automatic indexing, but also the authors, the computing

ns, the co-citations, and the links (references to or citations between

information was used in different types of Boolean queries for Fox’s

ection were true user queries, gathered from faculty, staff and students

ience departments.The relevance judgments were performed by these

ected documents that were likely to be relevant (selected in a manner

on done in TREC but using only various SMART runs).

peciﬁcally selected because of the co-citation data involved.The 1460

t of information science documents published between 1969 and 1977

r. Henry Small. The abstracts, titles and author list were used, along

citation vectors to be produced. The queries were the same queries as

41 queries from the ISPRA collection that had relevant documents in

modern queries constructed from the abstracts section of the SIGIR

SMART project did exhaustive relevance judgments.

ns became the basis for information retrieval evaluation for over 20

g science of test collection construction. First there was the insistence

then the fuller understanding of the effects of relevance judging that


t documents to non-relevant documents because of the particular

Additionally the queries have words that are very speciﬁc, making

ments at a high rank and therefore“everything works on Medlars”.

were built correctly for their initial goals; the problem is that later

s for different goals and were unaware of these biases.

ooked at the 7 still existing collections in his series of weighting

d the following conclusions [116].

ngth normalization experiments (strong bias towards long relevant

s (Queries are short, and all terms are equally good)

eriments. Title is simply ﬁrst sentence, so all terms duplicated.

ce feedback or expansion experiments (documents cluster around

rwise good.

eans random factors can dominate. Bimodal distribution of query

ments are suspect, but otherwise OK

TIVE SYSTEMS LABORATORY AT CASE

VERSITY

the Medlars study and the SMART system, the Comparative

was created by Tefko Saracevic at the Case Western University

otable not only for further innovations in evaluation but also for

the laboratory that allowed students to share in the excitement

ts working with the SMART system). The CSL work was done

a massive report which was summarized in [152]. Saracevic was

xing and (manual) search variables, but additionally had the goal

g of the variables and processes operating within retrieval systems

experimentation with such systems”. His test collection was built

eld 2 work, starting with a document collection of 600 full-text

h represented about half of the open literature at that time), and

olunteers from 13 organizations who were specialists in their ﬁeld


, keywords selected by a computer program, a metalanguage and the

ex terms. The indexing was done on titles and abstracts and full-text

esulting in 5 to 8 terms for titles, 23 to 30 terms for abstracts, and 36

ns gathered from the users that were then used for searching with ﬁve

on each question (unit concepts (A),expansion of A using a thesaurus

er tools (C), further expansion of C using thesaurus (D), and E which

D). Note that these question analysis types were mirroring the types of

termediary might make, as opposed to the more constrained approach

Cranﬁeld. However there was extensive checking of the searches for

ng re-run with errors ﬁxed. Additionally all of the searches were done

ed strictly on the question analysis and a broader search statement

ect aspect.

nts were created using a type of pooling method, i.e., a universal set

l sets of outputs from all the indexing strategies the question analysis

tegies.These universal sets were then judged by the 25 users on a three-

ument which on the basis of the information it conveys, is considered

n even if the information is outdated or familiar”), partially-relevant

basis of the information it conveys is considered only somewhat or in

tion or to any part of your question”), and nonrelevant.The judgments

ith over half of the questions having no relevant documents, and 80%

ving only one to ﬁve relevant.

ensitivity (deﬁned the same as recall) and speciﬁcity which is the number

OT retrieved divided by the total number of nonrelevant documents

hese metrics were calculated over all queries, i.e., the micro-averaging

ng with a combined metric effectiveness which was deﬁned as sensitivity

ns a thorough analysis of the indexing schemes, the question-analysis

rategies, coming to many of the same conclusions as both Cleverdon

however, was the ﬁrst to employ real user questions and relevance

o try pooling as a method of lowering the effort for relevance judging.

oling is that only three of the ﬁve indexing schemes were operational

(retrieving a total of 1518 answers); however, when the other index

08 additional answers almost all of which were nonrelevant. Saracevic

en to demonstrate that the ﬁles responded equally well in retrieving


ND THE “IDEAL” TEST COLLECTION

m (eventually joined by Karen Spärck Jones) was working on a

automatic classiﬁcation techniques [125].This was applied to the

goal of ﬁnding if these classiﬁcations were useful for information

manually indexed version of the collection was used (rather than the

xt), this work could be nicely compared with both Cleverdon’s and

ones was not satisﬁed with simply using the Cranﬁeld collection,

tions,an INSPEC collection of 541 documents (different than the

ollection by Mike Keen,the ISILT collection [110].Unfortunately

collection [163] and this led to a serious interest in the properties

1975 by Spärck Jones and Keith van Rijsbergen [166] for the

. The proposal had two overarching criteria: ﬁrst, that it would

across retrieval researchers, and second that it would be adequate

uld be noted that in 1975 the SMART collections (other than

y small in size, but lacked manual indexing and other “hooks” for

Cambridge and other British researchers continued to be more

ysis part of information retrieval, such as ﬁnding better ways of

an the searching stage of retrieval.

had chosen to use the British collections for her work in 1973,

s of British collections as background material and as a basis for

se are minimally described here to show the contrast in goals and

ollections, and to motivate some of the issues in the proposal.

rom Mike Keen, then at the University College of Wales in Aber-

ments from the area of documentation, with 63 real requests and

y subject experts. The purpose of this collection was to continue

sing ﬁve kinds of manual index terms. Three additional British

studies. The INSPEC collection [4] used 542 documents in the

loosely on SDI proﬁles (Selective Dissemination of Information,

ed set of citations to match an “SDI” proﬁle),with the users asking

pe of their SDI proﬁle and then making relevance judgments only

emed relevant to their proﬁle. Once again there were ﬁve types of

CIS collection [19] was for Chemical Abstracts, testing the effec-

es, titles plus keywords, and titles plus digests. There were many

nt subsets and 193 queries, again loosely based on SDI proﬁles

s. Finally the NPL collection [181] had 11571 documents (which


ring.

ollections (with the exception of ISILT) used data from commercial

ype of data, the collections were guaranteed real user requests and also

ng.This ﬁtted in well with the kinds of experiments done in Cambridge,

n what was available. The SMART collections were mainly built for

age of retrieval and were funded by NSF. This allowed more freedom

o mostly negated the use of commercial search services.

the proposal for the “ideal” test collection continued primarily in the

est collections. The proposal was presented at a workshop and a six-

worked on further details [164]. The following outline speciﬁcation

rectly from this report.

0 documents broadly representative of service data bases in size

n.

of 3000 documents complementing the main set in subject, etc.

e main set was in a scientiﬁc area, one other set would in social

short time periods and English language material; they would

ons. A random subset of the main set, containing 3000 docu-

shed, with enriched characterizations.

and the complementary other sets, would be heterogeneous on

ables such as subject solidity, document type, author type, etc.

should permit the identiﬁcation of subsets, say containing 3000

d be homogeneous on such variables.

e other sets would be required for time and language contrasts,

ntrasts on other variables, for example covering monographs as

se would have core or enriched characterizations as appropriate

0-1000 requests would accompany the main set of documents.

50 requests would accompany other sets of documents.

ld be of one form, envisaged as retrospective off-line queries, al-

ng different forms, e.g., SDI is one such method, and containing


t of the primary set, containing 150-250 requests, would be

haracterizations.

ting time and language contrasts, subsets of the primary set

uests.

alternative sets would be heterogeneous on such variables

t should permit the selection of homogeneous subsets of

resent many users, as well as many requests.

worked out and are further developed in (this) report. For

were: default judgments by the users of their own search

ts of the random subset of documents; pooled judgments on

ut; these would all use abstracts; checking judgments for the

inst the texts of the random subset, against another random

are:

egular bibliographic information, abstracts, citations, natu-

rolled language indexing, (using thesaurus terms or subject

bject class codes, and an about sentence;

tatement, lists of free and controlled terms, and a Boolean

a more exhaustive indexing, indexing from different sources,

e, PRECIS, etc; for requests, term weights, indexing by dif-

ooled judgments would constitute further request formula-

ce documents should also be obtained.

ological” background information relative to requests should

ntial information would consist of two relevance grades and

nown relevant documents would be recorded. Judgments by

overed by the basic design.


fferent strategies (probably different document and request analysis

g strategies); then the total number of required assessments (number

mber of assessments per request) was to be estimated based on the

fferences between two potential strategies. A statistician was funded

thodology, and a third report [74] provided detailed analysis of the

ossible methods. Further discussion of this report is beyond the scope

readers are strongly encouraged to invest some time here since the

t in today’s testing methodology.

ed information on possible document collections (very few machine-

ble at that time), sources for requests (usually from commercial search

ons might be managed. This included cost estimates and also results

that might use this collection. The ﬁnal sentence of the report is as

D can satisfy itself that,say 7 good projects will be forthcoming,“ideal”

or even £100K is a good buy”.

peciﬁcations have been provided here in such detail because it was

ction built at Cornell had some of these characteristics, and a version

elevance assessment, but it was not large, and it was built more for

rategies than for document analysis. The large test collections that

not built until 1992, and the TREC collections were not built based

other requirements (which are detailed in the next chapter).

mbridge investigation was a book “Information Retrieval Experiment”

shed in 1981, it became a major inﬂuence in information retrieval

rs discussing different aspects of evaluation by most of the leading

L WORK IN METRICS UP TO 1992

nandfurtherdevelopedbySaltonandKeenforSMARTweregenerally

Several problems existed with these measures; most importantly that

ble single measure of performance. By the late 1960s the normalized

replaced by several methods of averaging the recall-level values, such

vels, or 10 recall-levels, or 11 (including the two end points). But two

that have had signiﬁcant use in the research community.

William Cooper’s expected search length [55]. His paper argued that

measure, but that the various averages did not take into account how

user actually wanted. So his idea of the expected search length was

ser to ﬁnd their desired number of relevant documents, which in the


bine recall and precision was van Rijsbergen’s E measure [180].

t-based retrieval, such as the Cranﬁeld or Lancaster experiments,

ist retrieval by calculating recall and precision at document cutoff

at 20 documents.

E = 1 −

1

α 1

P + (1 − α) 1

R

trol how much emphasis to put on precision versus recall, and P

values for the set being examined. Again this measure allows user

est by the use of the α parameter. However it works best for set

rieval since use of the document level cutoff creates problems with

A similar measure called the F measure (1 - E) is heavily used in

ural language experiments where set retrieval is the norm.

180]and the papers by Robertson [133] and Sanderson [147] are

in this period and are strongly recommended reading for both

mparisons across the various metrics that were proposed (including

onally the various chapters in the Spärck Jones book [167] discuss

sbergen and Cooper.


Evaluation Since 1992

ION

ction became available to the information retrieval community. The

ation Conference) collection and its sister evaluation efforts built on

ding the methodology where appropriate. This chapter elaborates on

s were created, how the various evaluations were designed, and what

hods had to be made. The later sections of the chapter summarize

discuss general lessons that can be drawn from all these evaluations.

VALUATIONS

ndards (NIST) was asked in 1991 to build a test collection to evaluate

fense Advanced Research Projects Agency) TIPSTER project [121] .

signiﬁcantly improve retrieval from large, real-world data collections,

A contractors were involved,theTREC initiative opened the evaluation

val research community,with 25 additional groups taking part in 1992.

for nearly 20 years. Full coverage of the research is clearly beyond the

ers are referred to [184] as a general reference and to the full series

://trec.nist.gov/ for details relating to each year. However the

ethodology and the metrics presented in this chapter show the further

ormation retrieval

D HOC TESTS (1992-1999)

to the classic information retrieval user model used in the Cranﬁeld

s are searched against a ﬁxed document collection. This section dis-

methodology (test collection and metrics) in detail both because this

or later TREC tasks (and other evaluation efforts) and because the

heavily used in research today and it is important to understand how

AD HOC COLLECTIONS

sign was based squarely on the Cranﬁeld paradigm, with a test collec-

sts (called topics in TREC), and relevance judgments. Like Cranﬁeld,


eeded to have a very large number of full-text documents (2 giga-

ach year), which needed to cover different timeframes and subject

ed length, writing style, level of editing and vocabulary. Table 2.1

uring the initial eight years of the ad hoc evaluations; these were

suitability to the TIPSTER task. Articles from newspapers and

nd contrasted in their format, style, and level of editing. Docu-

e from different initial sources, but dealt with the single domain

here were documents selected less for their content than for the

ster ones were especially long, and of non-uniform length, and the

ll documents were converted to a SGML-like format with enough

ng by the systems. Note that at 2 gigabtyes these collections were

h systems in 1992 to handle, mainly because the storage to index

ost around $10,000 at that time.

typically provided only sentence-length requests; however, the

ed multiple ﬁelds, including a user need statement that is a clear

a document relevant. Having these multiple ﬁelds allowed for a

methods, and having clear statements about relevancy improved

ents.All topics were designed to mimic a real user’s need,although

format and the method of construction evolved over time. The

nvolved actual users of a TIPSTER-like search system and had

3 the topics were reduced to three ﬁelds and were written by the

ho did the relevance assessments. Figure 2.1 shows a sample topic

ns a number and title, followed by a one-sentence description of

ction is the narrative section, meant to be a full description of the

t separates a relevant document from a nonrelevant document.

has always been problematic in building information retrieval test

The TIPSTER task was deﬁned to be a high-recall task where it

ation.Therefore the assessors were instructed to judge a document

ocument would be used in some manner for the writing of a report

it was just one relevant sentence or if that information had already

This also implies the use of binary relevance judgments; that is, a

nformation and is therefore relevant, or it does not. Documents

d by a single assessor so that all documents screened would reﬂect

opic.


Docs

Words

Words

l

267

98,732

245

434.0

89

254

84,678

446

473.9

242

75,180

200

473.0

989

260

25,960

391

1315.9

OE

184

226,087

111

120.4

l

242

74,520

301

508.4

88

237

79,919

438

468.7

175

56,920

182

451.9

988

209

19,860

396

1378.1

News 1991

287

90,257

379

453.0

90

237

78,321

451

478.4

345

161,021

122

295.4

243

6,711

4445

5391.0

564

210,158

316

412.7

994

395

55,630

588

644.7

rd 1993

235

27,922

288

1373.5

t Information

470

130,471

322

543.6

475

131,896

351

526.5

l requirement that the relevance assessments be as complete as possible.

of both the implementation of TREC and the later analysis of the

thods for ﬁnding the relevant documents could have been used. In the

dgments could have been made on over a million documents for each

lion judgments (clearly impossible).The second approach, a true non-


he role of the Federal Government in

f the National Railroad Transportation

provide information on the government’s

TRAK an economically viable entity. It

ivatization of AMTRAK as an alternative

subsidies. Documents comparing

n to air and bus transportation with

would also be relevant.

t from TREC-3

uments, would have been prohibitively expensive for acceptable

biased sampling method called “pooling” was adopted from the

ary for building an “ideal” test collection [164]. To construct the

s done. Given a ranked list of results from a single system, for each

uments for input to the pool. Then merge this set with sets from

ed on the document identiﬁers, and remove duplicates (identical

tems in the pool). This created the pooled list for each topic that

AD HOC COLLECTIONS

run for 8 years (see Table 2.2 for details of the eight collections), it

he various evaluation decisions were working and to modify them

nted in detail here because these large collections are still in heavy

strengths and weaknesses in order to avoid experimental bias. It

f the types of issues that need to be investigated in building future

nd semi-standardized formatting worked very well. Groups had

nts (other than scaling issues), and had no problems with domain

d that by far the largest number of relevant documents came from

domains: Wall Street Journal (WSJ) and Associated Press (AP). In


disks 1 &amp;2

101-150

disks 1 &amp; 2

151-200

disks 2 &amp;3

201-250

disks 2 &amp; 4

251-300

disks 4 &amp;5

301-350

ks 4 &amp; 5 (minus Congressional Record)

351-400

ks 4 &amp;5 (minus Congressional Record)

401-450

Register (FR) documents had few relevant documents,but inTREC-2

ﬁculty in screening out these long documents. By TREC-3 this effect

e systems made major corrections in their term weighting algorithms

thus could cope with any length document.

for TREC underwent major evolution across the ﬁrst ﬁve TRECs.

a result of changes in the personnel constructing the topics, but most

anging of the topic speciﬁcations.The elaborate topics in the ﬁrst two

manually-selected keywords (the concepts ﬁeld) and this was removed

lt that real user questions would not contain this ﬁeld, and because

aged research into techniques for expansion of “too short” user need

ics were made even shorter, with removal of the title and the narrative

to be too short, especially for groups building manual queries, so the

dard.

in how the topics were constructed.InTREC-3 the assessors brought

of issues on which to build a topic. These seeds were then expanded

ing at the items that were retrieved. To avoid this tuning to the data,

sors were asked to bring in a one-sentence description that was used

imate the number of relevant documents that are likely to be found.

bers of relevant documents were then kept for further development

opics.

e topics relates to measuring the difﬁculty of a given topic. There has

o build topics to match any particular characteristics, partly because

topics, but also because it is not clear what particular characteristics

ure called topic “hardness” was deﬁned for each topic as the average

e precision at R (where R is the number of relevant documents for

t 100 if there were more than 100 relevant documents. This measure

high recall performance and how well systems do at ﬁnding all the

C-5 an attempt was made to correlate topic characteristics with this


82].

e “required” number of topics for a test collection, i.e., how many

e performance averages to be stable, much less show signiﬁcant

chniques. There has always been a huge variability in the perfor-

Lancaster experiments described earlier or in the selection of the

n the Cranﬁeld collection. TREC was no exception here, with a

of the topics, in the system performance on each topic, and in the

es, such as relevance feedback on each topic. However it is critical

sure truly reﬂect differences rather than just random performance

n the SMART project, that a minimum of 25 topics were needed.

ave been shown to produce stable averages [34, 189], the measure-

till a problem in information retrieval, with some TREC-speciﬁc

and much more work since then (see Chapter 5 in [147]).

ments were speciﬁcally designed to model users interested in high

complete the relevance judgments are,the better the test collection

hese users. Additionally, the more complete the test collection, the

sing the collection for evaluation can trust that all/most of the

tion have been identiﬁed. Note that the pooling methodology

ave not been judged can be considered non-relevant.

ment completeness assumption was made using TREC-2 results,

luation.In both cases,a second set of 100 documents was examined

sample of topics and systems in TREC-2, and using all topics

. The more complete TREC-3 testing found well less than one

These levels of completeness are quite acceptable; furthermore the

s found was shown to be more strongly correlated with the original

e., topics with many relevant documents are more likely to have

mber of documents judged.

ndently veriﬁed by Justin Zobel at the Royal Melbourne Institute

ditionally Zobel found that lack of completeness did not bias the

that systems that did not contribute documents to the pool can

pooled judgments. Since the goal of the TREC collections is to

s,either across systems or within systems,having the exact number

an exact recall number is not as important as knowing that the

insure that comparisons of two methods using the test collections


l experiment [50] showing minimal effects on system comparison. For

by a single assessor to ensure the best consistency of judgment and

done after TREC-2, and more completely for TREC-4 [78, 80]. All

rejudged by two additional assessors, with the results being about 72%

measure of the intersection over the union) among all three judges,

the initial judge and either one of the two additional judges. This

ement is probably due to the similar background and training of the

mbiguity in the topics as represented by the narrative section.

hisagreementwasforthelargenumbersofdocumentsthatwereclearly

an 3% of the initial nonrelevant documents were marked as relevant

the documents judged relevant by the initial judge were marked as

ional judges. This average hides a high variability across topics; for 12

ent on relevant documents was higher than 50%.

agreements were likely caused by mistakes, most of them were caused

ent, often magniﬁed by a mismatch between the topic statement, the

tion. For example, topic 234 is “What progress has been made in fuel

terpretation might declare relevant most documents that discuss fuel

uire that relevant documents literally present a progress report on fuel

ome of the more problematic topics were either very open to different

What are the trends and developments in retirement communities?”)

e document collection that the initial assessor made extremely lenient

49: “How has the depletion or destruction of the rain forest effected

r variation is very realistic and must be accepted as part of any testing.

ms with different expectations, and most of these expectations are

not reﬂect this noisy situation, then the systems that are built using

algorithms will not work well in operational settings.

ow all this variation affects system comparisons. Voorhees [185] in-

ent subsets of the relevance judgments from TREC-4. As her most

ersection of the relevant document sets (where all judges had agreed),

ments (where any judge had marked a document relevant). She found

ge precision of a given set of system results did change, the changes

systems and the relative ranking of different system runs did not sig-

n the two runs were from the same organization (and therefore are

two systems were ranked in the same order by all subsets of relevance

nstrates the stability of the TREC ad hoc relevance judgments in the


el judged over 13,000 documents for relevance, and these judg-

similar manner as the TREC-4 multiple judgments. Even though

tween the NIST assessors and the Waterloo assessors (very dif-

, the changes in system rankings were still not signiﬁcant. The

mparison between two same-system runs in which one run had

For this reason, comparison between automatic runs and runs with

manual relevance feedback which basically adds a third relevance

nalyzed as they are the comparisons most likely to be affected by

C METRICS

etrics, essentially using the metrics developed by Mike Keen and

tarting in TREC-1 Chris Buckley made available the evaluation

trec_eval. Researchers in the ﬁeld at this time were using various

different implementations and with different choices of which

parison across systems difﬁcult and the availability of a standard

on implementation of these metrics.

metrics provided for a run in the TREC-8 ad hoc track.The recall

n averages across the 50 topics are shown, in addition to a new

n, deﬁned as “the precision at each relevant document, averaged

pic” [33].The non-interpolated average precision is then averaged

mean average precision” or MAP, which has been used as the main

etrics include the R-Precision which was proposed by Buckley to

sk being modeled in TREC. For more details on these metrics,

ative strengths and weaknesses, see [33, 147]. Note that this result

owing the results for all of the 50 topics so that groups could easily

med with respect to the median system performance per topic.

ETRIEVAL TASKS

ed in TREC-4, and led to the design and building of many spe-

hese test collections were as extensive as nor as heavily used as the

er, but the necessary changes in the design criteria provide useful

ctions. Note that these changes were required either because of

r because the track research goals dictated modiﬁcations to the

on. The track descriptions that follow are ordered by the amount


Summary Statistics

Total number of documents over all topics

Recall Level Precision Averages

Recall 

Precision

Document Level Averages

Precision

 Retrieved:

50000

4728

2986

 Relevant:

 Rel-ret:

Run Number

Run Description

Number of Topics

Automatic, title + desc

Sab8A4

50

0.00

0.10

0.20

0.30

0.40

0.50

0.60

0.70

0.80

0.90

1.00

0.7860

0.5229

0.4324

0.3644

0.3084

0.2498

0.1912

0.1360

0.0776

0.0362

0.0133

At 5 docs

At 10 docs

At 15 docs

At 20 docs

At 30 docs

At 100 docs

At 200 docs

At 500 docs

At 1000 docs

0.5200

0.4800

0.4413

0.4090

0.3733

0.2384

0.1702

0.0985

0.0597

R-Precision (precision after

R does retrieved (where R is 

the number of relevant 

documents))

Exact

0.3021

Average precision over all 

relevant docs

non-interpolated 

0.2608 

1.0

0.8

0.6

0.4

0.2

0.0

0.0

0.2

0.4

0.6

0.8

1.0

Recall

Recall-Precision Curve

Precision

1.0

0.5

nce

oc results       SabIR Research/Cornell University


“NOISY” TEXT

hoc task, but the documents are “noisy” text, such as that produced

ption, and the goal is to understand how retrieval performance is

4 the ad hoc topics were used against artiﬁcially degraded Wall

error rates of 10 and 20% character errors. For TREC-5 [191], a

al OCR data from the 1994 Federal Register, with comparison of

tronic version), plus two scanned copies at 5% error rates and 20%

ged to known item topics, i.e., each topic was created to uniquely

search that is particularly useful for testing in “noisy” data, where

use a system to miss that document. The metric used to measure

riation of Cooper’s expected search length [55], and was based on

eved. In this case however a measure called mean-reciprocal-rank,

eraging. MRR is the mean of the reciprocal of the rank where the

across all the topics, and is mathematically equivalent to the mean

nly one target document [183].

comes from speech recognition systems.Two groups at NIST, the

oup, collaborated to implement test collections for broadcast news

for speech processing there is no clear deﬁnition of a document;

eciﬁc stories, usually separated by a change in speaker and/or a

e “documents” were provided both as text transcriptions of the

ranscribed) or as recorded waveforms. TREC-6 used a 50 hour

ours, and there were 557 hours for TREC-8. In addition to the

ormance metrics, the speech metric word error rate (WER) was

deo retrieval track, working with 11 hours of video, and 74 known

articipants.This increased the next year to 40 hours of video from

nternet Archive, with 25 specially created topics from NIST. The

media way, with text supplemented by video clips or bits of speech

f requests from likely users. The deﬁnition of documents became

ideo needed to respond to the topic had to be measured at shot

rieval task had become much more concerned with the image part

into its own evaluation, TRECvid (http://www-nlpir.nist.

more on video retrieval, see both the TRECvid site and references

ty such as [160].


vely. The only change necessary to the ad hoc model was the issue of

o deal with the accents in Spanish and the Chinese character sets.One

ed in this ﬁrst evaluation was the importance of using native speakers

relevance judgments to reﬂect a real user model and to allow speedy

trying to retrieve documents in a language that is not their native

for cross-language retrieval (CLIR) would be creating topics in their

then be searched across the set of other languages, since it is assumed

y in reading another language than in creating topics in that language.

hat the topics be created by native speakers in each language to insure

would actually express their information need in that language.

r the ﬁrst CLIR track (TREC-6) was the creation of a document

llel” documents in three languages (French,German,and Italian) from

ische Depeschen Agentur. English documents from the AP newswire in

d all documents were put in the SGML-like format with a new ﬁeld

language of that document. The 25 topics used the ﬁrst year were

NIST, however by the second year this became a co-operative effort

Germany and Italy, with 56 topics written in four languages (English,

for TRECs 7 and 8 [29].

t from the initial topic languages,equal numbers of topics were chosen

ic set.The full set of topics were then translated to all four languages.

ade at all four sites for all topics, with each site examining only the

ive language.This distributed scenario for building topics and making

cessary but major departure from the Cranﬁeld model.

the distributed method of relevance judgments on results is probably

tion was across languages, not topics. As long as results are compared

pairs of results on German documents, and not across languages, i.e.,

vs. German documents, there are no problems. However comparing

es is both comparing across completely different document collections

ce judges and is not valid.

ack moved to Europe after TREC-8 to become a separate evaluation

campaign.org/) and information about further work in CLEF is in

ued CLIR for three more years, ﬁrst with English and Chinese, and

glish and Arabic.


Hawking and his colleagues at CSIRO put together ﬁve large

ater to test web applications [87]. This was piloted in TREC-6

(called VLC1), but scaled up to 100 gigabytes of web data from

next year [86, 88]. The TREC-7 ad hoc topics were used against

ols (20 documents deep vs. 100), and whereas this test collection

est, it was appropriate for normal high-precision web testing. In

ed the VLC2 data for 10,000 web queries from a search log (only

d). Additionally the VLC2 data was appropriately down-sampled

ent of 2 gigabytes (WT2g) and the TREC-8 ad hoc topics were

nce assessment done for both the text and web collections. This

of the smaller web track against the ad hoc topics, but with more

r the larger web collection [89].

itially only for efﬁciency (and various efﬁciency metrics were used

b data as documents was also of interest. Like the speech data,

ar deﬁnition: in this case a document was deﬁned as a single web

ts but not including other documents linked to it. TREC-9 saw

collection, a 10 gigabyte subset of the VLC2 corpus speciﬁcally

ther than simply being a large chunk of documents. There was

nectivity, along with other desirable corpus properties that were

lection that was a proper testbed for web experiments [15]. Two

at 18 gigabytes was created from material in the *.gov domain [60].

of the task, the web track up until 2000 (TREC-9) was still much

ial topics were the ad hoc topics because a major goal of these

ale and the structural information for the web documents affected

much). However discussion with various web groups at this point

topic style was only a small part of the web activity and if TREC

ser models, the topics needed to change. So the TREC topics in

ch as ordering ﬂowers), with short topics reverse-engineered from

ected topics from the search log and created a “full” TREC topic

at matched their interpretation of that topic before examining the

ntry was included as the title of the new topic (with any original

thatthe topic waswell-enough deﬁned to have consistent relevance

atched real web logs.

method continued to be used in the web track, but topics evolved

ic) tasks in 2002, such as ﬁnding homepage locations and topic

of relevant sites). The metrics also reﬂected this realistic model,


elevant, relevant or non-relevant.The three-level judgment evaluation

he NIST assessors liked this mode of judgment because it made the

nal communication from Ellen Voorhees). In 2004 there was a second

rack, which used a much larger web collection, the 426 gigabyte“gov2”

r discussed in Section 2.4.5.4.

b track became the enterprise track, with the emphasis on looking at

t searching. Here the user model is someone within an organization

n.The data for 2005 and 2006 was a crawl of the W3C site, including

es and text in various formats.Topics were constructed for three search

ail message), an ad hoc search for email on a speciﬁc topic, and search

59]. Two of these tasks continued in 2006, but similar to earlier web

n that the topics/tasks being modeled were not realistic because they

utside of the organization. So in 2007 and 2008 the data and topics

zation, CSIRO (Australian Commonwealth Scientiﬁc and Industrial

data was the CSIRO public web site, and the task that was modeled

erview” page for a speciﬁc topic, which would include key links from

long with key experts [17].The CSIRO staff generated 50 such topics,

ing most of the relevance judging. For a discussion of how well this

ee [16].

FIC RETRIEVAL TASKS

s sections have all dealt with domain independent tasks: the data came

es or from the web.With the possible exception of the enterprise track,

task-speciﬁc data, the domain of the documents did not inﬂuence the

TREC have worked with domain-speciﬁc data, where the tasks and

is data. One of the critical issues in dealing with domain-speciﬁc data

mation needs (and appropriate relevance criteria) for tasks within that

way the Cranﬁeld model was domain-speciﬁc in that the user model

ngineer that might be searching in the domain of aerodynamics.

th Sciences Institute ran the genomics track for retrieval from medical

C 2004. Part of the goal of this track was to see how well the ad hoc

cal domain, but an additional part was to develop test collections and

ted to the retrieval needs of the genomics community. The document

4 million MEDLINE records, about one-third the size of the full


to closely mirror the requirements of the genomics community.

ewed 43 genomic researchers, gathering 74 different information

reate 50 topics in a standard format similar to the ad hoc model.

ad hoc task, using pooling and relevance judgments by people

opics the second year were more focused, with six generic topic

50 topics [92]. These templates allowed the topics to concentrate

ms, such as the role of a speciﬁc gene (where different genes could

for the next two years, the track moved to a passage retrieval task,

the documents. Note that passages are difﬁcult to deﬁne so this

ons, deal with the difﬁcult relevance judgment issues that go with

ly use metrics that are appropriate for passage retrieval [131].The

maximum span of one paragraph, but could be much shorter, e.g.,

answer the topic/question. By the ﬁnal year of the track (2007), a

ision (MAP) measure had been developed that compared character

ssages and those that had been selected as the “gold-standard” by

was used with the passages, but then the submitted passages were

their coverage of all the speciﬁc aspects of the answer. “To assess

to break down the questions into the required elements (e.g., the

hat make up the questions) and isolate the minimum contiguous

on [96].” There was also an aspect-level MAP that looked at what

vered. The ﬁnal evaluation used all three MAPs (document-level,

making this one of the most complex evaluations at TREC.

resulted from a collaboration of information retrieval researchers

lop and apply objective criteria for comparing methods for search-

ns using topics that approximate how real lawyers would go about

tigation” [20]. This in turn was driven by new regulations gov-

d be used in civil cases in federal courts. Doug Oard and David

f the U.S. National Archives and Records Administration to adapt

ology to this task.

e IIT Complex Document Information Processing (CDIP) Test

n document records from the Legacy Tobacco Documents library.

XML records, contain many kinds of electronic records such as


al complaints, such as “a shareholder class action suit alleging securities

n with a ﬁctional tobacco company’s campaign”.These complaints were

al ad hoc topics (there were 45 in 2008). These complaints and the

by the Sedona Conference Working Group on Electronic Document

onproﬁt group of lawyers, who also then created the baseline Boolean

e Boolean queries would have been the “normal” way of requesting

gal situation. For more on this, including a deeper background of the

in 2008 and 2009 [91]: an ad hoc task, a relevance feedback task, and

hoc task, systems returned both a ranked list and their optimal cutoff

the main metric being the F measure at K. Although recall is critical

lso important to return “good” sets as opposed to just ranked lists.

e made using enormous pools of documents, with various sampling

rs .The ﬁnal set of documents (an average of over 500 per topic), were

d third-year law students. (The interactive task was very user oriented

along with the other interactive tasks in TREC).

nald from the University of Glasgow worked with NIST to design and

blogsphere in TREC2006.They created a large blog collection [126]

links, along with associated homepages, crawled from 100,649 blog

od. The crawl was speciﬁcally designed to include topics of interest

include assumed spam blogs (splogs) to allow for realistic tasks. Note

eciﬁc domain such as genomics and legal, they have a different nature

wswire or web text, and attract different types of user tasks.

Cs 2006, 2007 and 2008 worked with opinion ﬁnding tasks, where

was the blog post plus all the associated comments as identiﬁed by a

nd all documents containing an opinion on a given topic, and then

s positive, negative, or “mixed”. The 50 topics were generated by the

log from BlogPulse as seeds for reverse-engineering of the full topic.

s: not relevant to the topic (0), relevant but not containing an opinion

negative opinion (2), same as 2 but mixed opinion (3), and same as 2

itional ad hoc metrics were used both on the relevancy (marked 1 or

ncluding opinions (2 or higher) [127].

for blogs was the blog distillation task run in TRECs 2007 and 2008,

son seeking to ﬁnd an interesting blog to follow or read in their RSS


0] had a much larger collection (Blogs08), with 28.5 million blog

log feeds from January 2008 to February 2009. A more complex

ms use blog attributes (opinionated, personal, or in-depth) as well

developed and judged at NIST.

ITS OF THE CRANFIELD MODEL

sly discussed were basically using the Cranﬁeld model. Whereas

eech, or websites, or blogs, and there were various user goals being

sult was usually a ranked list of unique items for judgment, and

considered re-usable. This ﬁnal set of TREC tracks are grouped

quired some major deviation from the Cranﬁeld model, usually

e resulting test collection or unsolved problems in the evaluation.

hich ran for 9 years starting in 1999 (TREC-8), required systems

instead of returning a ranked list of documents. This task both

ted an opportunity to work with the natural language processing

nt collections were the basic English ones used in the ad hoc task,

with simple fact-based, short answer questions (factoids) such as

”, and progressing to “deﬁnition” questions such as “Who is Colin

a test collection had to be changed for this track. Whereas there

ns, the answer set is not the set of relevant documents, but rather

nts. In TRECs 8, 9 and 10, passages of lengths 250 and 50 words

answers (along with the document id), and variable length strings

86].

above do not constitute a reusable test collection because the unit

ing.Different runs seldom return exactly the same strings,and it is

atically if the difference between a new string and the judged string

rrectness of the answer.Any new system using this test collection is

which could be correct but would be “graded” wrong, making their

Whereas there is no known answer to this problem, NIST (and

ated answer patterns consisting of Perl string-matching algorithms

d researchers use these patterns along with the correct document

ew experiments (see software at http://people.csail.mit.


using Excite logs as seeds, real questions from Encarta, and full ques-

and AskJeeves logs. The answer strings for the factoid questions were

ng based on mean reciprocal rank (MRR) similar to the known item

List questions were added, such as “Name 4 countries that can produce

were not only graded for correctness but also for redundancy, with the

of correct answers divided by the target number).

answers were required in order to “prove” that the systems knew the

not be the actual user model, but could be considered as part of an

wer would be highlighted in the text. Only one answer was allowed

the number of correct answers using a new metric that also tested the

heir answers (see [186] for more details of this metric).

he track had factoid, list and deﬁnition questions built in a series of

t ask about a person, organization or a thing. For example, someone

comet might ask factoid questions such as “when was it discovered”,

untries was it visible in on its last return”, but also want to see “other”

tion. Whereas the factoid and list questions could be evaluated as

uired a new mechanism. The judging was done in two stages. In the

at a pool of all the system response strings and also added their own

en broken into “nuggets” for evaluation purposes, i.e., facts for which

ary decision as to whether a system response contained that nugget.

agged as “vital”, and systems were scored on recall (how many of the

d precision, where precision was based on the length of the full set of

more details).

n 2005 by Gordon Cormack of the University ofWaterloo to encourage

he major difﬁculty was the privacy issue since public distribution of

al and the few available public email ﬁles (such as the Enron data), or

y reﬂect real user experience.The track ran for 3 years, with a carefully

nsisting of 75,419 messages (about two-thirds spam), and a private

g of 161,975 messages (95% spam). The public data from previous

g purposes, whereas the private corpus was accessed only via a toolkit

erations (initialize, classify, train ham/spam, and ﬁnalize). The toolkit

on to implement the various feedback tasks and to record and score

e designed to model different user behavior.The ﬁrst gave immediate

wer was provided to the ﬁlter for quick “re-training”. The second


, with answers returned depending on the appropriate feedback

n classiﬁed, the scoring was based on ham misclassiﬁcation and

ROC curves and a single metric (logistic average misclassiﬁcation

ﬁnition of this metric).

sks

f task right from the beginning: the routing task. Here it was

meone with an on-going information need, someone who has done

s some relevant documents for their need), but wants to continue

etopicisﬁxed,butthedocumentsetkeepschanging/growing.This

telligence agent, but also news-clipping services, users following a

e Cranﬁeld model was essentially unchanged. The topics used

previous TRECs, with the relevance judgments available, and the

a that was specially gathered for the routing task. The results on

ranked list, then evaluated using the ad hoc pooling techniques

y worked, although at times it was difﬁcult to ﬁnd new data that

ipating systems tended to use this task as a way of improving their

g about a routing application.

5 that this task did not reﬂect many user models; a more realistic

rdered set of documents into a set of relevant ones for the user

eled in several ways, such as in the batch ﬁltering mode with

me-ordered set, and the test set from a later time-ordered set, or as

aining is incremental based on feedback from the user. However

etrieval rather than a ranked list.

EC-6, and running for ﬁve years, along with the routing task,

ms of evaluation [136]. The ﬁrst was the continuing need to ﬁnd

ns were tried, some more satisfactory than others. The ﬁnal year

ering collection speciﬁcally built for the Reuters collection using

complete” set of relevant documents formed by doing successive

nded that new ﬁltering experiments use these 50 manually built

ones built for the earlier TREC ﬁltering tracks. (Note however

perimental” topics built automatically in 2002 using the category

successful and should not be used.)

the pooling for relevance judging since the systems were not using

e tried [118], such as sampling the results from each system and


udgments.

olved the metrics, which were generally utility measures. Note that

g on these metrics, one speciﬁc metric needs to be selected in advance,

ics for the ad hoc task. Diverse utility measures were tried (again see

ures.The “correct” metric to use remains an unsolved problem as each

model and the choice of the metric strongly effects both the results

at are used for pooling.

ery and the “new” Web Tracks

TREC 2004, with the goal of working on a larger, very realistic web

OV2) had 426 gigabytes crawled from U.S. government sites (the .gov

from PDF, Word and postscript ﬁles. For the ﬁrst year of the track,

reated in a similar manner to the original web track and the top 85

re pooled for the relevance assessments. In 2005 and 2006 a named

n, where the task was to ﬁnd a particular page that a user might have

relocate (known item task with “near duplicates”). There was also an

rack was a mainly scale-up of the old web track to a larger collection,

vance judgments would not be “complete”. A new measure bpref [35]

mplete collections was tried, and the pools were made larger (top 100

ally it was found [32, 47] that the pools were biased in that there was a

ords from the topics in the relevant documents.This means that runs

score higher than more “exotic” runs because documents without title

This problem was likely caused by the huge scale of these collections,

vant documents that never appear in the pools.

ck [37] made an effort to measure this effect by building three separate

MAP evaluation) was the top 50 documents from each run.The second

ing additional documents up to a total of 1000 documents per topic to

dditional relevant documents were not judged, with the metric being

re it was found that there were indeed more relevant to be found at

titlestat measure still reading 0.6 at depths 400-500, down from 0.74

ool was a random sample using estimates of probability of relevance

ents from the ﬁrst pool (top 50), and results of experiments done in

d for a new evaluation measure inferred average precision, infAP [202].

s were used in 2006: MAP, bpref and infAP.

pooling problems led to the Million Query Track starting in 2007 [7].

nvestigate whether it was better to use many more queries/topics with


and by the participants.The queries to be judged were selected in

e assessor picking a query from a set of 10 (or skipping to the next

a seed to build a full TREC topic statement. The documents that

come from traditional pooling but were sampled using one of two

Collections (MTC) [41] or statAP [12, 13], using the appropriate

s.

several parameters in order to more deeply investigate these two

equally divided into four sets:those with 6 or fewer words that had

e with more words and more clicks, and the other two possibilities

s/short query). The judgments were done only at NIST, with 792

nt stopping points (8,16,32,64,and 128 judgments).This allowed

he strengths and weaknesses of the two different methods. It was

re stable using MTC for the smaller number of judgments (16/32),

for 64/128 judgments. However MTC seemed more affected by

d Terabyte (renamed Web) tracks continued in 2009 using a new

boston.lti.cs.cmu.edu/Data/clueweb09). This collection

ages (25TB of uncompressed data) in multiple languages resulting

February 2009. A smaller subset (Category B) of the collection

which are approximately the ﬁrst 50 million English documents

version of Wikipedia. It should be noted that the Category B

haracteristics, including high-quality “seed” documents that were

rawl and results from this subcollection (or any subset of a large

lly considered with respect to any conclusions or generalizations

b track in 2009 had an ad hoc task (50 topics using MTC pooling

ity task where documents were also judged with respect to special

ATION CAMPAIGNS

evaluation campaigns, often starting in a similar vein as TREC

w areas especially appropriate for their participants. In addition

is the FIRE evaluation (http://www.isical.ac.in/) with a

us Indian languages and the Russian evaluation (http://romip.


tion to the NLP community, allowing for some unique tracks. Ad-

retrieval evaluations with patents, developing appropriate evaluation

iﬁcation, and translation efforts in this ﬁeld. Online proceedings and

CIR can be found at http://research.nii.ac.jp/ntcir.

gust 1999) worked with 339,483 Japanese abstracts from 65 Japanese

f of these abstracts were also in English, and all were written by the

cs were gathered from researchers, with assessments done via pooling

partially relevant and non-relevant). NTCIR-2 (March 2001) had a

lection but with the 49 new topics also translated to English (allowing

se) and a fourth grade for relevance judgments (highly relevant).

r 2002) there were three languages for CLIR, with over two hundred

n Japanese and Chinese from 1998-1999, plus a smaller collection in

four languages (including English), and were translated for the other

pics to indicate both the source language and the target (translated)

rec_eval was not set up for graded relevance judgments, there was a

nt and relevant) score and a “relaxed relevant” (also including partial

ers were added in NTCIR-4 (June 2004),making almost equally sized

ese, Korean and English (from English versions of Asian newspapers).

ument sets across all four languages allowed complete multilingual

ntinued in NTCIR-5 (December 2005) with new document sets from

007) CLIR task was run in two stages and without the English doc-

re 140 topics reused from NTCIR-3 and 4, but this time against the

00-2001. Only shallow pools of the top 10 documents per run were

ving enough relevant documents were used in the scoring. In addition

, the multi-grade relevance scoring (nDCG and Q) was used. Stage 2

systems, without parameter adjustment, on the NTCIR-3 thru 5 test

2 was to provide information for cross-collection analysis.

NTCIR encouraged both a summarization task (NTCIR-2-NTCIR-

task. The summarization task was done in Japanese only; looking at

summaries of newspapers. Different evaluation methods were tried,

mitted summaries on a scale from 1-4 (best to worst) for completeness

he number of character edits (insertion, deletions, substitutions) that

matic summary into the human summary.The question answering task

y and was similar to the TREC 2002 QA task in that exact answers

tions. However a subtask consisted of 40 “followup” questions, where


nd the question-answering effort merged into one task in NTCIR-

ased evaluation in both the retrieval and the question components,

hanging interim results and combining modules for different sys-

tent retrieval task since NTCIR-3 [71].The ﬁrst year the task was

opics inspired by newspaper articles in four languages against two

e goal of ﬁnding relevant patents. This task became a true patent

nst ten years of Japanese patents, including major experimentation

opics (invalid patents) more easily by using the rejection citations

ethod was enhanced in NTCIR-5, along with including a subtask

documents. NTCIR-6 (2007) saw more emphasis on CLIR in

nt invalidity task and an English one using 10 years of U.S. Patent

in NTCIR-7 and 8, including an aligned corpus of these patents.

evaluated, along with assignment of patent classiﬁcation codes to

lly in Japanese and English, and in a cross-language mode.

from TREC to a new conference, CLEF (Cross-Language Eval-

one of the reasons for the move was that TREC needed to work

major reason was that it become obvious that U.S. participants did

aybe the interest) to progress much further in the research. The

ed with TREC were interested in starting a new conference, led by

ly to continue the European CLIR effort but to expand it to more

from Europe. Working notes from all of the CLEF workshops

ef-campaign.org/, with formal proceedings produced in the

uter Science each year.

ial four TREC languages (English, French, German and Italian),

the addition of one or two new languages, starting with Spanish

lly ending with 13 different languages for the documents, and

cs. In order for a new language to be added, a native-speaking

needed to obtain adequate newspaper data from the 1994/1995

ut in TREC-like formatting, and then provide personnel to create

nts, etc. This was a major effort and it is a credit to the CLIR

were able to do this. As languages were added, research groups in

o perform their research using their own language for the ﬁrst time

asks were offered in these new languages. Additionally the cross-


anguage pairs were offered in a given year in order to concentrate the

expansion of the CLEF CLIR ad hoc tasks into so many languages not

ch groups to join in, but for the ﬁrst time allowed major investigations

anguages with respect to retrieval [155].

ciﬁc evaluation issues given the widespread distributed nature of the

, topic creation needed to be managed very carefully to truly reﬂect

pics needed to mirror the types of questions asked within the various

for example) in CLEF 2001 by asking each of the 5 language groups

5 topics, along with doing a pre-search to make sure the topics were

uages [201]. These 75 initial topics were then jointly pruned to 50

lems in one or more of the languages. The ﬁnal set of topics was then

er languages, with indirect use of the English master set only when

cal that the translation not be word-for-word, but take into account

ultural background of the target languages, i.e., the ﬁnal topics must

n country would actually ask that question.

F was pooling given the sparse nature of the submissions. For example,

shandSwedish,therewereonly7groupsforFinnishand8forSwedish.

ed [30] to check for the completeness of the resulting test collection.

ultiple sets of relevance judgments, actually n+1 sets containing the

built by removing those relevant documents uniquely found by one

1 sets are then each used to create the test results, with the goal of

ould have performed if their unique relevant documents had not been

d relevant. The results for Finnish and Swedish showed a maximum

% for Finnish and 2.02% for Swedish, which can be considered not

any new tracks over the years, such as cross-language question answer-

in 10 languages against target text of 9 languages in 2005), a geospatial

pics with geospatial information, and a structured data track (GIRT)

n 3 languages. One of the more unusual (and popular) tracks was Im-

k with captioned photographs in 2003, where the goal was to search

inst the English captions.This track expanded to include over 20,000

aptions in English, German and Spanish, along with 50,000 medical

veral languages.Whereas the initial and continued main task was work

he images themselves were also used in the retrieval task, either alone

ptions.


he initiative used 12,107 full-text scientiﬁc articles from 18 IEEE

with each article containing 1,532 XML nodes on average. The

s in 2005 and moved on to using Wikipedia articles starting in

NEX has also had auxiliary “tracks” over the years (see http://

isburg.de/ for INEX through 2007; the current site is http://

rieval has run since the beginning but with major differences from

ause the goal is retrieval of a particular element of the document

he “relevant” documents can range from a paragraph to the whole

een to present the user with the “best” element in the article with

st, that is an element that exhaustively discusses the topic without

cs.The notions of exhaustivity and speciﬁcity,although well known

munity, are very difﬁcult to measure, and this has caused extensive

hin INEX over the years [114]. This difﬁculty also extends to the

of the reason that all of the relevance assessments in INEX are

also built by the participants) have reﬂected the structural nature of

topics resemble TREC topics, however the content-and-structure

y language in the title section, which provides speciﬁc structural

of topics have been evaluated similarly, although the structural

e interpreted variously.

trates the complexity of the evaluation. There were four subtasks

rough (ranked list of elements), focused (ranked list of focused

ant in context (ranked list of the full articles, but including a set

d best in context (ranked list of the articles, but including the best

fferent user approach,with the focused assuming a top-down look

he relevant in context and best in context assuming the user wants

more complex displays for the elements within that document.

acks in 2005, building up to 8 tracks plus ad hoc by 2009, with

of XML. The longer running tracks have been interactive (user

ltimedia (retrieving images using the XML structures), and doc-

ssiﬁcation of XML structured documents). A book search track

and question answering within the XML environment started in

dia, there is a Link-the-Wiki track, and also an efﬁciency track.


aluations, but also from the explosion of information access available

his chapter has been more on the design of test collections, this section

new metrics; for a much more detailed discussion, see [147].

ec_eval was in place using non-interpolated average precision and the

.3.2). But the known-item searching in the TRECs 5 and 6 OCR and

r new metric, mean-reciprocal-rank, MRR, to allow proper averaging.

rocal of the rank where the known item was found, averaged across all

ally equivalent to the mean average precision when there is only one

have required new metrics, both because of the different tasks and

or pooling. See the Terabyte track, Section 2.4.5.4, for discussion of

TREC has been the averaging of results across the topics,even though

tem performance when results are examined on a per topic basis.The

t-test used earlier (see Chapter 5 in [147] for a excellent discussion

rmation retrieval evaluation) is one possible way of examining these

c GMAP using the geometric mean of the average precision scores (as

an) was used in a variation of the ad hoc track in 2004 to emphasize

[187].

covered in this section was not driven byTREC issues but by a general

e judgments were not sufﬁcient and that some type of grading of judg-

what is done for commercial search engines). The group at Tampere

ework for dealing with graded relevance judgments Discounted Cumu-

both the relevance grades and the position on the document ranking

an be parameterized to give more or less discounting. A normalized

is heavily used both in commercial search engine evaluations and in

ed relevance assessing. The measure has also been recently adapted to

101] and diverse query evaluation [46].

E ON USING, BUILDING AND

G TEST COLLECTIONS

m the more formal coverage of the batch evaluations by offering some

e test collections, building new test collections, and evaluating those

meant to be a complete manual on this topic but rather comes from

many years of working with test collections.


ceptance of these test collections, including the ability to compare

his decision should not be taken automatically; the user task and

hould be appropriately matched to the selected test collection and

to be considered in any analysis of the results.

ollections for monolingual English retrieval are the TREC ad hoc

on mainly on newspapers and newswires, along with government

al-purpose and domain-independent, and there is a reasonable

ments are complete.But there are 9 sets of topics (1-450),searched

o which to pick? The best choice for most experiments are the 3

g of 150 topics (numbers 301-450) searched against (mostly) the

provides 150 topics, enough for good statistical analysis, plus this

rms of topic format and relevance judgments.

ad hoc collections need to be used with caution.Topics 1-50, used

, are poor topics with only minimal relevance judgments. Topics

have an expanded format; this may be useful for particular kinds

d query experiments, however the topics themselves were created

th the relevance judgments being done by another person.Topics

cted with reference to the documents, and because they often use

e “easiest” of the TREC topic sets. Topics 201-250 for TREC-4

y or may not be necessary depending on the experiment.

ectionsarealsoavailable(http://trec.nist.gov/data.html),

R and speech, non-English ad hoc collections, and collections for

hereas the Chinese and European language collections are available

ot recommended because of issues with the topic building process),

d CLEF collections for these languages. The Arabic collections

able at NIST, although the second one (2002) is the recommended

meant better pooling. Other collections are also available, usually

data on theTREC web site and a pointer to the documents which

wever that these collections are all specialized, based on possibly

s and these issues should be thoroughly understood (by reading

on characteristics) before using these collections.

ch older small collections? These collections are much too small

hnology; furthermore most of them use abstracts rather than full

e misleading because of this. However one exception to this would

TIME collection) as teaching tools; but here again the beneﬁts of


R MODIFYING EXISTING COLLECTIONS

existing collection or modify it to better ﬁt an experiment.Many of the

fore cannot be subset in terms of topics,however one could concentrate

. For example groups have studied the effects of hyphenation and

ctured Federal Register (FR) documents [199]. Other characteristics

ors (especially in newswires), duplication of information (again mostly

erican English (FT vs. WSJ), evolution over time of news stories, and

y. Additionally some sources of documents provide ﬁelded information

ines to manual indexing terms to the heavily structured data in the

ideas could be used to generate subcollections of documents, however

ere are enough documents in the subset to generate valid results and

in this subsetting need to be considered in result analysis.

is more difﬁcult and (possibly) loses some of the advantages of using

vious modiﬁcation would be to change the relevance judgments, either

ent (doing passage retrieval for example) or by changing the deﬁnition

ance judgments for the ad hoc task are the broadest type of judgments,

contained ANY information about a topic/question was enough to

portant because the perceived deﬁnition of the TREC task/user was

it also was important in terms of creating the most complete set of

The current judgments could be used as the starting point for other

such as removal of “duplicate” documents [24], or the use of graded

or even the measurement of some type of learning effect (the TREC

modiﬁcation is very tricky; in essence a second relevance judgment is

sistency difﬁculties discussed earlier,which may affect the experiment.

oblems, see [81, 203].

EVALUATING NEW AD HOC COLLECTIONS

major step to take; it is costly in terms of time and money and full

his task. However if it is to be done, then the critical thing is that

collection they use be modeled on some real user task and that the

be considered as part of this task. Are the users searching the web

ood reviews, are they searching their company’s intranet for patents,

for information about some speciﬁc type of tree they want to plant?

uires a different set of documents, different types of “topics”, different

ifferent types of metrics to use in evaluation. These decisions need to

ction is built.


ection (http://boston.lti.cs.cmu.edu/Data/clueweb09),

ndidates. Note that any sub-setting of these collections needs to be

web collections were carefully sampled during their construction

eﬂect the new user task design being envisioned. The small web

clues on how to do this, the construction of the WT10g [15] also

see the discussion of the recent “Category B” of the ClueWeb09

ese collections cannot be re-distributed, however one could make

e by others. If completely new document sets need to be collected

can be done in some manner so that the work can be used by

ectual property rights need to be resolved and that the data has to

ts are collected, the next step is the topics. Again the user

odeled; ideally some real topics from a log can be gathered, or

rogate” users such as those used in the TREC/NTCIR/CLEF

to be built to overcome topic variation in performance [188, 189];

minimum, with 50 a more reasonable number. The format of the

r example browsing the web looking for a speciﬁc item may need

the interactive search process. Topics for a speciﬁc domain need

, either by getting domain experts to build the topics (such as the

uch as that done before topic construction by the genomics track

the genomics community. It is equally critical to closely examine

enre, such as the patent tracks in NTCIR,TREC and CLEF.The

ave tackled different pieces of the patent retrieval problem, but all

needs of that community [71]. If multiple languages are involved,

ucted so that there is no bias towards one language [201].

ng the relevance judgments for the topics is again reliant on the

s the user want only document level judgments or are passage (or

eded (such as for genomics and question-answering)? What types

y, graded or other), how many judgments per topic (a major cost

he mechanics of getting the documents to judge (pooling, manual

h as the sampling done in the TREC Million Query Track). Many

ave been tried over the years (see section 6.3 in [147]), each with

hat need to be considered in light of the goals of the test collection.

ds to be done.

some types of validation need to be made. If this collection is used

alidation is needed only to understand any likely biases that affect


UNUSUAL DATA

he assumption that the test collection is a“standard” collection working

many interesting applications have more diverse data. For example, the

nd the video retrieval task) not only worked with multi-media data but

” of the documents. Here, and also in the blog track, these deﬁnitions

eciﬁc task being modeled, with a document deﬁned as the blog post

nts as identiﬁed by a Permalink for opinion ﬁnding, but deﬁned as the

og posts) for the blog distillation task. In both cases the deﬁnition of

output a user might expect to see for these tasks. This was also the

of the question-answering track, and for the passage retrieval part of

that the data to be searched has a major impact on the tasks; this is

EC web/enterprise tracks, but also true of the NTCIR patent tasks.

ltiple ﬁelds and was multi-lingual, leading to a series of different tasks

eval to related newspaper articles in four languages, patent invalidity

s,and patent translation tasks.The ImageCLEF track in CLEF started

mages, where the major emphasis was on using the captions, moved

ultilingual captions where the images became equally important, and

ieval tasks with x-rays. In each of these cases the data determined the

in the TRECvid evaluations. INEX has worked with semi-structured

edia, etc., with the tasks, topics, and metrics highly inﬂuenced by the

DATA COLLECTIONS

track built test collections based on the assumed needs of the TREC

ments (web) were basically what could be obtained,and the topics and

d over the years to allow different research “themes” to be investigated.

ch engines also build test collections, where they have the luxury of

tions, plus other interesting metadata about searching. There is little

done, although Chapter 3 of this lecture has a discussion of the work

ogs.

GIRin2009[122]providesoneexampleofhowthiswasdoneatYahoo.

he entire web (at some given time). The query collection has 22,822

m some population. A similar collection built by Microsoft [198] had

nts taken from interaction logs where the users had provided consent


he Microsoft collection also had relevance judgments (assigned on

those documents the users had examined during the study (note

ubset of the retrieved).

ons emphasized large numbers of topics with shallow relevance

were built speciﬁcally for the given experiments.The metrics used

eriments but they were all either high precision metrics or some

. The huge amount of other metadata collected for the searches

in [175]) allows combining the relevance judgments with other

des based on what percentage of the user population clicked on


ractive Evaluation

ION

are built for users and it is critical to examine how the various in-

(such as those evaluated by the batch retrieval methods) perform in

ly it is important to observe users in an information seeking task and

sers behave in order to devise new retrieval techniques. This chapter

er studies, with an emphasis on those studies aimed at understanding

ng the usability of systems. The chapter starts with a short review of

, followed by a discussion of the evaluation experiences in the interac-

section is a look at some recent user studies,with the goal of examining

experimental design affect the outcome of the ﬁnal evaluations. The

ng the massive log ﬁles from search engines, again with an emphasis

riment, including the selection of which log ﬁles to study, how to ﬁlter

viors to be teased out of that data.

the scope of this chapter is narrow; the recent tutorial (Methods for

mation Systems with Users) by Diane Kelly [111] provides detailed

user studies, data collection and analysis techniques, etc. This lecture

o that tutorial,examining the bridge between evaluation in user studies

ode. Other references for user studies are the recent book by Ingwersen

nts frameworks for evaluation of various information seeking tasks,

mation Processing and Management [28] on evaluation of interactive

K

s involved either indexers or search intermediaries, the only users of

Indeed the Cranﬁeld I experiment discussed in Chapter 1 looked into

dexers, including the knowledge of the subject matter being indexed

ing experience. Both the Medlars study and the Case Western study

draw the clear conclusion that “human error” is the cause of various

dexing stage, or at the search construction stage. However this might

ion” since the issue is not one of error but rather a mismatch of search

ndex terms in this case), a scenario very familiar to today’s searchers.


cision devices [105].There were 800 documents, 72 requests from

ments using library staff. But in this experiment the searching was

r by library science students, who were allowed a maximum of 40

a speciﬁc recall/precision target. A Latin Square design (see [111]

uare designs) was used so that each of the searchers processed each

all of the different indexes, manually recording their searches for

09] emphasized the searchers’ role, this time using different types

complete freedom of search. The aim “was to test the effects of

ance from the user’s viewpoint”, and 9 entry types were selected

rinted indexes. These entry types involved features such as index

nd number of access points, and looked at constructions like lead

bset of 50 requests, and 392 documents from the earlier testing

relevance judgments. The searchers were asked to “imagine that

ituation, to simulate delegated searches by subject experts, where

ed rapidly, and the search subsequently broadened, within a time

h”.The searchers kept extensive records of their search, including

annotated the index entry forms to track what they were using for

d by audio recordings (think-alouds), with all this material then

answers to what types of index entry forms were most useful, but

l processes of the search operations.

t of experiments, and other similar experiments looking at the

ddressing the “real” user of the system. It was assumed that the

tion needs to a human search intermediary, who constructed a

) against some online data base such as MEDLINE or DIALOG.

lored how to better help end users by examining the user/search

ular noting that a user’s initial question was “ambiguous,imprecise,

tem or from a colleague in order to provide an acceptable answer”.

in the late 1970s also wondered how end users might better

eeds either to the intermediary or to a system directly. His ASK

hypothesis was “that an information need arises from a recognized

owledge concerning some topic or situation and that, in general,

cisely what is needed to resolve that anomaly” [23]. He further

s would fall into various classes requiring different types of retrieval

f any information retrieval system would be to decide which types


these ASK structures, creating document structures and doing some

h was the one of ﬁrst to look in detail at these end user information

of [23] gives ﬁve different ASK types based on these interviews, and

what is seen today.

nd problem

em well deﬁned. Information wanted to back up research and/or

Problem not so well deﬁned. Research still at an early stage.

Problems not well deﬁned. No hypotheses underlying research

not well deﬁned. Topics often unfamiliar.

end users were ﬁnally getting a chance to search on their own, with

reating online versions (called OPACs) of their card catalogs. For

m at the University of California, Berkeley had a prototype online

m all University of California campuses available to their library staff

ry patrons in 1981. In 1982 the National Library of Medicine (NLM)

different prototype access systems to their CATLINE online catalog,

0 records at the time of the study. One of the systems, CITE [64, 65]

e input and ranking similar to the SMART system, with the other

oolean system (the ILS system). The user testing was done in three

available from late April through May, and the ILS system available

ver 600 surveys asked users about the amount of information retrieved,

user satisfaction with both the results and the system . Additionally 60

ons used both systems (controlled as to order) before taking the survey.

hree studies of user acceptance yield strong and consistent patterns of

eparately corroborated by the results of technical testing.” CITE was

ly preferred and was used as NLM’s online catalog system for 2 or 3

ul Med (personal communication from Tamas Doszkocs).

e into being, more user studies were done, with user surveys being

log analysis. Tolle [176] reported on work done with logs obtained

he NLM CATLINE (8 weeks of 441,282 transactions from over 11

ciﬁc types of data collected including the terminal identiﬁcation, the

d, user commands, times of starting and ﬁnishing, and the system

e logs was mapped into 11 primary user action states, such as start of

, display of records, access to help functions, and errors that occurred.

construct a state transition matrix, which could be analyzed to answer


hers, and the public OPACs, noting for example that OPAC users

quit searching almost 20% of the time after the ﬁrst error (9%

he error barrier, these users had trouble using the Boolean logic,

which constituted over 50% of the searches.

AC users attracted researchers in the 1980s, in particular a group at

n library. A series of retrieval experiments [132], evaluated within

e Okapi system), took place starting in 1984 [123], examining

user search performance. Note that most subject searching based

t “AND” operator between input terms, causing massive failures

h no records found [196]). Therefore the ﬁrst Okapi experiment

anking based on relative term frequency in the index (a variation

gle terminal in the library included 70 structured interviews (user

hing for, and comments on Okapi), plus transaction logging from

learly isolated. In general users liked the new system, but it was

es were subject searches rather than for speciﬁc items (title/author)

rms per subject search was just over two.

ts [196] investigated stemming and spelling correction since trans-

ystem revealed that more than 25% of the subject searches would

ming, and about 10% of the searches containing unnoticed spelling

m were installed on alternative days on two terminals at the li-

iewed over a one month period. The two versions were a control

ing (including an “s” stemmer and “ed” / “ing” verb stems), and

ith a strong stemmer (Porter), spelling correction and a lookup

classes of related terms. The evaluation involved both observa-

og analysis. The observations were mostly to note problems and

ion times, with the interviews asking about frequency of catalog

earch, the success of the search and any problems encountered.

experimenters to repeat the exact (initial) search, on either of the

th no stemming (the original system). It is worth noting that this

ecessary because earlier pilot studies had shown that most searches

regardless of the system and therefore a direct comparison of the

the CITE system at NLM) would not have been sensitive enough

ults bore this out; even though the weak stemming retrieved more

al searches repeated from the transaction logs, it “rarely turned a

o a success”, and the strong stemming hurt performance as much


d a third system (full) which also included a “shelf browsing” system

mber order. For reasons related to the logistics of dealing with the data

, it was impossible to set up true operational testing, and a laboratory

ects was done instead. The task was to build reading lists for speciﬁc

cts were asked to use the base system for 15 minutes and then use

m (brief demonstrations of these systems were also done before use).

transaction log and transcripts of recorded interviews. Based on the

nsion) system was considered highly acceptable, with the full system

More than two-ﬁfths of records based on query expansion were chosen

fth of those from the full system chosen.

i system moved to the Centre for Interactive Systems Research at City

ond set of experiments using relevance feedback was done [77, 195],

al setting using both the City catalogue and a section of the INSPEC

nd information technology). A terminal with Okapi query expansion

with searches logged and 120 user interviews done in the last three

offered only when searchers had chosen at least two items as relevant;

ogged searches and expansion was used in 31% of those searches, or

hes. For over half of the time it was used, no additional items were

arch interviews revealed that 41 out of the 45 users that did not use

the books they wanted already. However replays of those searches did

ore to be found 50% of the time.

of these experiments? Clearly the best match ranked results from

tter than the Boolean results, although there could be little actual

ming made a small improvement in some searches, with the spelling

w.Relevance feedback worked well for the particular task chosen in the

not heavily used in an operational setting.Note that these experiments

h methodologies proven in a batch mode were then evaluated in an

xception of the single CITE evaluation by NLM). As such they also

ransitioning (and then evaluating) these technologies to real world

een two techniques have to be noticed by users, the speciﬁc tasks being

ese new techniques, and the huge amount of noise involved in user

pics, etc.) can often swamp any signiﬁcant results. All of these issues

testing, as will be seen throughout the rest of this chapter.

rly user experiments, considerable discussion (and work) was being

icular looking at the meaning of relevance, a concept that is central to

eval.The batch experiments, starting back with Cranﬁeld, all carefully


ut relevance in the 1970s. A recent set of papers by him [153, 154]

rveyed other past work in relevance, both in the theoretical sense

f the user studies (second paper).

nted out that the simplifying assumptions made about relevance

reﬂect how users viewed relevance in the real world. The batch

olely on topicality, they were usually binary, each document was

was assumed that judgments did not change over time (or across

s a criticism of the batch evaluations (which clearly were doing

further user experiments to better understand how users actually

then described various user studies that had been done to examine

d a series of studies that looked at various relevance criteria,divided

depth, scope, currency, treatment, clarity

of information objects, e.g., type, organization, representa-

y, accessibility, costs

nformation provided, authority, trustworthiness of sources,

ch: appropriateness to situation or tasks, usability; value in

erstanding, novelty, mental effort

ional responses to information, fun, frustration, uncertainty

credence given to information, conﬁdence

strate the huge variation that can be seen in how users approach

hey judge/value what is found. This variation continues to widen

ted from the simple OPACs to the vast amount of information

rough various social media such as Facebook. It is further multi-

mber and characteristics of users (far beyond the graduate student

user studies!!).

EVALUATION IN TREC

as a batch evaluation program only, there was always interactive

were allowed to be created manually in addition to automatically


ed in routing (or conversely that the machine learning methods were

re importantly how difﬁcult it was to evaluate interactive methods in

hods. Some of this comes from the “normal” difﬁculty in evaluation of

the TREC methodology added major problems, including the issues

submission and how to deal with the natural disagreement between

levance assessors. For an excellent summary of these problems (and

general), see [67], and for more on the various individual results, see

REC proceedings and a special issue of Information Processing and

ve track was more speciﬁcally designed for the interactive research

hoc topics being used and two tasks created (ﬁnd and save the most

nutes or create the “best” ﬁnal query). This helped avoid some of the

the issue of how to deal with the relevance assessment differences.

art, including three commercial search engines, exploring such areas

zation, etc.These papers (included in the TREC-4 proceedings) serve

r studies because they share the common thread of the same task(s).

sized the analysis of the data and the search process by specifying what

ted, such as search logs, timings, use of various search features, etc.

ccess, the interactive research community wanted better ways of com-

ask that was realistic for interactive searching.The TREC-5 task was

ld ad hoc ones), but with the goal of ﬁnding relevant documents that

e topic. The evaluation of the TREC-5 interactive track was separate

h a different pool created from all of the documents submitted for the

used to create lists of unique aspects for each topic, including which

pects, and interactive groups were scored against these lists. Systems

event logging, including query terms, documents judged relevant, etc.

nt for one search, as once again the emphasis was on the search process.

experimental design [113] to allow direct comparison of results across

topics were divided into four blocks, and a common control system

d. Participating groups also tested their own system and the common

he order of the topics/systems to be searched.The goal was to be able

asuring users’ performance on the common control system and on the

ly two groups took part in TREC-5, but the same task (with only six

ms in TREC-6. An analysis of variance model was used to study the

the systems and the various interactions [113]. It was found that there

ll three factors, with the topic effects being the largest.


ous concerns about the logistic requirements both to “waste” time

m and to use so few topics (given the known heavy effects of topic

e common control system was dropped for TREC-7, with each

stituting a control system of their own, but keeping the common

is did not allow strict comparison across systems, it did allow each

to their own control system (unfortunately the differences were

ectual task was continued, with eight ad hoc topics speciﬁcally tai-

easures used were aspectual recall,aspectual precision,and elapsed

tinued the same task, for six topics and seven sites, with generally

mportant to note that whereas the evaluation issues remained the

ed a common task and common measures that allowed interesting

e informally compared among the systems.TREC-9 focused on a

ht challenging questions that users had to answer in ﬁve minutes,

g answer document. The scoring was based on getting an answer

es was probably too short for this task for some topics , but again

results from this task.

ed to emphasize observational studies as opposed to system com-

g a new task for TREC-11. The public web was the data to be

s provided by picking four search tasks (ﬁnding medical informa-

d research for a paper), with eight topics spread across these areas.

n TREC-11, this time using the TREC gov collection rather than

on search engine (Panoptic) was provided,with the task sharpened

a good web site for the answers. Additionally the experimental

EC-6 was used, again allowing for within-system comparison of

n TREC-12 as a subset of the main web track, where the task was

web topics were modiﬁed for better interactive searching.

he end of the formal interactive track in TREC. Just as the batch-

est collections had been useful in improving the underlying search

ack came along at a time when user studies were migrating from

o the much larger web. In addition to all of the results from the

bout what kinds of (TREC) tasks were most useful for system

informative, and what kinds of common data were interesting to

tem/experimental design methodology was developed, modiﬁed,

here was disappointment in the general failure to detect signiﬁcant

es. Dumais [67] concluded her comments on the track by saying

he failure to ﬁnd signiﬁcant effects–it could mean that there are no


reasing the number of tasks per searcher or focusing on subtasks.”

and expense of adding more tasks or searchers, the interactive com-

ove to a speciﬁc subtask. In particular the track looked at the effects

ut the user or topic, including the use of simple interactions, in order

ults (the High Accuracy Retrieval from Documents or HARD track).

ubmitted their results based solely on the standard input topic (similar

ad the opportunity to use additional information from the user (in this

e topic and also would make the ﬁnal judgments). This information

ata contained in the topic, or could be answers to a clariﬁcation form

he second stage was then a submission of a new ranked list modiﬁed

teraction.

k should be considered a pilot, although interesting research was done

o submit for quick assessment (the simple interaction part).The 2004

ics, all including metadata built into the initial topic. In addition to

ds, there were new ﬁelds for metadata-narrative, explaining how the

ed, retrieval-element, specifying whether a full document or a passage

such as news-report, opinion-editorial, etc., geography (U.S. or not),

topic, and related-text.relevant.The relevance judgments for the results

nrelevant, hard relevant (relevant to topic and satisﬁes metadata), and

s not satisfy metadata). Additionally if the granularity requested was

age relevance judgments needed to be made (passages were deﬁned by

ength). In addition to the metadata provided, the participants could

eding 3 minutes or less for completion (most groups used this form for

documents or passages). In general the use of the metadata did not

topics were such that the metadata did not provide useful additional

curred in the elaborate TRECs 1 and 2 ad hoc topics). However some

ain improved results using clariﬁcation forms. Note that several new

ere developed for this track [193] that were later used in the genomics

e HARD track in 2005 had to be more constrained (no metadata and

f funding issues, however the clariﬁcation forms were continued and

but still only require three minutes to complete). Most systems were

e runs, but with a wide variation across topics as to which types of

The HARD track morphed into a subtrack (ciQA) of the Question-

2007 [63]. The tasks in 2006 were similar to the HARD track, but

rms for complex question-answering about relationships. The 2007


appen consistently, partially due to some groups simply not using

problems with the task setup (assessors are not “naive” users after

ple interactions with clariﬁcation forms, etc.).

ARD track was as part of the current legal track. Starting in 2008,

the Enron data and mirrored the actual legal discovery task where

uthority who is the sole person who deﬁnes the task.There were 7

ve task, and the participants were encouraged to interact with the

orth of time) to clarify relevance issues, although other volunteer

ance judgments (which could be appealed to the topic authority).

OF INTERACTIVE EVALUATION

place where interactive evaluation was occurring; there were many

overs some of these, selected both to cover speciﬁc themes and to

n terms of higher level experimental design. They are organized

ally.

ontinuation of work done with the OPACs, i.e., do performance

ations hold when tested interactively. There have been a series of

particular examining if the TREC batch system improvements

for users. Hersh et al. [95] showed that better systems did lead to

owever the differences were not statistically signiﬁcant. This was

r (Turpin and Hersh [178]) investigating why this result occurred.

topics with 25 users and one task (the TREC-8 aspectual retrieval

ded the 8 questions from the TREC-9 question-answering task.

aseline system (tf*idf) and an“improved” system (Okapi),were set

ch of the two tasks.There were 25 users for the TREC-8 instance

g to ﬁnd at least one document for as many aspects of the topic

appropriate randomization as to topics and systems and basically

stem had a 15% better instance recall, this only came from one of

experiment was run using the TREC-9 question-answering task,

to ﬁnd correct answers. This time the “improved” system actually

(-6%).

n made to ﬁnd out what was behind these results. Since the logs

d documents found or saved, the researchers were able to trace how

ctual user queries (instead of the TREC topics). They found that

een better results (precision at 10 was 55% higher, instances at 10

i system, and therefore could issue fewer queries. However, these


system was ignored in the question-answering experiment and at best

ance recall experiment” [178]. An unexpected complication was that

retrieve shorter documents, therefore cutting reading time. However

ers seemed able to do the task well with either system.

these two papers is that only a small number of topics were used.

enough users in a study to detect signiﬁcant differences in results,

pics makes generalization of the results difﬁcult. This illustrates a

retrieval experiments, where logistics/expense usually force a choice

rs or large numbers of topics, even though the performance variation

arge (or larger) than the variation across users. Azzah Al-Maskari

ly 56 users but 56 TREC topics in a task to ﬁnd as many relevant

minutes using two different systems. Because systems have a highly

pics, the systems in this experiment were selected differently for each

hree “comparable” systems for a given topic, the “good” system was the

ision and the “bad” the worst .Table 1 in their paper shows signiﬁcant

n the time taken for the task,the number of relevant documents found,

etc. between the good system and the bad. The second part of their

s into four topic sets based on the size of gaps in batch performance,

ow that as the performance gap grows smaller, so do the differences

e gap needing to be at least 30% in order to see consistently better

hat these experiments considered documents relevant only if the user

C relevance judgments. If only user judgments were used (ignoring

were still signiﬁcant differences with all 56 topics but these disappear

GIR 2010 provides yet another experimental methodology for com-

results. Sanderson et al [148] performed a crowd-sourcing exper-

o were shown paired results from the diversity experiment in the

Topics where ALL of the 19 runs in the diversity track had found

nts were removed, leaving a total of 30 topics in test.The pairs of runs

s submitted to TREC by picking those runs with the same number of

but a minimum difference in average nDCG of 0.1. The users were

,snippet and URL) and asked which set of results they preferred based

full diversity topic (using the full topic had shown inconsistent results

was seen by an average of 8 Turkers and the votes were cumulative for

evel of agreement between the votes and systems’ nDCG performance

r gaps.


ms being compared. Maybe most critical are the speciﬁcs of the

d for task, the amount of interaction allowed, the instructions to

me from (the batch experiment or the users’ themselves), etc.

elated theme and that is the general area of user modeling, where

ng how a particular group of users behave as opposed to modeling

from highly controlled laboratory studies where users are exposed

t variations to wide open observational studies, possibly even in an

Kelly et al. [112] is an example of a very tightly controlled study.

gate how differences in system performance affect users, but also

odology that could be used to isolate speciﬁc issues in how users

81 subjects working with four topics from theTREC 2005 HARD

anking of the relevant documents or the total number of relevant

de a difference in users preferences of systems.The users searched

f four “search engines” (this was appropriately randomized) and

based on their preferences. They entered a single query, read the

de relevance decisions. The four search engines were not actually

of documents in which the ranks or the number of the relevant

e manipulated. For the ﬁrst two studies, there were exactly ﬁve

cuments, but they were ranked from best (ﬁrst ﬁve relevant seen

-10), with two intermediate cases. The third study had different

anging from 6 to 3, but controlled for order. The users were asked

h search and could change these preferences after they had seen all

Note here that the relevant documents were carefully selected by

s that would likely have a high-degree of agreement between the

% was observed). Detailed statistical analysis of the results showed

signiﬁcantly higher user preference, however the total number of

re important than the ranking.

or [158] kept tight control on the “systems” but much less on the

erve more natural user behavior. There were 36 users who each

er a standard system or one of two degraded ones. The standard

a Google search, with the other two being the same list but with

nsistently-low-ranking) one had the ranked list starting with those

d beyond, whereas the ILR one was inconsistent, with different

each query. All users searched on the same topics and were told

who found the most good information sources and the fewest

red, the top 20 documents were displayed in the typical Google


le block using the two different degraded rankings. Users submitted

who graded each selected document as either a good, marginal or bad

pic and the four evaluation measures were the average numbers for

me.The basic results show that users did just as well with the degraded

s by adapting their behavior, such as by issuing more queries (it takes

lts) or by not resubmitting the same query to bad systems. Some of the

n paper discussed earlier apply here, in that there could be many good

owever by lessening the control of the users, more natural phenomena

ational user study with minimal control is a 1992 study by Su [169]

measures were correlated with user satisfaction. The 40 users (library

y came into the library to search for information. They then sat with

es while the search was done (including paying the cost of the search),

t of documents retrieved in order to make relevance decisions. This

onal setting (at least in 1992). Almost 50% of the searches were for

gh recall areas, with the rest being class assignments, grant proposals,

cess was documented, including the time taken for the search, and

ecisions, and there was a 60-minute interview asking the users about

tisﬁed, etc. Table 1 in the paper lists 20 different measures that were

atisfaction, with the highest correlation being the users’ perception of

h results (recall). In general precision was much less important, either

ith only a few documents or because they were willing to look longer

be few relevant.

er study involves investigation of how user groups tackle different

5] studied how domain speciﬁc knowledge affects searching, using 8

C 2001 interactive track (four tasks in healthcare and four in online

e healthcare search experts from medical libraries and four students

ears of online shopping experience. This was an observational study,

ews to collect the data. Analysis of these studies showed that the users

techniques when they were in their expert domain than when outside

etter results in their domain. For example the shopping experts found

healthcare experts, but the healthcare experts found the 9 categories

going to an average of 3.7 reliable sites whereas the shoppers went to

none of them found all 9 categories. An important part of this study

process to determine the characteristics of domain speciﬁc searching.

knew what types of websites to visit in their domain, including speciﬁc


works), and searching with search engines. The two studies differ

he goal of the study. Morris et al [124] did an informal experiment

ng a search engine to ﬁnd answers to an information need.The 12

rch needs, most of which were relatively straight-forward requests

ing a kitchen backsplash”, or "Should I wait for ZuneHD or buy

ey then sent this request out to their Facebook friends (they needed

rted their own searching. When they had ﬁnished searching, they

work and captured a screenshot of the content and timestamps for

three days later. In general they got better information from the

ad added beneﬁts, including sometimes different answers.

] used two laboratory tasks, both of which were research questions

ere speciﬁcally picked not to be advice type questions, such as

d were questions that are complex to search (one question was

yrolysis) play in the debate over carbon emissions?”). Additionally

search strategies, including calling or emailing friends, all the

nd other “socially-generated” sites such as question-answer sites,

ts who worked in two blocks in a balanced experiment, with one

ds and the second using traditional search engines, databases, and

ocial” sites. There were semi-structured interviews, followed by

ategies that were used. One of their ﬁndings was that the results

he highly technical topics generally doing less well for social search

n is a continued look at relevance and the characteristics of relevant

begun looking at end users in libraries, but the information seeking

rry and Schamber [21] compared two user studies that had the

A more traditional one was done with 18 faculty and students

nline search and when presented with results (including full text

tc.) were instructed “to mark any portion of the materials that

nt would or would not pursue.” This was followed by open-ended

ance criteria (989 responses to 242 documents). The other study

with 30 users of weather information in areas such as construction,

n. In this case the information would be used to make planning

create time-lines of information seeking events leading to these

hen looked at three of these events, with the participants asked

entation modes of these sources. In this case the “criteria were

sources or presentations made a difference to respondents in their


ciﬁcity: extent to which information is in depth or focused; is

’s needs

extent to which information is accurate, correct or valid

which information is presented in a clear and well-organized

o which the information is current, recent, timely, up-to-date

to which information relates to real, tangible issues; deﬁnite;

n is provided; hard data or actual numbers are provided

extent to which general standards of quality or speciﬁc qualities

sed on the source providing the information; source is reputable,

nt to which some effort is required to obtain information; some

obtain information

rmation sources: extent to which information or sources of in-

able

t to which information is consistent with or supported by other

he ﬁeld

nt to which the user exhibits an affective or emotional response

sources of information; information or sources of information

ith pleasure, enjoyment or entertainment

rent for each study “appear to be due to the differences in situational

uirements: speciﬁcally, control for source type in the Barry study and

mber study [21]".

ked how relevance criteria have changed in the web searching arena.

ee different tasks and asked 24 participants to indicate the features of

ortant to them for relevance criteria. The researchers were interested

nt features were used in assessments, but also how the different tasks

as no control on the searching (other than time), and a think-aloud

ere used to capture information. All tasks were set within a search

pes: a background search, a decision task, and a listing task. The pre-

for familiarity with task topic, etc., while post-search questionnaires

easiness of task and the participants’ perception of the importance of

es they viewed (both positive and negative aspects). It was found that

sites was the text, including the content and the numbers (especially


), the increased use of numbers for task 2 (the decision task) and

ks 1 and 2. “As far as the scope/depth feature is concerned, its

based on that users required enough information in pages (e.g.,

ﬁcations, guarantees, availability of speakers, etc.) in order to make

ailable choices [177]."

ent [11] also looked at relevance criteria on the web, including

needs taken from the most frequent queries in Yahoo Buzz and

o weeks. Thirty-ﬁve of them were e-commerce searches, whereas

king for items such as IRS tax forms, government jobs, or health

were 17 available criteria, with eight of them adapted from the

] and nine more added speciﬁcally for the e-commerce task. The

2450 results),askingTurkers to select one or more criteria for these

ws that accuracy and validity were most important for both sets of

next (higher for e-commerce as would be expected). Differences

more importance for depth/scope in the non-ecommerce needs,

n the e-commerce ones.

ction differ in the amount of control being exerted during the

users/topics/tasks addressed in the study, with most of the groups

d fewer tasks. This decision allows tighter focus in the analysis,

al. with only three tasks, where they were able to observe how the

decisions. Note that in this study, and also in the Bhavnani study

details in the higher-level experimental design (such as what tasks

re critical in getting useful ﬁnal results. The two crowd-sourcing

ers, and therefore were able to have more conﬁdence in their end

ly because they carefully focused the tasks (and set up the crowd-

esting to contrast the tightly controlled study by Kelly et al. with

ne being able to use tight control to learn “micro” analysis of how

her able to observe how users adapt to badly ranked results. There

iments, including how much control to use, how to balance the

er of tasks, etc., but the key to the success of these various studies

focused initial goals and then determined which issues were most

se goals.

EVALUATION USING LOG DATA

with the log data collected by the various search engines (and

s data is not usually publicly available, however research be-


rch.microsoft.com/en-us/um/people/sdumais/Logs-talk-

al starts with listing the advantages of log analysis, including that the

ded as opposed to being reported or recalled or being the subjective

xperiments. Additionally the large (huge) sample permits subdivision

lution, allowing for very focused experimental design. The disadvan-

hey are not controlled, not annotated with “macro-events”, and there

g why some micro-event has happened. A big challenge also is that

ng serious efforts in focusing the experiment, in cleaning the data, in

understanding and interpreting the results. The papers presented in

ns of how this can be done successfully.

n log analysis is to decide what basic events/measures/counts to track.

IR conference [66] looked into this issue and one of the results was

iling methods of collecting and then using the implicit user behavior

er starts with a description of the embedded add-in for the client that

sers. The goal of the study was to collect both explicit feedback and

allow correlation of the various measures, and 146 internal Microsoft

6 week study.Two types of explicit feedback were gathered, including

f each individual search result visited (“liked it”, “interesting but need

e it”, “didn’t evaluate it”) and session level evaluation (“is this a new

el of satisfaction with the old search”); both of these were handled by

user actions. Tables II and III in the paper list 30 different implicit

the result level (time spent on page, scrolling count, time to ﬁrst click,

er of queries, number of results returned, end action, etc.). The rest

ical methods to combine the implicit feedback to predict the explicit

priate combination methods outperform simple uses of the implicit

measures and features that are collected, and the sophistication with

nalyzed, has exploded since then, with recent papers displaying large

e features now used. One example is the Teevan et al. [175] paper

nformation needs behind similar queries and looking for methods that

in getting better results from these “ambiguous” queries. This study

distinct queries, each input by at least 10 different people. The types

some that are based only on the query itself (such as query length,

me of issuance), some that need the result sets (such as the clicks, the

tory Project) categories or the portion of the urls that end in “.com”)

instances of the same query (such as the number of times the query


informational queries.

e subject of a workshop at the Second ACM International Confer-

ning (WSDM 2009), where a large query log from Microsoft was

9 million entries from one month in 2006, including 6.62 million

s [54] using this data looked at how click data could be combined

dentify “diverse” queries (similar to the ambiguous queries in the

measures used was click entropy, which measures the spread of

y multiple issuers of the same query (higher click entropies mean

nt results). This click entropy was measured across the collection,

tropy accounted for 80.2% of the queries and that 95% of these

The researchers used a subset of the data (queries with 50 or more

clicks) to investigate the correlation between click entropy and

They found little correlation between the query terms having many

WordNet or Wikipedia) and click entropy, however there was a

ize of a Wikipedia article and click entropy, indicating that high

d topics with a need for aspectual retrieval.

also discussed the need to properly partition the logs based on the

uch as partitioning by language, by time, and by the type of device

tudy by White et al.[197] looking at how domain expertise affects

n excellent example of careful data partitioning.They started with

million search sessions collected over a three-month period.The

ned (by automatically classifying the visited pages using the ODP)

mainly) searched in one of four domains (medicine, ﬁnance, legal

n selected the target group of users from this subset as those who

these domains and whose page views contained 1% or more of

his set of users are not necessarily domain experts, and the last

s those users in this set that had visited PubMed, online ﬁnancial

igital Library, noting that three of these cost money and are not

e users had been divided into experts and non-experts, the various

e correlated with the two groups,showing for example that experts

ies and visited more pages in unique domains in a given session.

ioning of the logs, often there also needs to be “cleaning” of those

ot visits, and allowances for anomalies such as data drops, capped

particularly important in studying patterns, such as the study by

n patterns. The goal of this study was to characterize how people

web interaction log, followed by a user survey to identify intent.


basis of the revisitation study. These URLs were binned by different

que visitors (4 bins), median per-user revisits (5 bins) and inter-arrival

e bins to examine for usage patterns. Interesting clusters could be seen

revisits could be porn or spam sites, with slightly different patterns

nce sites, whereas “medium” revisits could mean portals such as bank

arious patterns were then veriﬁed by a user study with 20 volunteers

these URLs and also surveyed on a small selection of their revisits.

rates the power of log studies to pinpoint and verify user behavior. In

clear focus before the experiment on speciﬁc goals and this provided

an” to analyze the data. Because the experiments basically started with

s likely that the results can then be incorporated into new systems for

s.



Conclusion

ION

me thoughts on how to design an experiment, pulling together some

pters. Additionally there is discussion of some very recent issues in

gy and in metrics,and a personal look ahead at some future challenges.

GHTS ON HOW TO DESIGN AN

T

wrote “The Pragmatics of Information Retrieval Experimentation,

ted meant that she had written an earlier article for the Karen Spärck

e 1992 article she discussed the series of decisions that needed to be

tion could start, and most of these ideas are still valid almost 20 years

To test or not to test”, where she said “An experiment should have a

nd, not an end in itself. It is therefore essential that the investigator

f the test, the addition to knowledge that will result from its execution,

has not already been made.” In these days of fast computers, readily

EC data, and a push to publish, it is sometimes easy to forget this.

he success of an experiment is to have such a focused goal that the

nd operationalize, it is “obvious” whether the experiment was a success

d the results can be presented in such a concise fashion that readers

was learned. Note that this applies whether the experiment involves

operational user study or using a test collection. The successful log

3 were strongly focused and therefore were able to draw conclusions,

effects of domain expertise [197] from massive data ﬁles.The Medlars

ad a clear goal even though it was necessarily less focused,and that goal

he recall and precision databases to allow non-biased measurements

similar) evaluations are even less focused, with the data and the task

ments up to the participants. Whereas this allows greater freedom, it

it is not clear what has been learned.

w to operationalize the variables.The documents to be searched need to

n of a document for the particular experiment needs to be determined.


Cvid and the genomics track at TREC have done extensive work

sed the issue of “information representation” because most of her

d. Whereas this does not seem to be a part of today’s experimental

e. The search log experiments rely on many types of information,

ried (see [175] for an excellent example of this). In test collection

the information may be encoded in the system, such as the use of

re difﬁcult to understand the underlying effects of data.

utcliffe discussed was the users. Here she was looking particularly

es of categories:

scientist, businessperson, child;

pational, educational, recreational;

needed – aid in understanding, deﬁne a problem, place a

sign a strategy, complete a solution;

tion need – immediate, current, future.

not just for user studies, they also are the basis of user models and

part of all experiments in the operationalization of other variables.

urce (and format) of the user questions/queries/search statements.

termining not only the types of questions that are being used for

ons are collected/assembled/created. One of the important lessons

ge evaluations was how to build questions that faithfully mirrored

d query in their language and also reﬂected the type of questions

language area [201]. Likewise the format of the questions needs

within a given application; good examples here are the TRECvid

patents in NTCIR.

asure the performance and how to analyze the data. This involves

uch that the user and the application will be cleanly modeled. It

re) the choice of how to measure the correct answers or relevance

closely related, although not tightly coupled. For example the

igh recall user where relevance was taken to mean any document

Note that this would reasonably include “duplicate” information

newspaper reporter) might well regard duplicate information as

meant that a document was either useful or not (relevant or not),

tionally the TREC ad hoc results were always considered ranked

cision metric. However other high recall users could be patent


nhighrecallofdifferentaspectsofagivensearch,suchasﬁndingbooks

g recipes. This user model occurred in the various TREC interactive

was used, and in the genomics track. It also was behind the diversity

goal was to identify the various subtopics of the query.The deﬁnition

hanged, incorporating the novelty of the information. Other possible

the order the information is presented, the perceived validity of the

information, in fact any of the relevance criteria discussed in Chapter

of relevance will require different or multiple metrics, such as graded

(see [46] for an example).

ly considered low recall, where the goal is to get the user something

trics like success@1,or nDCG measured at the top 10 or 20 documents

all use graded relevance judgments, often with ﬁve or more grades,

ese ﬁne grades important to users. Another issue involving web search

ial relevant documents and this scaling issue has led to new sampled

ew metrics) in the recent TREC web tracks (see Chapter 2).

ical decision of how to analyze the data (and what to present to the

l/precision averages showing small differences from baselines are not

ld question what is learned from that. Use of statistical testing can

150, 159]) but has not been heavily adopted by the community. A

to actually look at the data, examining issues like how many of the

rovement vs. lack of improvement, how large that improvement was,

this occurred. The Turpin and Hersh paper [178] exploring why user

ame results as batch evaluations is an excellent example of this, and

ome of the TREC research papers contribute more to understanding

rieval than these large tables of averages. However plans for this type

rated in the experimental design early on to ensure that appropriate

NT ISSUES IN EVALUATION OF

ON RETRIEVAL

IR and CIKM proceedings shows three main categories of current

rst of these categories is deeper examination of the implications of

d methodologies.

zed the Geometric Mean Average Precision (GMAP) used in the

d that the difference between GMAP and MAP “has to do solely with

f the effectiveness distribution over topics”. In particular, the GMAP


hers to observe more clearly how their systems are operating (such

vance feedback becomes more questionable when measured using

pers re-examining the use of statistical methods. Sanderson and

[149] showed some problems with the use of Kendall’s tau rank

e was the dependency of the threshold on the range of scores in the

hees’ poster at SIGIR 2009 [188] revisited the issue of how many

signiﬁcant differences across systems. In this paper she noted that

small number of results that will be falsely declared signiﬁcant (well

pe I error) and that researchers should validate results on multiple

cker et al. in SIGIR 2009 [159] compared different signiﬁcance

effects of varying sample sizes on the results.

for research is the quality/consistency of the relevance judgments

his has been investigated starting back with the SMART system.

at SIGIR 2008 [16] not only summarizes this work but adds new

RO assessors (the gold standard) in the TREC 2007 Enterprise

ience communicators outside of CSIRO) and TREC participants

tter being shown as not as reliable. A recent paper by Carterette

n to examine the effects of different types of assessors (optimist vs.

n Query collection; one of the goals of this paper was to speculate

etely untrained assessors, such as in crowdsourcing.

nt evaluation papers involves better/easier ways of building test

in TREC) have shown problems with the very large collections,

judgments are not “complete” and therefore that the collection

rms of reusability by systems very different from those used in

Query track was started in TREC 2007 to investigate better ways

nd Carterette et al. in SIGIR 2008 [43] presented an analysis of

more effective and efﬁcient to judge fewer documents using more

arterette et al. in SIGIR 2010 [42] looked at a methodology for

s at this large scale and then being able to validate (or not) their

h incomplete judgments is to use metrics less sensitive to incom-

of these and He et al. in SIGIR 2008 [90] compared the use of

fAP and nDCG) in effectively training systems as the levels of

was the least effective).


gather assessments, investigating the use of incentives to entice players

e use of Amazon Mechanical Turk or other crowdsourcing platforms

nso and Mizzaro’s poster at SIGIR 2009 showing the use of Turkers

a in e-commerce [11], and another paper [10] by the same group in a

asking if TREC assessors could be replaced by Turkers!! SIGIR 2010

d Crowdsourcing for Search Evaluation, and a paper by Sanderson

ng to compare the effectiveness of different retrieval systems where

s of paired results.

ecent evaluation papers involves better ways of “mirroring” the user

and Kekäläinen developed the nDCG metric [100] based on graded

as seen heavy use in commercial search engine evaluations. In 2008

tric to handle sessions of multiple queries, clearly better modeling real

m. This same theme has been continued in the TREC Sessions track

re has always been work on multi-query sessions in user studies, and

rch engine logs, however trying to simulate this in batch evaluations

modeling of users in batch evaluations are the simplistic relevance

ea of novelty and diversity has inspired the diversity task in the TREC

Clarke et al.in SIGIR 2008 [46] proposing a framework for evaluation

uding a new metric based on nDCG. Additionally there have been

IR: “Beyond Binary Relevance: Preferences, Diversity, and Set-Level

d “Redundancy, Diversity, and Interdependent Document Relevance”

. [179] at SIGIR 2009 suggested that evaluations could better model

ies of the documents in the judgment process and suggested a metric

is. Whereas work on sessions has concentrated on the multiple query

he process of deciding “where to click” has only be investigated in

SIGIR 2010 ( “Simulation of Interaction: Automated Evaluation of

ded on how users could be modeled for batch evaluation efforts.

engine community has enormous amounts of logging data that can

users. Chapter 3 discusses some of the recent papers in this area.

Craswell presented a paper at SIGIR 2010 [129] that examined the

from different search engines as a new way of evaluation without

d that interleaving was better at detecting very small differences in

at small differences do add up) than a more traditional test collection

precision tasks.


laced a high importance on evaluation, and this has allowed great

f improvements. However the downside of this is that areas that

to attract much research. Therefore the development of new and

s and metrics is a critical key to attacking future challenges. What

s in information retrieval that hopefully will see more research in

tter ways of evaluating them.

erstanding of how our retrieval processes/search engines actually

proposed by Spärck Jones and van Rijsbergen [166] envisioned a

, different indexing schemes, etc., with the goal of seeing how the

diﬁed to deal with these differences. Although this collection was

TREC and other evaluations tends to indicate that results from

domain and the type of textual documents (newspapers vs. medical

ments).The various indexing schemes (free vs. controlled terms vs.

een mainly ignored in experimentation today. The requests now

ifferent users but have (unfortunately) never reached the 700 to

be useful to do today?

er understanding of our systems could be envisioned at two levels.

igate how the systems are working in the simple task of retrieval

ad hoc task. Once this is better understood, a second level would

e as the systems move into truly diverse environments, such as

ocuments (blogs vs. web vs. mixed media such as in the legal track

ations.

tter metrics and methodology for diagnostics.Robertson’s analysis

stion of using different metrics to tease apart system behavior is

cs. The work with metrics likely needs to be tightly coupled with

wever. A fruitful area for this investigation would be the causes of

mance across topics, and there have been multiple papers trying to

nce on a given topic (for example [40, 62] and another lecture in

Difﬁculty for Information Retrieval by David Carmel and Elad

r, a major source of improvement would be the ability to mod-

basis. This unfortunately turns out to be quite difﬁcult; a 6-week

f 2003 tried to determine the characteristics of topics (from the

be helped by the use of relevance feedback, and could not reach

hop employed eight of the best retrieval systems, along with their

ontrolled experiments with the TREC data and to do many hours

using a specially-built tool. The results were often surprising and


s and topics. An attempt was made to do this with statistics [18], again

the interactions are. Guiver et. al [75] showed that small sets of topics

ffectiveness, with those sets composed of different topics depending

ed. This implies that there may be some way of generalizing topic

ng able to test more speciﬁc components.

e more research is needed is better integration of users into batch

ated the logistical difﬁculties of doing user studies at a large enough

variation and user variation issues. Small controlled studies are hard

ixed results from applying successful batch methodologies into user

tance of better understanding of how users interact with the systems.

es are able to track, and often generalize, these interactions, but may

of how to apply their user models.

would be a type of user simulation within a batch environment. This

servations of users doing a simple, well-deﬁned task. The goal would

sues that could then be modeled in a batch environment. Results of

user-tested as proof of “improvements”. For example the TREC-3

nd as many relevant documents as possible in up to 30 minutes, with

multiple users). There were 9 systems that tackled this task, usually

help” the users with the task. Suppose that from this user study it could

ﬁc types of document surrogates (or document space representations)

could then be further developed within a batch environment, with

esting.This example may be too simplistic (or naive), however the goal

he batch process is too important to just ignore. Moving forward will

methodologies and metrics. There is some promising recent work on

hop at SIGIR 2010 [14], and the new Sessions track at TREC. This

e user study community and the batch evaluation community.

ld be to take more advantage of the facts that we already know based

example of this would be to try different categories of relevance.

criteria such as novelty, validity, usability, fun, etc. that could be the

on). Some work has been done in novelty, and the criteria of validity

n the web world by the use of links. Research on removing spam is

part of usability. A second source for relevance criteria is the Alonso

e type of web searching. A major barrier to research in these different

aluation in a batch mode, that is, how do we effectively model these


ation,and speech recognition are also needed.WhereasTREC and

oss areas, such as with cross-language information retrieval, or the

ts,this is usually done by connecting components.There have been

-answering components dealing with poor retrieval components,

ponents feeding into information retrieval components, but little

ese components.

ogram has made several efforts in this area. One was the investi-

ranslation accuracy on patent retrieval [72], and another has been

stion-answering results to the retrieval results in the more recent

cases, and in other such research, the goal has been to measure

better integrate the components. But suppose that it was known

(words) were poor, would it be useful to integrate that knowledge

not as information retrieval seems rather tolerant of “errors” in

n involved a key word in the query, is there a way to avoid poor

translations. As another example, results from poor retrieval can

ng results, but can retrieval systems be more tightly coupled into

ow a true interaction, such as getting more documents if it is ob-

en found (several groups have tried this with success). Certainly

these technologies have very distinct communities, with different

eria, and often different publishing venues. One of the goals of

spanned communities (such as the TREC speech retrieval, the

ge retrieval and the multiple QA evaluations) has been to bring

his has generally not led to better integration.

teraction” occurs with other related areas, such as database tech-

ology. Effective use of metadata and work with semi-structured

ed by database technology. The various image and video retrieval

d text and speech, and here there has been more integration, with

of the retrieval process. It is not clear if this has happened because

unities involved or because the evaluations have encouraged spe-

uiring runs using a single component for comparison. Maybe this

th more success in other cross-community evaluations or maybe

Jones’s cascading evaluations [168] will emphasize the effects of

urage tighter integration.

research in evaluation addresses a much broader issue.The OPACs

ted our ﬁeld have exploded in many directions. A look at the vast

commercial search engines, such as e-commerce and people/URL


d this is just the web; what about enterprise search, patent retrieval,

e-discovery? How do we go about producing meaningful evaluations

vironment?

g very speciﬁc tasks within a speciﬁc environment. The TREC tracks

ciﬁc issues in (plain text) retrieval, such as the robust track, but mostly

vironments,giving us the legal track,the genomics track,the enterprise

EX and FIRE have also taken this approach.This at least has allowed

n these different areas and has made some initial exploration of effective

egies. Possibly this is all that can be done, and it is certainly useful as

nces for this work.

we) try to learn more general principles about the retrieval process

his process)? Are there commonalities across these tasks and applica-

improve retrieval in general or at least allow faster/better adaptation of

nments? A workshop (MINDS [38]) attempted to ﬁnd some answers,

me of the challenges requiring research (and evaluation) that occur in

a: spam, audio, video, slides, notes, images, metadata/structure

text: diverse tasks such as ﬁnding, learning, monitoring, com-

ng

list: information analysis and organization

ng of what the user is actually doing

hallenges, not the answers, but it is critical that we as a community

and necessarily our evaluation methodologies, to tackle today’s world.

GIR 2009 “Workshop on The Future of IR Evaluation” had good

nt opinions as to what could be done as there were workshop attendees.

earch because our evaluation methodology is too restricted is not an



g

p y

Truth ... American Documentation, 6:56, 1955. Cited on page(s) 2

van, and Susan T. Dumais. Large Scale Analysis of Web Revisita-

ngs of the Twenty-Sixth Annual SIGCHI Conference on Human Factors

pages 1197–1206, 2008. DOI: 10.1145/1357054.1357241 Cited on

Cleverdon. A Report on aTest of the Index of Metallurgical Literature

iversity. Aslib Cranﬁeld Research Project, Cranﬁeld, England, 1963.

Hall, K.H. Lavelle, and J.M. Tracy. Comparative Evaluation of Index

f Electrical Engineers, London, England, 1970. Cited on page(s) 22

rk Sanderson, and Paul Clough. The Good and the Bad System: Does

dict Users’Effectiveness. In Proceedings of the 31st Annual International

e on Research and Development in Information Retrieval, pages 59–66,

390334.1390347 Cited on page(s) 67

V. Pavlu, E. Kanoulas, and B. Carterette. Million Query Track 2008

ngs of the Seventeenth Text REtrieval Conference (TREC 2008), 2008.

B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million Query

In Proceedings of the SixteenthText REtrieval Conference (TREC 2007),

ited on page(s) 45

RD Track overview of the TREC 2004 High Accuracy Retrieval from

dings of the Thirteenth Text REtrieval Conference (TREC 2004), pages

page(s) 65

RD Track overview of the TREC 2005 High Accuracy Retrieval from

dings of the Fourteenth Text REtrieval Conference (TREC 2005), pages

page(s) 65

ano Mizzaro. Can We Get Rid of TREC Assessors. In Proceedings of

op on the Future of IR Evaluation, pages 15–16, 2009. Cited on page(s)


72115 Cited on page(s) 72, 81, 83

l Pavlu. A Practical Sampling Strategy for Efﬁcient Retrieval

ort, College of Computer and Information Science, Northeastern

page(s) 46

avlu, and Emine Yilmaz. A Statistical Method for System Eval-

udgments. In Proceedings of the 29th Annual International ACM

ch and Development in Information Retrieval, pages 541–548, 2006.

48263 Cited on page(s) 46

ärvelin, Jaap Kamp, and Mark D. Smucker. Report on the SI-

the Simulation of Interaction.

SIGIR Forum, 44(2):35, 2010.

24484 Cited on page(s) 83

, and David Hawking. Engineering a Multi-Purpose Test Collec-

eriments. Information Processing and Management, 39(6):853–871,

6-4573(02)00084-5 Cited on page(s) 38, 54

Ian Soboroff,PaulThomas,Arjen P.de Vries,and Emine Yilmaz.

e Judges Exchangeable and Does It Matter. In Proceedings of the

CM SIGIR Conference on Research and Development in Information

2008. DOI: 10.1145/1390334.1390447 Cited on page(s) 39, 80

omas, Peter Bailey, Nick Craswell, , and A.P. de Vries. Overview

Track. In Proceedings of the Seventeenth Text REtrieval Conference

on page(s) 39

d Nien-Fan Zhang. Blind Men and Elephants: Six Approaches to

Retrieval, 1:7–34, 1999. DOI: 10.1023/A:1009984519381 Cited

d B.K. Gray. Retrieval Experiments based on Chemical Abstracts

ort No. 2, UKCIS, Nottingham, England, 1974. Cited on page(s)

d D.W. Oard. TREC 2006 Legal Track Overview. In Proceedings

val Conference (TREC 2006), pages 79–98, 2006. Cited on page(s)


malous States of Knowledge as a Basis for Information Retrieval. The

formation Science, 5:133–143, 1980. Cited on page(s) 58

y, and H.M. Brooks. Ask for Information Retrieval: Part II. Results of

of Documentation, 38:145–164, 1982. DOI: 10.1108/eb026726 Cited

ustin Zobel. Redundant Documents and Search Effectiveness. In

ACM CIKM International Conference on Information and Knowledge

6–743, 2005. DOI: 10.1145/1099554.1099733 Cited on page(s) 53

mportant Cognitive Components of Domain-Speciﬁc Search Knowl-

the Ninhth Text REtrieval Conference (TREC-9), pages 19–26, 2001.

Relevance. Journal of the American Society for Information Science, pages

979. Cited on page(s) 28

. Why are Online Catalogs Hard to Use? Lessons Learned from

Studies. Journal of the American Society for Information Science, 37:387–

097-4571(198611)37:6%3C387::AID-ASI3%3E3.0.CO;2-8 Cited

Ruthven. Evaluation of Interactive Information Retrieval Systems.

and Management, 44:1–142, 2008. DOI: 10.1016/j.ipm.2007.03.006

ble, and C. Peters. Cross-Language Information Retrieval (CLIR)

roceedings of the Eighth Text REtrieval Conference (TREC-8), pages

page(s) 37

2002 – Overview of Results. In Evaluation of Cross-Language Infor-

d Workshop of the Cross-Language Forum, pages 9–27. Springer LNCS

007/978-3-540-45237-9_2 Cited on page(s) 49

urrent IR Engines Fail. Information Retrieval, 12(6):652–665, 2009.

009-9103-2 Cited on page(s) 82


48284 Cited on page(s) 45

oorhees. Retrieval System Evaluation. In TREC: Experiment and

Retrieval, chapter 3. The MIT Press, 2005. Cited on page(s) 34

M. Voorhees. Evaluating Evaluation Measure Stability. In Proceed-

rnational ACM SIGIR Conference on Research and Development in

s 33–40, 2000. DOI: 10.1145/345508.345543 Cited on page(s)

M. Voorhees. Retrieval Evaluation with Incomplete Information.

nnual International ACM SIGIR Conference on Research and Devel-

eval, pages 25–32, 2004. DOI: 10.1145/1008992.1009000 Cited

in Relevance Judgments and the Evaluation of Retrieval Perfor-

ing and Management, 28(5):619–627, 1992.

92)90031-T Cited on page(s) 28

and I. Soboroff. The TREC 2006 Terabyte Track. In Proceedings

eval Conference (TREC 2006), pages 128–141, 2006. Cited on

, Charles L. A. Clarke, Susan Dumais, David A. Evans, Mark

ng Zhai.

Meeting of the MINDS: An Information Retrieval

Forum,41(2):25–34,2007.DOI: 10.1145/1328964.1328967 Cited

Effectiveness of Feedback Strategies on Collections of Differing

port ISR-18 to NSF, chapter IX. Cornell University, Ithaca, N.Y,

ov,Adam Darlow,and Dan Pelleg.What Makes a Query Difﬁcult?

Annual International ACM SIGIR Conference on Research and De-

etrieval, pages 390–397, 2006. DOI: 10.1145/1148170.1148238

lan, and Ramesh K. Sitaraman.

Minimal Test Collections

In Proceedings of the 29th Annual International ACM SIGIR

d Development in Information Retrieval, pages 268–275, 2006.

48219 Cited on page(s) 46


9.1835541 Cited on page(s) 80

u Pavlu, Evangelos Kanoulas, Javed A. Aslam, and James Allan. Eval-

of Queries. In Proceedings of the 31st Annual International ACM SI-

arch and Development in Information Retrieval, pages 651–658, 2008.

4.1390445 Cited on page(s) 80

Soboroff. The Effect of Assessor Error on IR System Evaluation. In

Annual International ACM SIGIR Conference on Research and Develop-

rieval, pages 539–546, 2010. DOI: 10.1145/1835449.1835540 Cited

Gordon V. Cormack, Thomas R. Lynam, Chris Buckley, and Donna

ocuments and Terms. Information Retrieval, 12(6):680–694, 2009.

009-9105-0 Cited on page(s) 83

Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin

her, and Ian MacKinnon.

Novelty and Diversity in Informa-

ion.

In Proceedings of the 31st Annual International ACM SIGIR

and Development in Information Retrieval, pages 659–666, 2008.

4.1390446 Cited on page(s) 51, 79, 81

ler, and I. Soboroff. The TREC 2005 Terabyte Track. In Proceedings

REtrieval Conference (TREC 2005), pages 109–119, 2005. Cited on

ort on the First Stage of an Investigation into the Comparative Ef-

ystems. Aslib Cranﬁeld Research Project, Cranﬁeld, England, 1960.

rt ontheTesting and AnalysisofanInvestigationinto the Comparative

Systems. Aslib Cranﬁeld Research Project, Cranﬁeld, England, 1962.

Effect of Variations in Relevance Assessments in Comparative Ex-

dex Languages. Cranﬁeld Library Report No. 3, Cranﬁeld, England,

33

M.Keen. Factors Determining the Performance of Indexing Systems,

slib Cranﬁeld Research Project, Cranﬁeld, England, 1966. Cited on


ﬁcance of the Cranﬁeld Tests on Index Languages. In Proceedings

ational ACM SIGIR Conference on Research and Development in

s 3–12, 1991. DOI: 10.1145/122860.122861 Cited on page(s) 2

son, Murad Adouammoh, Sergio Navarro, and Monica Paramita.

nalysing Query Diversity. In Proceedings of the 32nd Annual Inter-

erence on Research and Development in Information Retrieval, pages

145/1571941.1572102 Cited on page(s) 74

arch Length: A Single Measure of Retrieval Effectiveness Based

tion of Retrieval Systems. American Documentation, pages 30–41,

02/asi.5090190108 Cited on page(s) 25, 36

n of Relevance for Information Retrieval. Information Storage and

DOI: 10.1016/0020-0271(71)90024-6 Cited on page(s) 28

rles L. A. Clarke, Christopher R. Palmer, and Samuel S. L. To.

(MultiText Experiments for TREC-6). In Proceedings of the Sixth

TREC-6), pages 303–320, 1998. Cited on page(s) 34

007 Spam Track Overview. In Proceedings of the Sixteenth Text

C 2007), pages 123–131, 2007. Cited on page(s) 43, 44

es, and Ian Soboroff. Overview of TREC 2005 Enterprise Track.

enth Text REtrieval Conference (TREC 2005), pages 17–24, 2005.

Hawking. Overview of TREC 2002 Web Track. In Proceedings of

l Conference (TREC 2002), 2002. Cited on page(s) 38

Hawking. Overview of TREC 2004 Web Track. In Proceedings of

al Conference (TREC 2004), pages 89–97, 2004. Cited on page(s)

hou, and W.B. Croft. Predicting Query Performance. In Proceed-

rnational ACM SIGIR Conference on Research and Development in

s 299–306, 2002. DOI: 10.1145/564376.564429 Cited on page(s)


rom Research to Application: the CITE Natural Language System.

pment in Information Retrieval, pages 251–262, 1982.

6350 Cited on page(s) 59

d Barbara A. Rapp. Searching MEDLINE in English: a Prototype

tural Language Query, Ranked Output, and Relevance Feedback. In

Annual Meeting of the American Society for Information Science, pages

on page(s) 59

Bharat, Thorsten Joachims, and Andreas Weigend. SIGIR 2003

plicit Measures of User Interests and Preferences. SIGIR Forum, pages

1145/959258.959266 Cited on page(s) 73

Nicholas J. Belkin. The TREC Interactive Tracks: Putting the User

Experiment and Evaluation in Information Retrieval, chapter 6. The

d on page(s) 63, 64

ay Kairam, and Peter Pirolli. Do Your Friends Make You Smarter?

trategies in Online Information Seeking. Information Processing and

9–692, 2010. DOI: 10.1016/j.ipm.2009.12.001 Cited on page(s) 70

of Two New Experimental Collections in Computer and Information

extual and Bibliographic Concepts. Technical Report TR 83-561,

mputing Science Department, 1983. Cited on page(s) 19

rnawat, Mark Myland, Susan Dumais, and Thomas White. Evalu-

s to Improve Web Search. ACM Transactions on Information Systems,

DOI: 10.1145/1059981.1059982 Cited on page(s) 73

Iwayama, and Noriko Kando. Introduction to the special issue on

rmation Processing and Management, 43:1149–1153, September 2007.

006.11.004 Cited on page(s) 48, 54

Utiyama, Mikio Yamamoto, and Takehito Utsuro. Evaluating Effects

n Accuracy on Cross-Lingual Patent Retrieval. In Proceedings of the

nal ACM SIGIR Conference on Research and Development in Information

75, 2009. DOI: 10.1145/1571941.1572072 Cited on page(s) 84

uzanne, and E.M.Voorhees. The TREC Spoken Document Retrieval

y. In Proceedings of the Eighth Text REtrieval Conference (TREC-8),

Cited on page(s) 36


aro, and Stephen Robertson. A Few Good Topics: Experiments

r Retrieval Evaluation. ACM Transactions of Information Systems,

/1629096.1629099 Cited on page(s) 83

ork on the Organisation of Materials in a Special Library. American

1956. DOI: 10.1002/asi.5090070408 Cited on page(s) 3

ieu and Stephen Walker. An Evaluation of Automatic Query

brary Catalogue. Journal of Documentation, 48(4):406–421, 1992.

Cited on page(s) 61

view of the Second Text REtrieval Conference (TREC-2).

ond Text REtrieval Conference (TREC-2), pages 1–20, 1994.

94)00047-7 Cited on page(s) 32, 33

of the Third Text REtrieval Conference (TREC-3). In Overview

Conference (TREC-3) [Proceedings of TREC-3.], pages 1–20, 1995.

of the Fourth Text REtrieval Conference (TREC-4). In Proceed-

rieval Conference (TREC-4), pages 1–23, 1996. Cited on page(s)

w of the TREC 2002 Novelty Track. In Proceedings of the Eleventh

TREC 2002), pages 46–56, 2002. Cited on page(s) 53

Buckley. Overview of the Reliable Information Access Workshop.

6):615–641, 2009. DOI: 10.1007/s10791-009-9101-4 Cited on

Buckley. Overview of the Reliable Information Access Workshop.

6):615–641, 2009. DOI: 10.1007/s10791-009-9101-4 Cited on

I Relevance Assessments:a Critical Evaluation. Library Quarterly,

0.1086/619960 Cited on page(s) 8

ons in Relevance Assessments and the Measurement of Retrieval

the American Society for Information Science, 47(1):37–49, 1996.

-4571(199601)47:1%3C37::AID-ASI4%3E3.3.CO;2-I

Cited


ick Craswell. The Very Large Collection and Web Track. In TREC:

tion in Information Retrieval, chapter 9. The MIT Press, 2005. Cited

aul Thistlewaite. Overview of TREC-6 Very Large Corpus Track. In

Text REtrieval Conference (TREC-6), pages 93–107, 1998. Cited on

Voorhees, and Nick Craswell. Overview of TREC-8 Web Track. In

h Text REtrieval Conference (TREC-8), pages 131–151, 2000. Cited

onald, and Iadh Ounis.

Retrieval Sensitivity under Training us-

es.

In Proceedings of the 31st Annual International ACM SIGIR

and Development in Information Retrieval, pages 67–74, 2008.

4.1390348 Cited on page(s) 80

n, J.R. Baron, and D.W. Oard. Overview of the TREC 2009 Legal

f the Einhteenth Text REtrieval Conference (TREC 2009), 2009. Cited

iraju,L.Ross,A.M.Cohen,D.F.Kraemer,,P.Johnson,and M.Hearst.

05 Genomics Track. In Proceedings of the Fourteenth Text REtrieval

5), pages 25–50, 2005. Cited on page(s) 40

J. Yang, R.T. Bhupatiraju, P. Roberts, and M. Hearst. Overview of

s Track. In Proceedings of the Thirteenth Text REtrieval Conference

3–24, 2004. Cited on page(s) 40

tivityattheTextRetrievalConference(TREC).InformationProcessing

):365–541, 2001. DOI: 10.1016/S0306-4573(00)00052-2 Cited on

ew Turpin, Susan Price, Benjamin Chan, Dale Kraemer, Lynetta

Olsen. Do Batch and User Evaluations Give the Same Results. In

Annual International ACM SIGIR Conference on Research and Develop-

trieval, pages 17–24, 2000. DOI: 10.1145/345508.345539 Cited on


Algorithms: A Case Study for Detailed Evaluation. Journal of the

ation Science, 47(1):70–84, 1996.

-4571(199601)47:1%3C70::AID-ASI7%3E3.3.CO;2-Q

Cited

in. The Turn: Integration of Information Seeking and Retrieval in

ht, The Netherlands, 2005. Cited on page(s) 57

Kekäläinen. Ir Evaluation Methods for Retrieving Highly Relevant

of the 23rd Annual International ACM SIGIR Conference on Research

ation Retrieval, pages 41–48, 2000. DOI: 10.1145/345508.345545

Kekäläinen. Cumulated Gain-Based Evaluation of IRTechniques.

22–446, October 2002. DOI: 10.1145/582415.582418 Cited on

Price, Lois M. L. Delcambre, and Marianne Lykke Nielsen. Dis-

Based Evaluation of Multiple-Query IR Sessions. In Proceedings of

nconferenceonAdvancesininformationretrieval,ECIR’08,pages4–

08. Springer-Verlag. DOI: 10.1007/978-3-540-78646-7_4 Cited

ocument Retrieval Systems – Optimization and Evaluation. Sci-

NSF, Cambridge, Massachusetts, 1966. Cited on page(s) 13

e of Scale on Relevance Judgments. Information Storage and Re-

: 10.1016/0020-0271(68)90002-8 Cited on page(s) 28

ilic-Frayling, and Jamie Costello. Towards Methods for the Col-

lity Control of Relevance Assessments. In Proceedings of the 32nd

M SIGIR Conference on Research and Development in Information

2009. DOI: 10.1145/1571941.1572019 Cited on page(s) 81

Aberystwyth Index Languages Test.

Journal of Documentation,

0.1108/eb026547 Cited on page(s) 58

Length. In Scientiﬁc Report ISR-13 to NSF, chapter V. Cornell

67. Cited on page(s) 16

rameters. In Scientiﬁc Report ISR-13 to NSF, chapter II. Cornell

67. Cited on page(s) 13


formance of Nine Printed Index Entry Types. British Library Report

ales, 1972. Cited on page(s) 58

gger. Report of an Information Science Index Languages Test. British

Aberystwyth, Wales, 1972. Cited on page(s) 22

for Evaluating Interactive Information Retrieval Systems with Users.

in Information Retrieval, 3:1–224, 2010. DOI: 10.1561/1500000012

nd Chirag Shah. Effects of Postition and Number of Relevant Docu-

ers’ Evaluations of System Performance. ACM Transactions of Infor-

9, 2010. DOI: 10.1145/1740592.1740597 Cited on page(s) 68

ul Over. Comparing Interactive Information Retrieval Systems Across

teractive Track Matrix Experiment. In Proceedings of the 21st Annual

GIR Conference on Research and Development in Information Retrieval,

DOI: 10.1145/290941.290986 Cited on page(s) 63

nastasiosTombros. Evaluating XML Retrieval Effectiveness at INEX.

–57, 2007. DOI: 10.1145/1273221.1273225 Cited on page(s) 50

ation of the MEDLARS Demand Search Service. National Library

on, D.C., 1968. Cited on page(s) 9, 10, 77

E. Fox, and C. Buckley. The SMART Lab Report. SIGIR Forum,

: 10.1145/263868.263870 Cited on page(s) 11, 16, 18, 19, 20

alton.

Relevance Assessments and Retrieval System Evaluation.

R-14 to NSF, chapter III. Cornell University, Ithaca, N.Y, 1968.

271(68)90029-6 Cited on page(s) 16, 18, 33

ert E. Schapire, James P. Callan, and Ron Papka. Training Algo-

Classiﬁers. In Proceedings of the 19th Annual International ACM SI-

arch and Development in Information Retrieval, pages 298–306, 1996.

243277 Cited on page(s) 44

cal Approach to Mechanized Encoding and Searching of Literary

urnal, October:309–317, 1957. DOI: 10.1147/rd.14.0309 Cited on


roceedings of the TIPSTER Text Program

Phase I, 1994. Morgan

San Mateo, California. Cited on page(s) 27

Novak, Hang Cui, and Srihari Reddy. Building Enriched Doc-

ng Aggregated Anchor Text. In Proceedings of the 32nd Annual

Conference on Research and Development in Information Retrieval,

: 10.1145/1571941.1571981 Cited on page(s) 55

enner, and Stephen Walker. Designing an Online Public Acess

gue on a local area network. Library and Information Research

h Library, 1985. Cited on page(s) 60

imeTeevan,and Katrina Panovich. A Comparison of Information

nes and Social Networks. In Proceedings of the Fourth International

and Social Media, pages 291–294, 2010. Cited on page(s) 70

parck Jones. KEYWORDS AND CLUMPS: Recent work on

Cambridge Language Research Unit. Journal of Documentation,

.1108/eb026337 Cited on page(s) 22

M. de Rijk, G. Mishne, and I. Soboroff. Overview of TREC 2006

s of the Fifteenth Text REtrieval Conference (TREC 2006), pages

e(s) 41

d I.Soboroff. Overview ofTREC 2008 BlogTrack. In Proceedings

rieval Conference (TREC 2008), 2008. Cited on page(s) 41

Criteria for Designing Information Retrieval Systems. American

1955. DOI: 10.1002/asi.5090060209 Cited on page(s) 7

raswell. Comparing the Sensitivity of Information Retrieval Met-

rd Annual International ACM SIGIR Conference on Research and De-

etrieval, pages 667–674, 2010. DOI: 10.1145/1835449.1835560

nformation Systems and Services. In Annual Review of Information

pter 3. Interscience, 1967. Cited on page(s) 8

n, and W.R. Hersh. Tasks, Topics and Relevance Judging for the

ive Years of Experience Evaluating Biomedical Text Information

ionRetrieval,12:81–97,2009.DOI: 10.1007/s10791-008-9072-x


Parametric Description of Retrieval Tests. Journal of Documentation,

0.1108/eb026466 Cited on page(s) 26

history of evaluation in IR. Journal of Information Science,34:439–456,

165551507086989 Cited on page(s) 1

n GMAP: and Other Tansformations. In Proceedings of the 2006 ACM

onference on Information and Knowledge Management, pages 78–83,

183614.1183630 Cited on page(s) 79, 82

d Jamie Callan. Routing and Filtering. In TREC: Experiment and

ion Retrieval, chapter 5. The MIT Press, 2005. Cited on page(s) 44,

Processing of Foreign Language Documents. In Scientiﬁc Report ISR-

Cornell University, Ithaca, N.Y, 1967. DOI: 10.3115/990403.990407

rality” Effect and the Retrieval Evaluation for Large Collections. In

8 to NSF, chapter II. Cornell University, Ithaca, N.Y, 1970. Cited on

erality” Effect and the Retrieval Evaluation for Large Collections.

Society for Information Science, pages 11–22, January-February 1972.

230105 Cited on page(s) 15

mparison Between Conventional Indexing (Medlars) and Automatic

RT). In Scientiﬁc Report ISR-21 to NSF, chapter I. Cornell University,

ed on page(s) 18

mparison Between Conventional Indexing (Medlars) and Automatic

ART). Journal of the American Society for Information Science, pages

972. DOI: 10.1002/asi.4630230202 Cited on page(s) 18

esk. Information Analysis and Dictionary Construction. In Scientiﬁc

chapter IV. Cornell University, Ithaca, N.Y, 1966. Cited on page(s)

illiamson. A Comparison Between Manual and Automatic Indexing

Report ISR-14 to NSF, chapter VI. Cornell University, Ithaca, N.Y,

i.4630200109 Cited on page(s) 18


ation of Automatic Retrieval Procedures

Selected Test Results

m. American Documentation, 16(3):209–222, 1965.

308 Cited on page(s) 16

SMART Retrieval System. Prentice-Hall, Englewood Cliffs, New

e(s) 11, 13, 15, 16, 18, 33

ection Based Evaluation of Information Retrieval Systems. Foun-

rmation Retrieval, 4:247–375, 2010. DOI: 10.1561/1500000009

2, 34, 51, 54, 55

Lestari Paramita, Paul Clough, and Vanja Josifovski. Do User

Measures Line Up. In Proceedings of the 33rd Annual International

Research and Development in Information Retrieval, pages 555–562,

449.1835542 Cited on page(s) 67, 81

Soboroff. Problems with Kendall’s tau. In Proceedings of the 30th

M SIGIR Conference on Research and Development in Information

2007. DOI: 10.1145/1277741.1277935 Cited on page(s) 80

tin Zobel.

Information Retrieval System Evaluation: Effort,

In Proceedings of the 28th Annual International ACM SIGIR

d Development in Information Retrieval, pages 162–169, 2005.

76064 Cited on page(s) 79

Research and Teaching. American Documentation, pages 398–403,

02/asi.5090190407 Cited on page(s) 20

Results from an Inquiry into Testing of Information Retrieval

rican Society for Information Science, pages 126–139, March-April

30220212 Cited on page(s) 20, 21

A Review of the Literature and a Framework forThinking on the

ence. Part II: Nature and Manifestations of Relevance. Journal of

mationScience,58(13):1915–1933,2007.DOI: 10.1002/asi.20682

e: A Review of the Literature and a Framework for Thinking on

Science. Part III: Behavior and Effects of Relevance. Journal of the

ation Science, 58(13):2126–2144, 2007. DOI: 10.1002/asi.20681


ni, and P. Schäuble. Building a Large Multilingual Test Collection

s Documents. In Cross-language Information Retrieval,pages 137–180.

ishers, 1998. Cited on page(s) 37

Kameen, Sally.K. Sinn, and Frieda O. Weise. Research Strategy and

duct a Comparative Evaluation of Two Prototype Online Catalog

gs of the National Online Meeting, pages 503–511, 1984. Cited on

d Paul B. Kantor. User Adaptation: Good Results from Poor Systems.

st Annual International ACM SIGIR Conference on Research and De-

n Retrieval, pages 147–154, 2008. DOI: 10.1145/1390334.1390362

es Allan, and Ben Carterette. Agreement Among Statistical Signiﬁ-

ation Retrieval Evaluation at Varying Sample Sizes. In Proceedings of

ational ACM SIGIR Conference on Research and Development in Infor-

630–631, 2009. DOI: 10.1145/1571941.1572050 Cited on page(s)

Worring. Multimodal Video Indexing:A Review of the State-of-the-

pl.,25(1):5–35,2005.DOI: 10.1023/B:MTAP.0000046380.27575.a5

hen Robertson. Building a Filtering Test Collection for TREC 2002.

h Annual International ACM SIGIR Conference on Research and Devel-

Retrieval, pages 243–250, 2003. DOI: 10.1145/860435.860481 Cited

Relevance Criteria of TREC—Counting on Negligible Documents?

h Annual International ACM SIGIR Conference on Research and Devel-

Retrieval, pages 324–330, 2002. DOI: 10.1145/564376.564433 Cited

ction Properties Inﬂuencing Automatic Term Classiﬁcations Perfor-

rage and Retrieval, 9:499–513, 1973.

271(73)90036-3 Cited on page(s) 22


. Jackson. The Use of Automatically-Obtained Keyword Clas-

Retrieval. Information Storage and Retrieval, 5:175–201, 1970.

70)90046-X Cited on page(s) 22

n Rijsbergen. Report on the Need for and Provision of an “Ideal”

Collection. British Library Research and Development Report

y, University of Cambridge, 1975. Cited on page(s) 22, 82

Information Retrieval Experiment. Butterworths, 1981. Cited on

ds Better NLP System Evaluation. In Proceedings of the workshop on

y, pages 102–107, 1994. DOI: 10.3115/1075812.1075833 Cited

easures for Interactive Information Retrieval. Information Process-

):503–516, 1992. DOI: 10.1016/0306-4573(92)90007-M Cited

Allan. Aspect Windows, 3-D Visualizations, and Indirect Com-

etrieval Systems. In Proceedings of the 21st Annual International

Research and Development in Information Retrieval, pages 173–

290941.290987 Cited on page(s) 64

explained Aspects of the Cranﬁeld Tests of Indexing Language

rterly, 41:223–228, 1971. DOI: 10.1086/619959 Cited on page(s)

Pragmatics of Information Retrieval Experimentation, Revisited.

Management, 28:467–490, 1992.

92)90005-K Cited on page(s) 77

ames Blustein. A Statistical Analysis of the TREC-3 Data. In

REtrieval Conference (TREC-3) [Proceedings of TREC-3.], pages

age(s) 32

n-Negotiation and Information Seeking in Libraries. College and

194, 1968. Cited on page(s) 58


390334.1390364 Cited on page(s) 56, 73, 78

oring and Evaluation of Information Systems Via Transaction Log

nd Development in Information Retrieval, pages 247–258, 1984. Cited

an Ruthven, and Joemon M. Jose. How Users Assess Web Pages for

Journal of the American Society for Information Science, 56(4):327–344,

i.20106 Cited on page(s) 71, 72

illiam Hersh. Why Batch and User Evaluations do not Give the Same

f the 24th Annual International ACM SIGIR Conference on Research and

ation Retrieval, pages 225–231, 2001. DOI: 10.1145/383952.383992

7, 79

choler, Kalvero Järvelin, Mingfang Wu, and J. Shane Culpepper. In-

System Evaluation. In Proceedings of the 32nd Annual International

e on Research and Development in Information Retrieval, pages 508–

45/1571941.1572029 Cited on page(s) 81

formation Retrieval. Butterworths, 1975. Cited on page(s) 26

. Cameron. The National Physical Laboratory Experiments in Sta-

ons and their Use in Document Indexing and Retrieval. Publication

uter Science, National Physical Laboratory, Teddington, 1970. Cited

tion Answering in TREC. In TREC: Experiment and Evaluation

l, chapter 10. The MIT Press, 2005. DOI: 10.1145/502585.502679

hn S. Garofolo. Retrieving Noisy Text. In TREC: Experiment and

ion Retrieval, chapter 8. The MIT Press, 2005. Cited on page(s) 36,

nna Harman, editors. TREC: Experiment and Evaluation in Informa-

T Press, 2005. Cited on page(s) 27

Variations in Relevance Judgments and the Measurement of Re-

Information Processing and Management, 36(5):697–716, 2000.

4573(00)00010-8 Cited on page(s) 33


ew of TREC 2004 Robust Track. In Proceedings of the Thirteenth

TREC 2004), pages 70–79, 2004. Cited on page(s) 51

Set Size Redux. In SIGIR09, pages 806–807, 2009.

72138 Cited on page(s) 54, 80

ris Buckley. The Effect of Topic Set Size on Retrieval Experiment

25th Annual International ACM SIGIR Conference on Research and

n Retrieval, pages 316–323, 2002. DOI: 10.1145/564376.564432

T.Dang. Overview of theTREC 2005 Question AnsweringTrack.

enth Text REtrieval Conference (TREC 2005), pages 69–80, 2005.

nna Harman. Overview of the Fifth Text REtrieval Conference

f the FifthText REtrieval Conference (TREC-5), pages 1–28, 1997.

wn T. Tice. Building a Question Answering Test Collection. In

ual International ACM SIGIR Conference on Research and Devel-

eval, pages 200–207, 2000. DOI: 10.1145/345508.345577 Cited

ssage Retrieval and Evaluation. Technical Report IR-396, CIIR,

Science, University of Massachusetts, Amherst, 2005. Cited on

el de Vere. Improving Subject Retrieval in Online Catalogues: 2.

ry expansion. British Library Research Paper 72, London: British

ge(s) 61

eline Hancock-Beaulieu. Okapi at City, an evaluation facility for

ary Research Report 6056, London: British Library, 1991. Cited

rd M.Jones. Improving Subject Retrieval in Online Catalogues: 1 .

ng correction and cross-reference tables. British Library Research

Library, 1987. Cited on page(s) 60


7

Dan Morris. Investigating the Querying and Browsing Behaviour

ngine Users. In Proceedings of the 30th Annual International ACM

search and Development in Information Retrieval, pages 255–262, 2007.

1.1277787 Cited on page(s) 55

ctive Retrieval of Structured Documents. In Proceedings of the 17th

CM SIGIR Conference on Research and Development in Information

17, 1994. Cited on page(s) 53

iamson,and M.E.Lesk. The Cornell Implementation of the SMART

port ISR-16 to NSF, chapter I. Cornell University, Ithaca, N.Y, 1967.

er. Multilingual Topic Generation within the CLEF 2001 Exper-

n of Cross-Language Information Systems, the Second Workshop of the

pages 389–393. Springer LNCS 2406, 2001.

45691-0_36 Cited on page(s) 49, 54, 78

d A. Aslam. Estimating Average Precision with Incomplete and Im-

Proceedings of the 2006 ACM CIKM International Conference on Infor-

Management, pages 102–111, 2006. DOI: 10.1145/1183614.1183633

T. Minka. Novelty and Redundancy Detection in Adaptive Filtering.

th Annual International ACM SIGIR Conference on Research and De-

n Retrieval, pages 81–88, 2002. DOI: 10.1145/564376.564393 Cited

liable are the Results of Large-Scale Information Retrieval Experi-

f the 21st Annual International ACM SIGIR Conference on Research and

ation Retrieval, pages 307–314, 1998. DOI: 10.1145/290941.291014



g

p y

from Cornell University as an Electrical Engineer, and started her

Gerard Salton in the design and building of several test collections,

RS one. Later work was concerned with searching large volumes of

uters, starting with building the IRX system at the National Library

n the Citator/PRISE system at the National Institute of Standards

988. In 1990 she was asked by DARPA to put together a realistic

2 gigabytes of text, and this test collection was used in the ﬁrst Text

C). TREC is now in its 20th year, and along with its sister evaluations

X, and FIRE, serves as a major testing ground for information retrieval

999 Strix Award from the U.K Institute of Information Scientists for

he worked with Paul Over at NIST to form a new effort (DUC) to

which has now been folded into the Text Analysis Conference (TAC),

al areas in NLP.

