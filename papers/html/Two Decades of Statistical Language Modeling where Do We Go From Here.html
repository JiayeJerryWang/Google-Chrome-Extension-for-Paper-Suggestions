<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:96.2pt;left:130.1pt;line-height:12.0pt"><b><span style="font-family:Times New Roman,serif;font-size:12.0pt">TWO DECADES OF STATISTICAL LANGUAGE MODELING:</span></b></p>
<p style="top:110.2pt;left:201.1pt;line-height:12.0pt"><b><span style="font-family:Times New Roman,serif;font-size:12.0pt">WHERE DO WE GO FROM HERE?</span></b></p>
<p style="top:139.2pt;left:255.4pt;line-height:12.0pt"><i><span style="font-family:Times New Roman,serif;font-size:12.0pt">Ronald Rosenfeld</span></i></p>
<p style="top:167.0pt;left:229.2pt;line-height:12.0pt"><span style="font-family:Times New Roman,serif;font-size:12.0pt">School of Computer Science</span></p>
<p style="top:181.0pt;left:230.9pt;line-height:12.0pt"><span style="font-family:Times New Roman,serif;font-size:12.0pt">Carnegie Mellon University</span></p>
<p style="top:194.9pt;left:247.0pt;line-height:12.0pt"><span style="font-family:Times New Roman,serif;font-size:12.0pt">Pittsburgh, PA 15213</span></p>
<p style="top:208.8pt;left:285.8pt;line-height:12.0pt"><span style="font-family:Times New Roman,serif;font-size:12.0pt">USA</span></p>
<p style="top:225.1pt;left:257.3pt;line-height:9.0pt"><tt><span style="font-family:Courier,monospace;font-size:9.0pt">roni@cs.cmu.edu</span></tt></p>
<p style="top:263.4pt;left:141.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">ABSTRACT</span></b></p>
<p style="top:281.4pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Statistical Language Models estimate the distribution of</span></p>
<p style="top:293.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">various natural language phenomena for the purpose of speech</span></p>
<p style="top:305.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">recognition and other language technologies. Since the &#xfb01;rst</span></p>
<p style="top:317.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">signi&#xfb01;cant model was proposed in 1980, many attempts have</span></p>
<p style="top:329.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">been made to improve the state of the art. We review them</span></p>
<p style="top:341.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">here, point to a few promising directions, and argue for a</span></p>
<p style="top:353.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Bayesian approach to integration of linguistic theories with</span></p>
<p style="top:365.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">data.</span></p>
<p style="top:394.2pt;left:138.2pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">1. OUTLINE</span></b></p>
<p style="top:417.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Statistical language modeling (SLM) is the attempt to cap-</span></p>
<p style="top:428.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ture regularities of natural language for the purpose of im-</span></p>
<p style="top:440.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">proving the performance of various natural language appli-</span></p>
<p style="top:452.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cations. By and large, statistical language modeling amounts</span></p>
<p style="top:464.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to estimating the probability distribution of various linguis-</span></p>
<p style="top:476.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tic units, such as words, sentences, and whole documents.</span></p>
<p style="top:489.0pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Statistical language modeling is crucial for a large va-</span></p>
<p style="top:500.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">riety of language technology applications. These include</span></p>
<p style="top:512.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">speech recognition (where SLM got its start), machine trans-</span></p>
<p style="top:524.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lation, document classi&#xfb01;cation and routing, optical charac-</span></p>
<p style="top:536.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ter recognition, information retrieval, handwriting recogni-</span></p>
<p style="top:548.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion, spelling correction, and many more.</span></p>
<p style="top:560.8pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In machine translation, for example, purely statistical</span></p>
<p style="top:572.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">approaches have been introduced in [1]. But even researchers</span></p>
<p style="top:584.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">using rule-based approaches have found it bene&#xfb01;cial to in-</span></p>
<p style="top:596.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">troduce some elements of SLM and statistical estimation</span></p>
<p style="top:608.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[2]. In information retrieval, a language modeling approach</span></p>
<p style="top:620.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">was recently proposed by [3], and a statistical/information</span></p>
<p style="top:632.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">theoretical approach was developed by [4].</span></p>
<p style="top:644.8pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">SLM employs statistical estimation techniques using lan-</span></p>
<p style="top:656.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage training data, that is, text. Because of the categorical</span></p>
<p style="top:668.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nature of language, and the large vocabularies people natu-</span></p>
<p style="top:680.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">rally use, statistical techniques must estimate a large num-</span></p>
<p style="top:692.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ber of parameters, and consequently depend critically on the</span></p>
<p style="top:704.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">availability of large amounts of training data.</span></p>
<p style="top:263.4pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Over the past twenty years, successively larger amounts</span></p>
<p style="top:275.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of text of various types have become available online. As</span></p>
<p style="top:287.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a result, in domains where such data became available, the</span></p>
<p style="top:299.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">quality of language models has increased dramatically. How-</span></p>
<p style="top:311.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ever, this improvement is now beginning to asymptote. Even</span></p>
<p style="top:323.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">if online text continues to accumulate at an exponential rate</span></p>
<p style="top:335.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(which it no doubt will, given the growth rate of the web),</span></p>
<p style="top:347.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the quality of currently used statistical language models is</span></p>
<p style="top:359.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">not likely to improve by a signi&#xfb01;cant factor. One informal</span></p>
<p style="top:371.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">estimate from IBM shows that bigram models effectively</span></p>
<p style="top:383.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">saturate within several hundred million words, and trigram</span></p>
<p style="top:395.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">models are likely to saturate within a few billion words. In</span></p>
<p style="top:407.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">several domains we already have this much data.</span></p>
<p style="top:421.6pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Ironically, the most successful SLM techniques use very</span></p>
<p style="top:433.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">little knowledge of what language really is. The most pop-</span></p>
<p style="top:445.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ular language models (</span></p>
<p style="top:448.9pt;left:404.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -grams) take no advantage of the</span></p>
<p style="top:457.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">fact that what is being modeled is language &#x2013; it may as well</span></p>
<p style="top:469.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">be a sequence of arbitrary symbols, with no deep structure,</span></p>
<p style="top:481.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">intention or thought behind them.</span></p>
<p style="top:496.0pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A possible reason for this situation is that the knowl-</span></p>
<p style="top:508.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">edge impoverished but data optimal techniques of</span></p>
<p style="top:511.5pt;left:512.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -grams</span></p>
<p style="top:520.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">succeeded too well, and thus stymied work on knowledge</span></p>
<p style="top:531.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">based approaches.</span></p>
<p style="top:546.4pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">But one can only go so far without knowledge. In the</span></p>
<p style="top:558.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">words of the premier proponent of the statistical approach</span></p>
<p style="top:570.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to language modeling, Fred Jelinek, we must &#x2018;put language</span></p>
<p style="top:582.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">back into language modeling&#x2019; [5]. Unfortunately, only a</span></p>
<p style="top:594.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">handful of attempts have been made to date to incorporate</span></p>
<p style="top:606.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">linguistic structure, theories or knowledge into statistical</span></p>
<p style="top:618.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">language models, and most such attempts have been only</span></p>
<p style="top:630.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">modestly successful.</span></p>
<p style="top:644.8pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In what follows, section 2 introduces statistical language</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">modeling in more detail, and discusses the potential for im-</span></p>
<p style="top:668.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">provement in this area. Section 3 overviews major estab-</span></p>
<p style="top:680.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lished SLM techniques. Section 4 lists promising current</span></p>
<p style="top:692.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">research directions. Finally, section 5 suggests both an in-</span></p>
<p style="top:704.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">teractive approach and a Bayesian approach to the integra-</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion of linguistic knowledge into the model, and points to</span></p>
<p style="top:85.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the encoding of such knowledge as a main challenge facing</span></p>
<p style="top:97.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the &#xfb01;eld.</span></p>
<p style="top:128.8pt;left:69.8pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">2. STATISTICAL LANGUAGE MODELING</span></b></p>
<p style="top:152.1pt;left:49.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">2.1. De&#xfb01;nition and use</span></b></p>
<p style="top:171.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A statistical language model is simply a probability distri-</span></p>
<p style="top:183.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">bution</span></p>
<p style="top:184.9pt;left:77.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> over all possible sentences</span></p>
<p style="top:184.9pt;left:208.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> .</span><span style="font-family:Times New Roman,serif;font-size:7.0pt">1</span></p>
<p style="top:196.2pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">It is instructive to compare statistical language model-</span></p>
<p style="top:208.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing to computational linguistics. Admittedly, the two &#xfb01;elds</span></p>
<p style="top:220.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(and communities) have fuzzy boundaries, and a great deal</span></p>
<p style="top:232.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of overlap. Nonetheless, one way to characterize the differ-</span></p>
<p style="top:244.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ence is as follows. Let</span></p>
<p style="top:252.1pt;left:145.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:244.2pt;left:155.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">be the word sequence of a given</span></p>
<p style="top:256.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sentence, i.e. its</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> surface</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> form, and let</span></p>
<p style="top:263.9pt;left:210.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:256.0pt;left:222.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">be some</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> hidden</span></i></p>
<p style="top:268.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">structure associated with it (i.e. its parse tree, word senses,</span></p>
<p style="top:280.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">etc.). Statistical language modeling is mostly about esti-</span></p>
<p style="top:292.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mating Pr</span></p>
<p style="top:293.1pt;left:90.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:299.9pt;left:93.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:293.1pt;left:100.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> , whereas computational linguistics is mostly</span></p>
<p style="top:304.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">about estimating Pr</span></p>
<p style="top:305.1pt;left:128.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:311.9pt;left:131.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xb;&#xa;</span></p>
<p style="top:311.9pt;left:143.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:305.1pt;left:149.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> . Of course, if one could estimate</span></p>
<p style="top:316.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">well the joint Pr</span></p>
<p style="top:317.1pt;left:116.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:323.9pt;left:119.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xd;&#xc;&#xe;&#xfffd;</span></p>
<p style="top:317.1pt;left:139.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> , both Pr</span></p>
<p style="top:317.1pt;left:179.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:323.9pt;left:182.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:317.1pt;left:189.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> and Pr</span></p>
<p style="top:317.1pt;left:223.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:323.9pt;left:226.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xf;&#xa;</span></p>
<p style="top:323.9pt;left:238.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:317.1pt;left:245.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> could be</span></p>
<p style="top:327.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">derived from it. In practice, this is usually not feasible.</span></p>
<p style="top:340.5pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Statistical language models are usually used in the con-</span></p>
<p style="top:352.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">text of a Bayes classi&#xfb01;er, where they can play the role of</span></p>
<p style="top:364.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">either the prior or the likelihood function. For example, in</span></p>
<p style="top:376.2pt;left:49.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">automatic speech recognition</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt">, given an acoustic signal</span></p>
<p style="top:384.1pt;left:279.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x10;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ,</span></p>
<p style="top:388.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the goal is to &#xfb01;nd the sentence</span></p>
<p style="top:389.3pt;left:175.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> that is most likely to have</span></p>
<p style="top:400.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">been spoken. Using a Bayesian framework, the solution is:</span></p>
<p style="top:424.4pt;left:66.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#x11;&#x13;&#x12;&#x15;&#x14;&#x17;&#x16;&#x19;&#x18;&#x1b;&#x1a;&#x1c;&#x14;&#x1e;&#x1d;</span></p>
<p style="top:435.9pt;left:104.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x1f;</span></p>
<p style="top:424.4pt;left:125.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;</span></p>
<p style="top:431.2pt;left:142.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:431.2pt;left:144.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x10;</span></p>
<p style="top:424.4pt;left:149.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#x1b;&#x12; &#x14;&#x17;&#x16;&#x19;&#x18;!&#x1a;&quot;&#x14;&#x1e;&#x1d;</span></p>
<p style="top:435.9pt;left:182.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x1f;</span></p>
<p style="top:424.4pt;left:202.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;</span></p>
<p style="top:431.2pt;left:214.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x10;#&#xa;</span></p>
<p style="top:424.4pt;left:222.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;$&#xfffd;&#xd;%&#xfffd;&amp;&#xfffd;&apos;&#xfffd;$&#xfffd;</span></p>
<p style="top:423.3pt;left:274.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(1)</span></p>
<p style="top:451.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">where the language model</span></p>
<p style="top:452.9pt;left:158.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> plays the role of the prior.</span></p>
<p style="top:463.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In contrast, in</span><b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> document classi&#xfb01;cation</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt">, given a document</span></p>
<p style="top:476.5pt;left:50.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> , the goal is to &#xfb01;nd the class</span></p>
<p style="top:483.5pt;left:173.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">)</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> to which it belongs. Typi-</span></p>
<p style="top:487.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cally, examples of documents from each of the (say)</span></p>
<p style="top:495.5pt;left:255.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">*</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> classes</span></p>
<p style="top:499.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">are given, from which</span></p>
<p style="top:507.5pt;left:140.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">*</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> different language models</span></p>
<p style="top:507.5pt;left:255.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">+</span></p>
<p style="top:500.7pt;left:260.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#x1b;,$&#xfffd;</span></p>
<p style="top:500.5pt;left:275.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span></p>
<p style="top:500.7pt;left:280.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ,</span></p>
<p style="top:512.7pt;left:50.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xd;-&#x1e;&#xfffd;</span></p>
<p style="top:512.5pt;left:64.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span></p>
<p style="top:512.7pt;left:70.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ,</span></p>
<p style="top:519.5pt;left:80.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">.$.$.</span><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:512.7pt;left:97.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;0/1&#xfffd;</span></p>
<p style="top:512.5pt;left:112.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span></p>
<p style="top:512.7pt;left:117.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;32</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> are constructed. Using a Bayes classi-</span></p>
<p style="top:523.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">&#xfb01;er, the solution</span></p>
<p style="top:531.3pt;left:117.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">)</span></p>
<p style="top:525.0pt;left:121.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x11;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is:</span></p>
<p style="top:554.5pt;left:73.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">)</span></p>
<p style="top:547.5pt;left:77.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x11;</span></p>
<p style="top:547.7pt;left:85.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x12;4&#x14;&#x1e;&#x16;5&#x18;&#x1b;&#x1a;&quot;&#x14;&#x17;&#x1d;</span></p>
<p style="top:559.3pt;left:110.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">6</span></p>
<p style="top:547.7pt;left:131.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;</span></p>
<p style="top:554.5pt;left:143.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">)7&#xa;</span></p>
<p style="top:547.5pt;left:150.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span></p>
<p style="top:547.7pt;left:155.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;8&#x12;&#x15;&#x14;&#x17;&#x16;&#x19;&#x18;&#x1b;&#x1a;&#x1c;&#x14;&#x1e;&#x1d;</span></p>
<p style="top:559.3pt;left:187.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">6</span></p>
<p style="top:547.7pt;left:208.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;</span></p>
<p style="top:547.5pt;left:220.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span></p>
<p style="top:554.5pt;left:226.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:554.5pt;left:228.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">)</span></p>
<p style="top:547.7pt;left:232.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;8%&#xfffd;&#xfffd;</span></p>
<p style="top:554.5pt;left:255.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">)</span></p>
<p style="top:547.7pt;left:259.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:546.6pt;left:274.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(2)</span></p>
<p style="top:575.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">where the language model</span></p>
<p style="top:576.1pt;left:158.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:581.1pt;left:165.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">6</span></p>
<p style="top:576.1pt;left:169.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:575.9pt;left:173.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">(</span></p>
<p style="top:576.1pt;left:178.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> plays the role of the like-</span></p>
<p style="top:587.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lihood. In a similar fashion, one can derive the role of lan-</span></p>
<p style="top:599.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage models in Bayesian classi&#xfb01;ers for the other language</span></p>
<p style="top:611.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">technologies listed above.</span></p>
<p style="top:640.7pt;left:49.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">2.2. Measures of progress</span></b></p>
<p style="top:660.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">To assess the quality of a given language modeling tech-</span></p>
<p style="top:672.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nique, the likelihood of new data is most commonly used.</span></p>
<p style="top:684.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The average log likelihood of a new random sample is given</span></p>
<img style="position:absolute;transform:matrix(126.40001,0,-0,-.64000007,129.408,937.248)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:705.1pt;left:60.5pt;line-height:6.0pt"><span style="font-family:Times New Roman,serif;font-size:6.0pt">1</span><span style="font-family:Times New Roman,serif;font-size:8.0pt">Or spoken utterances, documents, or any other linguistic unit.</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">by:</span></p>
<p style="top:97.8pt;left:318.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Average-Log-Likelihood</span></p>
<p style="top:98.9pt;left:418.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;9</span></p>
<p style="top:105.7pt;left:431.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:105.7pt;left:433.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">:</span></p>
<p style="top:98.9pt;left:444.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#x1b;&#x12;&lt;;</span></p>
<img style="position:absolute;transform:matrix(8,0,-0,-.64000007,620.2881,137.88797)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:108.3pt;left:462.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&gt;=1?A@CB</span></p>
<p style="top:98.9pt;left:495.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x18;D&#xfffd;8EF&#xfffd;&#xfffd;9</span></p>
<p style="top:98.0pt;left:528.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:98.9pt;left:532.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:118.5pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(3)</span></p>
<p style="top:130.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">where</span></p>
<p style="top:131.6pt;left:336.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">9G&#x12;</span></p>
<p style="top:138.4pt;left:358.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">+</span></p>
<p style="top:131.6pt;left:363.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">9&#xfffd;,</span></p>
<p style="top:138.4pt;left:376.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;</span></p>
<p style="top:131.6pt;left:380.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">9H-</span></p>
<p style="top:138.4pt;left:393.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;&#xfffd;.$.$.&#x19;&#xc;</span></p>
<p style="top:131.6pt;left:415.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">9JI#2</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is the new data sample, and</span></p>
<p style="top:142.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">M is the given language model. This latter quantity can also</span></p>
<p style="top:154.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">be viewed as an empirical estimate of the</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> cross entropy</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> of</span></p>
<p style="top:166.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the true (but unknown) data distribution</span></p>
<p style="top:167.3pt;left:477.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:166.2pt;left:487.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">with regard to</span></p>
<p style="top:178.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the model distribution</span></p>
<p style="top:179.3pt;left:399.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;0E</span></p>
<p style="top:178.2pt;left:414.5pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">:</span></p>
<p style="top:209.7pt;left:316.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cross-entropy</span></p>
<p style="top:210.8pt;left:371.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;K3&#xfffd;</span></p>
<p style="top:210.7pt;left:394.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">E</span></p>
<p style="top:210.8pt;left:402.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;L&#x12;NM</span></p>
<p style="top:220.1pt;left:434.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">=PO</span></p>
<p style="top:210.8pt;left:450.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&apos;9&#xfffd;&#xd;%RQ&apos;SRTU&#xfffd;</span></p>
<p style="top:210.7pt;left:501.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">E</span></p>
<p style="top:210.8pt;left:510.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;9&#xfffd;</span></p>
<p style="top:209.7pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(4)</span></p>
<p style="top:238.0pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Actual performance of language models is often reported</span></p>
<p style="top:250.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in terms of</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> perplexity</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> [6]:</span></p>
<p style="top:285.2pt;left:338.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">VXW</span></p>
<p style="top:282.8pt;left:348.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x16;</span></p>
<p style="top:285.2pt;left:352.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">V</span></p>
<p style="top:292.2pt;left:357.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">@</span></p>
<p style="top:285.2pt;left:360.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">W</span></p>
<p style="top:282.8pt;left:364.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x1d;ZY\[P]0&#xfffd;&#xfffd;&#xfffd;K3&#xfffd;</span></p>
<p style="top:282.7pt;left:404.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">E</span></p>
<p style="top:282.8pt;left:413.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;^&#x12;&#x15;_</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> cross-entropy</span></p>
<p style="top:285.5pt;left:490.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">`ba&#xd;c</span></p>
<p style="top:285.5pt;left:500.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">aedgf</span></p>
<p style="top:281.7pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(5)</span></p>
<p style="top:301.1pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Perplexity can be interpreted as the (geometric) average</span></p>
<p style="top:313.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">branching factor of the language according to the model. It</span></p>
<p style="top:325.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is a function of both the language and the model. When</span></p>
<p style="top:337.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">considered a function of the model, it measures how good</span></p>
<p style="top:349.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the model is (the better the model, the lower the perplexity).</span></p>
<p style="top:360.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">When considered a function of the language, it estimates the</span></p>
<p style="top:372.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">entropy, or complexity, of that language.</span></p>
<p style="top:384.9pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Ultimately, the quality of a language model must be</span></p>
<p style="top:396.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">measured by its effect on the speci&#xfb01;c application for which</span></p>
<p style="top:408.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">it was designed, namely by its effect on the error rate of that</span></p>
<p style="top:420.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">application. However, error rates are typically non-linear</span></p>
<p style="top:432.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and poorly understood functions of the language model. Lower</span></p>
<p style="top:444.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">perplexity usually result in lower error rates, but there are</span></p>
<p style="top:456.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">plenty of counterexamples in the literature. As a rough rule</span></p>
<p style="top:468.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of thumb, reduction of 5% in perplexity is usually not prac-</span></p>
<p style="top:480.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tically signi&#xfb01;cant; a 10%&#x2013;20% reduction is noteworthy, and</span></p>
<p style="top:492.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">usually (but not always) translates into some improvement</span></p>
<p style="top:504.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in application performance; a perplexity improvement of</span></p>
<p style="top:516.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">30% or more over a good baseline is quite signi&#xfb01;cant (and</span></p>
<p style="top:528.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">rare!).</span></p>
<p style="top:540.4pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Several attempts have been made to devise metrics that</span></p>
<p style="top:552.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">are better correlated with application error rate than perplex-</span></p>
<p style="top:564.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ity, yet are easier to optimize than the error rate itself. These</span></p>
<p style="top:576.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">attempts have met with limited success. For now, perplexity</span></p>
<p style="top:588.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">continues to be the preferred metric for practical language</span></p>
<p style="top:600.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">model construction. For more details, see [7].</span></p>
<p style="top:626.1pt;left:309.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">2.3. Known weaknesses in current models</span></b></p>
<p style="top:644.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Even the simplest language model has a drastic effect on the</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">application in which it is used (this can be observed by, say,</span></p>
<p style="top:668.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">removing the language model from a speech recognition</span></p>
<p style="top:680.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">system). However, current language modeling techniques</span></p>
<p style="top:692.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">are far from optimal. Evidence for this comes from several</span></p>
<p style="top:704.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sources:</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:64.6pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">Brittleness across domains:</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Current language mod-</span></p>
<p style="top:85.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">els are extremely sensitive to changes in the style, topic or</span></p>
<p style="top:97.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">genre of the text on which they are trained. For example,</span></p>
<p style="top:109.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to model casual phone conversations, one is much better off</span></p>
<p style="top:121.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">using 2 million words of transcripts from such conversa-</span></p>
<p style="top:133.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tions than using 140 million words of transcripts from TV</span></p>
<p style="top:145.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and radio news broadcasts. This effect is quite strong even</span></p>
<p style="top:157.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">for changes that seem trivial to a human: a language model</span></p>
<p style="top:169.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">trained on Dow-Jones newswire text will see its perplexity</span></p>
<p style="top:181.6pt;left:49.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">doubled</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> when applied to the very similar Associated Press</span></p>
<p style="top:193.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">newswire text from the same time period ([8, p. 220]).</span></p>
<p style="top:206.1pt;left:64.6pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">False independence assumption:</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> In order to remain</span></p>
<p style="top:218.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tractable, virtually all existing language modeling techniques</span></p>
<p style="top:230.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">assume some form of independence among different por-</span></p>
<p style="top:242.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tions of the same document. For example, the most com-</span></p>
<p style="top:253.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">monly used model, the</span></p>
<p style="top:257.3pt;left:145.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram, assumes that the probabil-</span></p>
<p style="top:265.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ity of the next word in a sentence depends only on the iden-</span></p>
<p style="top:277.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tity of the last</span></p>
<p style="top:281.3pt;left:111.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -1 words. Yet even a cursory look at any</span></p>
<p style="top:289.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">natural text proves this assumption patently false. False in-</span></p>
<p style="top:301.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">dependence assumptions in statistical models usually lead</span></p>
<p style="top:313.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to overly sharp distributions. This is precisely what is hap-</span></p>
<p style="top:325.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">pening in language modeling, as can be seen for example in</span></p>
<p style="top:337.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">document classi&#xfb01;cation: the posterior computed by equa-</span></p>
<p style="top:349.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion 2 is usually extremely sharp, reaching virtually one for</span></p>
<p style="top:361.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">one of the classes and virtually zero for all others. This of</span></p>
<p style="top:373.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">course cannot be the true posterior, since the average classi-</span></p>
<p style="top:385.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">&#xfb01;cation error rate is typically much greater than zero.</span></p>
<p style="top:398.1pt;left:64.6pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">Shannon-style experiments:</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Claude Shannon pioneered</span></p>
<p style="top:410.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the technique of eliciting human knowledge of language by</span></p>
<p style="top:422.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">asking human subjects to predict the next element of text</span></p>
<p style="top:433.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[9, 10]. Shannon used this technique to bound the entropy</span></p>
<p style="top:445.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of English. [11] formulated a gambling setup and used it</span></p>
<p style="top:457.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to derive its own estimate of the entropy of English. In</span></p>
<p style="top:469.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the 1980s, the speech and language research group at IBM</span></p>
<p style="top:481.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">performed &#x2018;Shannon-style&#x2019; experiments, in which potential</span></p>
<p style="top:493.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sources for language modeling improvement were identi&#xfb01;ed</span></p>
<p style="top:505.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">by observing and analyzing the performance of human sub-</span></p>
<p style="top:517.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">jects in predicting or correcting text. Since then, Shannon-</span></p>
<p style="top:529.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">style experiments have been performed by several other re-</span></p>
<p style="top:541.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">searchers. For example, [12] performed experiments aimed</span></p>
<p style="top:553.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">at establishing the potential for language modeling improve-</span></p>
<p style="top:565.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ments in speci&#xfb01;c linguistic areas. A common observation</span></p>
<p style="top:577.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">during all these experiments is that people improve on the</span></p>
<p style="top:589.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">performance of a language model easily, routinely and sub-</span></p>
<p style="top:601.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">stantially. They apparently do so by using reasoning at the</span></p>
<p style="top:613.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">linguistic, common sense, and domain levels.</span></p>
<p style="top:644.6pt;left:67.0pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">3. SURVEY OF MAJOR SLM TECHNIQUES</span></b></p>
<p style="top:668.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">This section brie&#xfb02;y reviews major established SLM tech-</span></p>
<p style="top:680.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">niques. For a more detailed technical treatment, see [13].</span></p>
<p style="top:692.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Almost all language models to date decompose the prob-</span></p>
<p style="top:704.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ability of a sentence into a product of conditional probabil-</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ities:</span></p>
<p style="top:104.6pt;left:346.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:105.7pt;left:356.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;&#xfffd;$&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> def</span></p>
<p style="top:105.7pt;left:373.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x12;</span></p>
<p style="top:104.6pt;left:385.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:105.7pt;left:395.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h&#x13;,</span></p>
<p style="top:112.5pt;left:412.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">.$.$.</span></p>
<p style="top:105.7pt;left:425.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h!I1&#xfffd;&#x1b;&#x12;</span></p>
<p style="top:91.8pt;left:459.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">I</span></p>
<p style="top:102.9pt;left:455.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">i</span></p>
<p style="top:115.0pt;left:455.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?kj</span></p>
<p style="top:116.0pt;left:464.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> </span><sup><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></sup></p>
<p style="top:105.7pt;left:479.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;h</span></p>
<p style="top:104.7pt;left:489.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:112.5pt;left:493.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:112.5pt;left:496.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:104.7pt;left:501.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:105.7pt;left:504.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:104.6pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(6)</span></p>
<p style="top:138.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">where</span></p>
<p style="top:139.7pt;left:337.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h</span></p>
<p style="top:138.8pt;left:344.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is the</span></p>
<p style="top:146.5pt;left:376.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">m</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> th word in the sentence, and</span></p>
<p style="top:146.5pt;left:499.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:138.8pt;left:505.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> def</span></p>
<p style="top:139.7pt;left:514.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x12;</span></p>
<p style="top:146.5pt;left:527.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">+</span></p>
<p style="top:139.7pt;left:532.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h</span></p>
<p style="top:139.8pt;left:539.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ,</span></p>
<p style="top:151.5pt;left:309.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">hn-</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ,</span></p>
<p style="top:158.3pt;left:326.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">.&#xfffd;.$.</span></p>
<p style="top:151.5pt;left:339.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h</span></p>
<p style="top:150.5pt;left:346.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?&#xfffd;o</span></p>
<p style="top:151.5pt;left:355.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,p2</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is called the</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> history</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">.</span></p>
<p style="top:178.5pt;left:309.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">3.1.</span></b></p>
<p style="top:182.0pt;left:330.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -grams</span></b></p>
<p style="top:201.4pt;left:309.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -grams are the staple of current speech recognition tech-</span></p>
<p style="top:209.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nology. Virtually all commercial speech recognition prod-</span></p>
<p style="top:221.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ucts use some form of an</span></p>
<p style="top:225.2pt;left:415.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram. An</span></p>
<p style="top:225.2pt;left:467.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram reduces the</span></p>
<p style="top:233.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">dimensionality of the estimation problem by modeling lan-</span></p>
<p style="top:245.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage as a Markov source of order</span></p>
<p style="top:249.2pt;left:449.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -1:</span></p>
<p style="top:271.7pt;left:350.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&apos;h</span></p>
<p style="top:270.8pt;left:369.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:278.5pt;left:373.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:278.5pt;left:375.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:270.8pt;left:381.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:271.7pt;left:384.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xd;q4&#xfffd;&#xfffd;&apos;h</span></p>
<p style="top:270.8pt;left:420.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:278.5pt;left:424.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:271.7pt;left:426.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h</span></p>
<p style="top:270.8pt;left:433.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?&#xfffd;o</span></p>
<p style="top:271.8pt;left:442.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">IZr8,</span></p>
<p style="top:278.5pt;left:458.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;$.&#xfffd;.$.3&#xc;</span></p>
<p style="top:271.7pt;left:480.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h</span></p>
<p style="top:270.8pt;left:487.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?&#xfffd;o</span></p>
<p style="top:271.8pt;left:497.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,</span></p>
<p style="top:271.7pt;left:501.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:270.6pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(7)</span></p>
<p style="top:289.8pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The value of</span></p>
<p style="top:293.3pt;left:380.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:289.8pt;left:390.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">trades off the stability of the estimate</span></p>
<p style="top:301.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(i.e. its variance) against its appropriateness (i.e. bias). A</span></p>
<p style="top:313.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">trigram (</span></p>
<p style="top:317.1pt;left:345.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> =3) is a common choice with large training cor-</span></p>
<p style="top:325.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">pora (millions of words), whereas a bigram (</span></p>
<p style="top:329.1pt;left:492.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> =2) is often</span></p>
<p style="top:337.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">used with smaller ones.</span></p>
<p style="top:349.8pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Deriving trigram and even bigram probabilities is still</span></p>
<p style="top:361.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a sparse estimation problem, even with very large corpora.</span></p>
<p style="top:373.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">For example, after observing all trigrams (i.e., consecutive</span></p>
<p style="top:385.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">word triplets) in 38 million words&#x2019; worth of newspaper ar-</span></p>
<p style="top:397.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ticles, a full third of trigrams in new articles from the same</span></p>
<p style="top:409.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">source are novel [8, p. 8]. Furthermore, even among the ob-</span></p>
<p style="top:421.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">served trigrams, the vast majority occurred only once, and</span></p>
<p style="top:433.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the majority of the rest had similarly low counts. Therefore,</span></p>
<p style="top:445.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">straightforward maximum likelihood (ML) estimation of</span></p>
<p style="top:448.9pt;left:536.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -</span></p>
<p style="top:457.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">gram probabilities from counts is not advisable. Instead,</span></p>
<p style="top:469.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">various smoothing techniques have been developed. These</span></p>
<p style="top:481.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">include discounting the ML estimates [14, 15], recursively</span></p>
<p style="top:493.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">backing off to lower order</span></p>
<p style="top:496.9pt;left:413.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -grams [16, 17, 18], and linearly</span></p>
<p style="top:505.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">interpolating</span></p>
<p style="top:508.6pt;left:364.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -grams of different order [19].</span></p>
<p style="top:505.1pt;left:506.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Other ap-</span></p>
<p style="top:517.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">proaches include variable-length</span></p>
<p style="top:520.6pt;left:440.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram [20, 21, 22, 23, 24]</span></p>
<p style="top:529.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">as well as a lattice approach [25]. Much work has been done</span></p>
<p style="top:541.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to compare and perfect smoothing techniques under various</span></p>
<p style="top:553.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">conditions. A good recent analysis can be found in [26]. In</span></p>
<p style="top:565.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">addition, toolkits implementing the various techniques have</span></p>
<p style="top:576.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">been disseminated [27, 28, 29, 30].</span></p>
<p style="top:589.1pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Yet another way to battle sparseness is via vocabulary</span></p>
<p style="top:601.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">clustering. Let</span></p>
<p style="top:609.0pt;left:374.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:601.3pt;left:381.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> be the class word</span></p>
<p style="top:602.2pt;left:464.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h</span></p>
<p style="top:601.3pt;left:471.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> was assigned to.</span></p>
<p style="top:613.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Then any of several model structures could be used. For</span></p>
<p style="top:625.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">example, for a trigram:</span></p>
<p style="top:659.7pt;left:330.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:660.8pt;left:339.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:667.6pt;left:355.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:660.8pt;left:357.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;,</span></p>
<p style="top:667.6pt;left:369.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;</span></p>
<p style="top:660.8pt;left:373.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">hn-3&#xfffd;u&#x12;</span></p>
<p style="top:659.7pt;left:416.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:660.8pt;left:426.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:667.6pt;left:441.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:667.6pt;left:443.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:660.8pt;left:450.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">t&#xfffd;&#xd;%</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pr</span></p>
<p style="top:660.8pt;left:475.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:667.6pt;left:479.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:660.8pt;left:486.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">t</span></p>
<p style="top:667.6pt;left:491.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:660.8pt;left:493.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;,</span></p>
<p style="top:667.6pt;left:505.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;</span></p>
<p style="top:660.8pt;left:509.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">hn-p&#xfffd;</span></p>
<p style="top:659.7pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(8)</span></p>
<p style="top:674.8pt;left:330.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:675.9pt;left:339.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:682.7pt;left:355.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:675.9pt;left:357.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;,</span></p>
<p style="top:682.7pt;left:369.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;</span></p>
<p style="top:675.9pt;left:373.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">hn-3&#xfffd;u&#x12;</span></p>
<p style="top:674.8pt;left:416.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:675.9pt;left:426.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:682.7pt;left:441.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:682.7pt;left:443.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:675.9pt;left:450.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">t&#xfffd;&#xd;%</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pr</span></p>
<p style="top:675.9pt;left:475.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:682.7pt;left:479.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:675.9pt;left:486.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">t</span></p>
<p style="top:682.7pt;left:491.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:675.9pt;left:493.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;,</span></p>
<p style="top:682.7pt;left:505.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;3s</span></p>
<p style="top:675.9pt;left:516.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">-p&#xfffd;</span></p>
<p style="top:674.8pt;left:534.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(9)</span></p>
<p style="top:689.7pt;left:330.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:690.8pt;left:339.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:697.6pt;left:355.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:690.8pt;left:357.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;,</span></p>
<p style="top:697.6pt;left:369.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;</span></p>
<p style="top:690.8pt;left:373.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">hn-3&#xfffd;u&#x12;</span></p>
<p style="top:689.7pt;left:416.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:690.8pt;left:426.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:697.6pt;left:441.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:697.6pt;left:443.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:690.8pt;left:450.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">t&#xfffd;&#xd;%</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pr</span></p>
<p style="top:690.8pt;left:475.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:697.6pt;left:479.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:690.8pt;left:486.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">t</span></p>
<p style="top:697.6pt;left:491.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:697.6pt;left:493.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:690.8pt;left:501.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,</span></p>
<p style="top:697.6pt;left:505.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;3s</span></p>
<p style="top:690.8pt;left:516.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">-p&#xfffd;</span></p>
<p style="top:689.7pt;left:529.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(10)</span></p>
<p style="top:704.6pt;left:330.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:705.7pt;left:339.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:712.5pt;left:355.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:705.7pt;left:357.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;,</span></p>
<p style="top:712.5pt;left:369.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;</span></p>
<p style="top:705.7pt;left:373.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">hn-3&#xfffd;u&#x12;</span></p>
<p style="top:704.6pt;left:416.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Pr</span></p>
<p style="top:705.7pt;left:426.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h!t</span></p>
<p style="top:712.5pt;left:441.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:712.5pt;left:443.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:705.7pt;left:451.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,</span></p>
<p style="top:712.5pt;left:455.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xc;3s</span></p>
<p style="top:705.7pt;left:467.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">-p&#xfffd;</span></p>
<p style="top:704.6pt;left:529.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(11)</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The quality of the resulting model depends of course</span></p>
<p style="top:85.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">on the clustering</span></p>
<p style="top:93.7pt;left:122.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">s</span></p>
<p style="top:86.9pt;left:130.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> . In narrow discourse domains (e.g.</span></p>
<p style="top:97.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ATIS, [31]), good results are often achieved by manual clus-</span></p>
<p style="top:109.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tering of semantic categories (e.g. [32]). But in less con-</span></p>
<p style="top:121.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">strained domains, manual clustering by linguistic categories</span></p>
<p style="top:133.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(e.g. parts of speech) does not usually improve on the word-</span></p>
<p style="top:145.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">based model. Automatic, iterative clustering using informa-</span></p>
<p style="top:157.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion theoretic criteria [33, 34] applied to large corpora can</span></p>
<p style="top:169.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sometimes reduce perplexity by 10% or so, but only after</span></p>
<p style="top:181.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the model is interpolated with its word-based counterpart.</span></p>
<p style="top:212.8pt;left:49.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">3.2. Decision tree models</span></b></p>
<p style="top:233.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Decision trees and CART-style [35] algorithms were &#xfb01;rst</span></p>
<p style="top:245.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">applied to language modeling by [36]. A decision tree can</span></p>
<p style="top:257.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">arbitrarily partition the space of histories by asking arbitrary</span></p>
<p style="top:268.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">binary questions about the history</span></p>
<p style="top:276.6pt;left:190.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:268.7pt;left:198.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">at each of the internal</span></p>
<p style="top:280.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nodes. The training data at each leaf is then used to con-</span></p>
<p style="top:292.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">struct a probability distribution Pr</span></p>
<p style="top:293.8pt;left:184.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;h</span></p>
<p style="top:300.6pt;left:196.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:300.6pt;left:198.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:293.8pt;left:204.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> over the next word.</span></p>
<p style="top:304.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">To reduce the variance of the estimate, this leaf distribution</span></p>
<p style="top:316.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is interpolated with internal-node distributions found along</span></p>
<p style="top:328.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the path to the root.</span></p>
<p style="top:341.4pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">As usual, trees are grown by greedily selecting, at each</span></p>
<p style="top:353.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">node, the most informative question (as judged by reduction</span></p>
<p style="top:365.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in entropy). Pruning and cross validation are also used.</span></p>
<p style="top:378.2pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Applying CART technology to language modeling is</span></p>
<p style="top:390.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">quite a challenge: The space of histories is very large (</span></p>
<p style="top:398.0pt;left:264.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">;&#xfffd;v</span></p>
<p style="top:386.3pt;left:274.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,&#x19;w&#xe;w</span></p>
<p style="top:401.9pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">for a 20 word sequence over a 100,000 word vocabulary),</span></p>
<p style="top:414.9pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and the space of possible questions is even larger (</span></p>
<p style="top:416.0pt;left:257.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">_</span></p>
<p style="top:411.0pt;left:262.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,&#x19;wRxzy&#xfffd;y</span></p>
<p style="top:414.9pt;left:280.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">).</span></p>
<p style="top:426.9pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Even if questions are restricted to individual words in the</span></p>
<p style="top:439.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">history, there are still</span></p>
<p style="top:440.9pt;left:137.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">_</span></p>
<p style="top:447.6pt;left:142.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">v</span></p>
<p style="top:440.9pt;left:150.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">%&#xfffd;_</span></p>
<p style="top:435.9pt;left:160.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">,&#x19;w3{</span></p>
<p style="top:439.8pt;left:175.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">such questions. Very strong</span></p>
<p style="top:451.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">bias must be introduced, by restricting the class of ques-</span></p>
<p style="top:463.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tions to be considered and using greedy search algorithms.</span></p>
<p style="top:475.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">To support optimal single-word questions at a given node,</span></p>
<p style="top:487.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">algorithms were developed for rapid optimal binary parti-</span></p>
<p style="top:499.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tioning of the vocabulary (e.g. [37]).</span></p>
<p style="top:512.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The &#xfb01;rst attempt at CART-style LM [36] used a history</span></p>
<p style="top:524.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">window of 20 words and restricted questions to individual</span></p>
<p style="top:536.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">words, though it allowed more complicated questions con-</span></p>
<p style="top:548.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sisting of composites of simple questions. It took many</span></p>
<p style="top:560.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">months to train, and the result fell short of expectations:</span></p>
<p style="top:572.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a 4% reduction in perplexity over the baseline trigram, and</span></p>
<p style="top:584.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a further 9% reduction when interpolated with the latter. In</span></p>
<p style="top:596.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the second attempt [38], much stronger bias was introduced:</span></p>
<p style="top:608.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">&#xfb01;rst, the vocabulary was clustered into a binary hierarchy as</span></p>
<p style="top:620.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in [33], and each word was assigned a bit-string represent-</span></p>
<p style="top:632.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing the path leading to it from the root. Then, tree questions</span></p>
<p style="top:644.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">were restricted to the identity of the most signi&#xfb01;cant as-yet-</span></p>
<p style="top:656.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">unknown bit in each word in the history. This reduced the</span></p>
<p style="top:667.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">candidate set to a handful of questions at each node. Unfor-</span></p>
<p style="top:679.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tunately, results here were also disappointing, and the ap-</span></p>
<p style="top:691.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">proach was largely abandoned.</span></p>
<p style="top:704.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Theoretically, decision trees represent the ultimate in</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">partition based models. It is likely that trees exist which</span></p>
<p style="top:85.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">signi&#xfb01;cantly outperform ngrams. But &#xfb01;nding them seems</span></p>
<p style="top:97.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">dif&#xfb01;cult, for both computational and data sparseness rea-</span></p>
<p style="top:109.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sons.</span></p>
<p style="top:142.0pt;left:309.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">3.3. Linguistically motivated models</span></b></p>
<p style="top:162.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">While all SLMs get some inspiration from an intuitive view</span></p>
<p style="top:174.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of language, in most models actual linguistic content is quite</span></p>
<p style="top:186.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">negligible. Several SLM techniques, however, are directly</span></p>
<p style="top:198.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">derived from grammars commonly uses by linguists.</span></p>
<p style="top:211.4pt;left:324.0pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">Context free grammar (CFG)</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is a crude yet well un-</span></p>
<p style="top:223.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">derstood model of natural language. A CFG is de&#xfb01;ned by a</span></p>
<p style="top:235.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">vocabulary, a set of non-terminal symbols and a set of pro-</span></p>
<p style="top:247.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">duction or transition rules. Sentences are generated, starting</span></p>
<p style="top:259.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">with an initial non-terminal, by repeated application of the</span></p>
<p style="top:271.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">transition rules, each transforming a non-terminal into a se-</span></p>
<p style="top:283.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">quence of terminals (i.e. words) and non-terminals, until a</span></p>
<p style="top:295.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">terminals-only sequence is achieved. Speci&#xfb01;c CFGs have</span></p>
<p style="top:307.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">been created based on parsed and annotated corpora such as</span></p>
<p style="top:319.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[39], with good, though still incomplete, coverage of new</span></p>
<p style="top:330.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">data.</span></p>
<p style="top:343.8pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A probabilistic (or stochastic) context free grammar puts</span></p>
<p style="top:355.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a probability distribution on the transitions emanating from</span></p>
<p style="top:367.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">each non-terminal, thereby inducing a distribution over the</span></p>
<p style="top:379.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">set of all sentences. These transition probabilities can be es-</span></p>
<p style="top:391.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">timated from annotated corpora using the Inside-Outside al-</span></p>
<p style="top:403.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">gorithm [40], an Estimation-Maximization (EM) algorithm</span></p>
<p style="top:415.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(see [41]). However, the likelihood surfaces of these models</span></p>
<p style="top:427.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tend to contain many local maxima, and the locally maximal</span></p>
<p style="top:439.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">likelihood points found by the algorithm usually fall short of</span></p>
<p style="top:451.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the global maximum. Furthermore, even if global ML esti-</span></p>
<p style="top:463.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mation were feasible, it is generally believed that context</span></p>
<p style="top:475.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sensitive transition probabilities are needed to adequately</span></p>
<p style="top:487.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">account for actual behavior of language. Unfortunately, no</span></p>
<p style="top:499.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ef&#xfb01;cient training algorithm is known for this situation.</span></p>
<p style="top:512.3pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In spite of this, [42] successfully incorporated CFG knowl-</span></p>
<p style="top:524.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">edge sources into a SLM to achieve a 15% reduction in a</span></p>
<p style="top:536.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">speech recognition error rate in the ATIS domain. They</span></p>
<p style="top:548.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">did so by parsing the utterances with a CFG to produce a</span></p>
<p style="top:560.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sequence of grammatical fragments of various types, then</span></p>
<p style="top:572.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">constructing a trigram of fragment types to supplant the</span></p>
<p style="top:584.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">standard ngram.</span></p>
<p style="top:597.0pt;left:324.0pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">Link grammar</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is a lexicalized grammar proposed by</span></p>
<p style="top:609.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[43]. Each word is associated with one or more ordered</span></p>
<p style="top:621.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sets of typed links; each such link must be connected to a</span></p>
<p style="top:632.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">similarly typed link of another word in the sentence. A legal</span></p>
<p style="top:644.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">parse consists of satisfying all links in the sentence via a</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">planar graph. Link grammar has the same expressive power</span></p>
<p style="top:668.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">as a CFG, but arguably conforms better to human linguistic</span></p>
<p style="top:680.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">intuition. A link grammar for English has been constructed</span></p>
<p style="top:692.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">manually with good coverage. Probabilistic forms of link</span></p>
<p style="top:704.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">grammar have also been attempted [44]. Link grammar is</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">related to dependency grammar, which will be discussed in</span></p>
<p style="top:85.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">section 4.</span></p>
<p style="top:114.2pt;left:49.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">3.4. Exponential models</span></b></p>
<p style="top:133.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">All models discussed so far suffer from data fragmentation,</span></p>
<p style="top:145.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in that more detailed modeling necessarily results in each</span></p>
<p style="top:157.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">new parameter being estimated with less and less data. This</span></p>
<p style="top:169.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is very apparent in decision trees, where, as the tree grows,</span></p>
<p style="top:181.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">leaves contain fewer and fewer data points.</span></p>
<p style="top:193.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Fragmentation can be avoided by using an exponential</span></p>
<p style="top:205.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">model of the form:</span></p>
<p style="top:231.7pt;left:91.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;h</span></p>
<p style="top:238.5pt;left:111.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:238.5pt;left:113.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:231.7pt;left:119.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;0&#x12;</span></p>
<p style="top:231.6pt;left:145.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">;</span></p>
<img style="position:absolute;transform:matrix(28.160002,0,-0,-.64000007,196.12799,314.528)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:238.4pt;left:137.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">|J&#xfffd;</span></p>
<p style="top:245.2pt;left:148.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:238.4pt;left:154.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:231.7pt;left:161.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">%</span></p>
<p style="top:234.1pt;left:166.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">W</span></p>
<p style="top:231.7pt;left:170.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x1d;</span></p>
<p style="top:234.1pt;left:175.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">V&#xd;}</span></p>
<p style="top:241.0pt;left:184.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">=1? ~</span></p>
<p style="top:230.7pt;left:205.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?&#x80;$?</span></p>
<p style="top:231.7pt;left:218.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:238.5pt;left:221.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l8&#xc;</span></p>
<p style="top:231.7pt;left:231.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x81;&#xfffd;&#xfffd;&#x82;</span></p>
<p style="top:230.6pt;left:269.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(12)</span></p>
<p style="top:262.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">where</span></p>
<p style="top:272.9pt;left:77.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">~</span></p>
<p style="top:262.6pt;left:83.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> are the parameters,</span></p>
<p style="top:263.6pt;left:171.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">|J&#xfffd;</span></p>
<p style="top:270.4pt;left:183.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:263.6pt;left:188.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is a normalizing term,</span></p>
<p style="top:274.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and the features</span></p>
<p style="top:274.3pt;left:113.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt"></span></p>
<p style="top:274.4pt;left:118.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:275.3pt;left:122.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:282.1pt;left:125.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l8&#xc;</span></p>
<p style="top:275.3pt;left:136.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x13;&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> are arbitrary functions of the word-</span></p>
<p style="top:286.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">history pair. Given a training corpus, the ML estimate can</span></p>
<p style="top:298.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">be shown to satisfy the constraints:</span></p>
<p style="top:332.9pt;left:59.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">=&#x84;&#x83;&#x86;&#x85;</span></p>
<p style="top:323.6pt;left:75.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;</span></p>
<p style="top:330.4pt;left:87.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:323.6pt;left:93.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xd;%</span></p>
<p style="top:332.9pt;left:104.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">=5&#x87;</span></p>
<p style="top:323.6pt;left:120.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;h</span></p>
<p style="top:330.4pt;left:140.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:330.4pt;left:142.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:323.6pt;left:148.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;8%</span></p>
<p style="top:322.6pt;left:159.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">$?</span></p>
<p style="top:323.6pt;left:167.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:330.4pt;left:171.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l8&#xc;</span></p>
<p style="top:323.6pt;left:181.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x81;&#xfffd;&#x88;&#x12;&#x8a;&#x89;L&#x8b;</span></p>
<p style="top:333.3pt;left:217.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">a</span></p>
<p style="top:322.6pt;left:227.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">$?</span></p>
<p style="top:323.6pt;left:235.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:330.4pt;left:239.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l0&#xc;</span></p>
<p style="top:323.6pt;left:249.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">h&#x81;&#xfffd;</span></p>
<p style="top:322.5pt;left:269.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(13)</span></p>
<p style="top:356.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">where</span></p>
<p style="top:366.5pt;left:79.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x85;</span></p>
<p style="top:357.4pt;left:77.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:356.3pt;left:86.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is the empirical distribution of the training corpus.</span></p>
<p style="top:368.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The ML estimate can also be shown to coincide with</span></p>
<p style="top:380.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the Maximum Entropy (ME) distribution [45], namely the</span></p>
<p style="top:392.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">one with highest entropy among all distributions satisfying</span></p>
<p style="top:404.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">equation 13. This unique ML/ME solution can be found by</span></p>
<p style="top:416.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">an iterative procedure [46, 47].</span></p>
<p style="top:428.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The ME paradigm, and the more general MDI frame-</span></p>
<p style="top:440.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">work, were &#xfb01;rst suggested for language modeling by [48],</span></p>
<p style="top:452.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and have since seen considerable success (e.g. [49, 50, 8]).</span></p>
<p style="top:464.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Its strength lies in principly incorporating arbitrary knowl-</span></p>
<p style="top:476.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">edge sources while avoiding fragmentation. For example,</span></p>
<p style="top:488.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in [8], conventional ngrams, distance-2 ngrams, and long</span></p>
<p style="top:500.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">distance word pairs (&#x201c;triggers&#x201d;) were encoded as features,</span></p>
<p style="top:512.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and resulted in up to 39% perplexity reduction and up to</span></p>
<p style="top:524.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">14% speech recognition word error rate reduction over the</span></p>
<p style="top:536.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">trigram baseline.</span></p>
<p style="top:548.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">While ME modeling is elegant and general, it is not</span></p>
<p style="top:560.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">without its weaknesses. Training a ME model is compu-</span></p>
<p style="top:572.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tationally challenging, and sometimes altogether infeasible.</span></p>
<p style="top:584.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Using a ME model is also CPU intensive, because of the</span></p>
<p style="top:596.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">need for explicit normalization. Unnormalized ME model-</span></p>
<p style="top:608.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing is attempted in [51]. ME smoothing is analyzed in [52].</span></p>
<p style="top:620.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The relative success of ME modeling focused attention</span></p>
<p style="top:632.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">on the remaining problem of feature induction, namely, se-</span></p>
<p style="top:644.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lection of useful features to be included in the model. An</span></p>
<p style="top:656.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">automatic iterative procedure for selecting features from a</span></p>
<p style="top:668.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">given candidate set is described in [47]. An interactive pro-</span></p>
<p style="top:680.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cedure for eliciting candidate sets is described in [53].</span></p>
<p style="top:692.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ME language modeling remains the subject of intensive</span></p>
<p style="top:704.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">research; see for example [54, 55, 56, 57, 58].</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">3.5. Adaptive models</span></b></p>
<p style="top:92.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">So far we have treated language as a homogeneous source.</span></p>
<p style="top:104.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">But in fact natural language is highly heterogeneous, with</span></p>
<p style="top:116.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">varying topics, genres and styles.</span></p>
<p style="top:128.6pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> cross-domain adaptation</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt">, test data comes from a</span></p>
<p style="top:140.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">source to which the language model has not been exposed</span></p>
<p style="top:152.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">during training. The only useful adaptation information is</span></p>
<p style="top:164.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in the current document itself. A common and quite effec-</span></p>
<p style="top:176.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tive technique for exploiting this information is the cache:</span></p>
<p style="top:188.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the (continuously developing) history is used to create, at</span></p>
<p style="top:200.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">runtime, a dynamic</span></p>
<p style="top:203.8pt;left:392.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram</span></p>
<p style="top:201.4pt;left:425.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;8&#x8c;&#x19;&#x8d;5&#x8c;&#xfffd;&#x8e;p&#x8f;&#xfffd;&#xfffd;h</span></p>
<p style="top:208.2pt;left:463.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:208.2pt;left:465.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:201.4pt;left:471.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> , which in turn is</span></p>
<p style="top:212.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">interpolated with the static model:</span></p>
<p style="top:247.3pt;left:317.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:247.2pt;left:323.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x8d;5&#x90;&#xfffd;&#x8d;5&#x91;p&#x92;k&#x93;</span></p>
<p style="top:247.2pt;left:345.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x94;&#x19;&#x8f;</span></p>
<p style="top:247.3pt;left:354.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h</span></p>
<p style="top:254.1pt;left:365.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:254.1pt;left:367.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:247.3pt;left:373.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xd;&#x12;</span></p>
<p style="top:256.6pt;left:390.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">~</span></p>
<p style="top:247.3pt;left:396.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;8&#x95;</span></p>
<p style="top:247.2pt;left:405.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x92;&apos;&#x8d;5&#x92;k&#x93;</span></p>
<p style="top:247.2pt;left:418.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x8c;</span></p>
<p style="top:247.3pt;left:423.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;h</span></p>
<p style="top:254.1pt;left:434.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:254.1pt;left:436.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:247.3pt;left:442.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;#&#x96; &#xfffd;</span></p>
<p style="top:254.0pt;left:462.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">;</span></p>
<p style="top:247.3pt;left:469.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">M</span></p>
<p style="top:256.6pt;left:479.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">~</span></p>
<p style="top:247.3pt;left:485.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#x19;&#xfffd;</span></p>
<p style="top:247.2pt;left:495.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x8c;&#x19;&#x8d;5&#x8c;&#xfffd;&#x8e;p&#x8f;</span></p>
<p style="top:247.3pt;left:515.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;h</span></p>
<p style="top:254.1pt;left:526.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:254.1pt;left:528.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:247.3pt;left:534.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span></p>
<p style="top:258.2pt;left:529.2pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(14)</span></p>
<p style="top:270.2pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">with the weight</span></p>
<p style="top:280.6pt;left:392.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">~</span></p>
<p style="top:270.2pt;left:401.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">optimized on held-out data. Cache</span></p>
<p style="top:281.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">LMs were &#xfb01;rst introduced by [59] and [60]. [61, 62] report</span></p>
<p style="top:293.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">reduction in perplexity, and [63] also reports reduction in</span></p>
<p style="top:305.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">recognition error rate. [64] introduced yet another adapta-</span></p>
<p style="top:317.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion scheme.</span></p>
<p style="top:329.9pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> within-domain adaptation</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt">, test data comes from the</span></p>
<p style="top:341.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">same source as the training data, but the latter is hetero-</span></p>
<p style="top:353.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">geneous, consisting of many subsets with varying topics,</span></p>
<p style="top:365.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">styles, or both. Adaptation then proceeds in the following</span></p>
<p style="top:377.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">steps:</span></p>
<p style="top:397.6pt;left:321.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1. Clustering the training corpus along the dimension of</span></p>
<p style="top:409.6pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">variability, say, topic (e.g. [65]).</span></p>
<p style="top:429.5pt;left:321.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">2. At runtime, identifying the topic or set of topics ([66,</span></p>
<p style="top:441.5pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">67]) of the test data.</span></p>
<p style="top:461.4pt;left:321.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">3. Locating appropriate subsets of the training corpus,</span></p>
<p style="top:473.4pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and using them to build a speci&#xfb01;c model.</span></p>
<p style="top:493.4pt;left:321.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">4. Combining the speci&#xfb01;c model with a corpus-wide model</span></p>
<p style="top:505.4pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(in statistical terminology, shrinking the speci&#xfb01;c model</span></p>
<p style="top:517.4pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">towards the general one, to trade off the former&#x2019;s vari-</span></p>
<p style="top:529.4pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ance against the latter&#x2019;s bias). This is usually done</span></p>
<p style="top:541.1pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">via linear interpolation, at either the word probability</span></p>
<p style="top:553.1pt;left:333.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">level or the sentence probability level [65].</span></p>
<p style="top:573.0pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A special (and very common) case is when one has only</span></p>
<p style="top:585.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">small amounts of data in the target domain and large amounts</span></p>
<p style="top:597.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in other domains. In this case, the only relevant step is the</span></p>
<p style="top:609.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">last one: combining models from the two domains. The</span></p>
<p style="top:621.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">outcome here is often disappointing, though: training data</span></p>
<p style="top:632.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">outside the domain has surprisingly little bene&#xfb01;t. For ex-</span></p>
<p style="top:644.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ample, when modeling the Switchboard domain (conversa-</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tional speech, [68]), the 40 million words of the WSJ corpus</span></p>
<p style="top:668.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(newspaper articles, [69]) and even the 140 million words</span></p>
<p style="top:680.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of the BN corpus (broadcast news transcriptions, [70]) im-</span></p>
<p style="top:692.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">prove by only a few percentage points the application per-</span></p>
<p style="top:704.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">formance of the in-domain model trained on a paltry 2.5</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">million words. Although this is a signi&#xfb01;cant improvement</span></p>
<p style="top:85.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">on such a dif&#xfb01;cult corpus, it is nonetheless disappointing</span></p>
<p style="top:97.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">considering the amount of data involved. By some esti-</span></p>
<p style="top:109.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mates [71], another 1 million words of Switchboard data</span></p>
<p style="top:121.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">would help the model more than 30 million words of out-</span></p>
<p style="top:133.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of-domain data. This suggest that our adaptation techniques</span></p>
<p style="top:145.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">are too crude.</span></p>
<p style="top:180.4pt;left:73.4pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">4. PROMISING CURRENT DIRECTIONS</span></b></p>
<p style="top:205.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">This section discusses current research directions that, in</span></p>
<p style="top:217.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">this author&#x2019;s subjective opinion, show signi&#xfb01;cant promise.</span></p>
<p style="top:250.5pt;left:49.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">4.1. Dependency models</span></b></p>
<p style="top:271.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Dependency grammars (DG) describe sentences in terms of</span></p>
<p style="top:283.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">asymmetric pairwise relationships among words. With a</span></p>
<p style="top:295.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">single exception, each word in the sentence is</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> dependent</span></i></p>
<p style="top:307.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">upon one other word, called its</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> head</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> or</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> parent</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">. The single</span></p>
<p style="top:319.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">exception is the</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> root</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, which serves as the head of the en-</span></p>
<p style="top:331.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tire sentence. For more about DGs, see [72]. Probabilistic</span></p>
<p style="top:343.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">DGs have also been developed, together with algorithms for</span></p>
<p style="top:355.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">learning them from corpora (e.g. [73]).</span></p>
<p style="top:368.6pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Probabilistic dependency grammars are particularly suited</span></p>
<p style="top:380.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to</span></p>
<p style="top:384.1pt;left:62.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram style modeling, where each word is predicted</span></p>
<p style="top:392.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">based on a small number of other words. The main dif-</span></p>
<p style="top:404.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ference is that in a conventional</span></p>
<p style="top:408.1pt;left:187.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram, the structure of</span></p>
<p style="top:416.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the model is predetermined: each word is predicted from</span></p>
<p style="top:428.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a few words that immediately preceded it. In DG, which</span></p>
<p style="top:440.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">words serve as predictors depends on the dependency graph,</span></p>
<p style="top:452.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">which is a hidden variable. A typical implementation will</span></p>
<p style="top:464.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">parse a sentence</span></p>
<p style="top:465.4pt;left:119.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> to generate the most likely dependency</span></p>
<p style="top:476.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">graphs</span></p>
<p style="top:484.2pt;left:80.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:476.5pt;left:88.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> (with attendant probabilities</span></p>
<p style="top:477.4pt;left:215.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&amp;&#xfffd;</span></p>
<p style="top:484.2pt;left:227.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:476.5pt;left:234.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:477.4pt;left:238.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ), compute</span></p>
<p style="top:488.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">for each of them a generation probability</span></p>
<p style="top:489.2pt;left:213.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&amp;&#xfffd;&apos;&#xfffd;</span></p>
<p style="top:496.0pt;left:230.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:496.0pt;left:232.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:488.2pt;left:240.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:489.2pt;left:243.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> (either</span></p>
<p style="top:491.6pt;left:277.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -</span></p>
<p style="top:500.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">gram style or perhaps as an ME model), and &#xfb01;nally estimate</span></p>
<p style="top:512.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the complete sentence probability as</span></p>
<p style="top:513.2pt;left:202.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;&#xfffd;&#xfffd;q&#x99;&#x98;</span></p>
<p style="top:513.7pt;left:249.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:513.2pt;left:254.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;</span></p>
<p style="top:520.0pt;left:266.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:512.2pt;left:274.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:513.2pt;left:277.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;n%</span></p>
<p style="top:525.2pt;left:50.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&apos;&#xfffd;</span></p>
<p style="top:532.0pt;left:67.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:532.0pt;left:69.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:524.2pt;left:77.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:525.2pt;left:80.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> (this is only approximate because the</span></p>
<p style="top:525.2pt;left:234.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&amp;&#xfffd;</span></p>
<p style="top:532.0pt;left:246.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:524.2pt;left:254.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:525.2pt;left:257.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> them-</span></p>
<p style="top:536.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">selves were derived from the sentence</span></p>
<p style="top:537.2pt;left:207.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> .) Sometime</span></p>
<p style="top:537.2pt;left:266.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&apos;&#xfffd;$&#xfffd;</span></p>
<p style="top:547.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is further approximated as</span></p>
<p style="top:548.9pt;left:156.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&apos;&#xfffd;</span></p>
<p style="top:555.7pt;left:173.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:555.7pt;left:176.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:549.4pt;left:184.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x11;</span></p>
<p style="top:548.9pt;left:188.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> , where</span></p>
<p style="top:555.7pt;left:224.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x97;</span></p>
<p style="top:549.4pt;left:232.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x11;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> is the single</span></p>
<p style="top:559.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">best scoring parse.</span></p>
<p style="top:573.0pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">An example of such a model is [74], which uses the</span></p>
<p style="top:585.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">parser of [75] to generate the candidate parses, and trains the</span></p>
<p style="top:597.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">parameters using maximum entropy. The probabilistic link</span></p>
<p style="top:609.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">grammar [44] mentioned in section 3.3 also falls roughly</span></p>
<p style="top:621.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in this category. Most recently, [76] employed a parser with</span></p>
<p style="top:632.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">probabilistic parameterization of a pushdown automata, and</span></p>
<p style="top:644.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">used an EM-type algorithm for training, with encouraging</span></p>
<p style="top:656.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">results (1% recognition word error rate reduction on the no-</span></p>
<p style="top:668.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">toriously dif&#xfb01;cult Switchboard corpus). In all, this method</span></p>
<p style="top:680.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of combining hidden linguistic structure with chain-rule pa-</span></p>
<p style="top:692.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">rameterization can yield a linguistically grounded yet com-</span></p>
<p style="top:704.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">putationally tractable model.</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">4.2. Dimensionality reduction</span></b></p>
<p style="top:93.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">One of the reasons language is so hard to model statistically</span></p>
<p style="top:105.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is that it is ostensibly categorical, with an extremely large</span></p>
<p style="top:117.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">number of categories, or dimensions. A prime example is</span></p>
<p style="top:129.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the vocabulary. To most language models, the vocabulary is</span></p>
<p style="top:141.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">but a very large set of unrelated entries. BANK is no closer</span></p>
<p style="top:153.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to LOAN or to BANKS than it is to, say, BRAZIL. This</span></p>
<p style="top:165.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">results in a large number of parameters. Yet our linguistic</span></p>
<p style="top:177.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">intuition is that there is a great deal of structure in the rela-</span></p>
<p style="top:189.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tionship among words. We feel that the &#x201c;true&#x201d; dimension of</span></p>
<p style="top:201.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the vocabulary is actually quite lower.</span></p>
<p style="top:213.5pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Similarly, for other phenomena in language, the under-</span></p>
<p style="top:225.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lying space may be of moderate or even low dimensional-</span></p>
<p style="top:237.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ity. Consider topic adaptation. As the topic changes, the</span></p>
<p style="top:249.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">probabilities of almost all words in the vocabulary change.</span></p>
<p style="top:261.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Since no two documents are exactly about the same thing, a</span></p>
<p style="top:273.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">straightforward approach would require an inordinate num-</span></p>
<p style="top:285.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ber of parameters. Yet the underlying topic space can be</span></p>
<p style="top:297.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">reasonably modeled in much fewer dimensions.</span></p>
<p style="top:309.5pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">This is the motivation behind [77], which uses the tech-</span></p>
<p style="top:321.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nique of Latent Semantic Analysis ([78]) to simultaneously</span></p>
<p style="top:333.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">reduce the dimensionality of the vocabulary and that of the</span></p>
<p style="top:345.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">topic space. First, the occurrence of each vocabulary word</span></p>
<p style="top:357.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in each document is tabulated. This very large matrix is</span></p>
<p style="top:369.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">then reduced via Singular Value Decomposition to a much</span></p>
<p style="top:381.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lower dimension (typically 100&#x2013;150). The new, smaller ma-</span></p>
<p style="top:393.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">trix captures the most salient correlations between speci&#xfb01;c</span></p>
<p style="top:405.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">combinations of words on one hand and clusters of docu-</span></p>
<p style="top:417.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ments on the other. The decomposition also yields matrices</span></p>
<p style="top:429.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">that project from document-space and word-space into the</span></p>
<p style="top:441.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">new, combined space. Consequently, any new document</span></p>
<p style="top:453.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">can be projected into the combined space, effectively be-</span></p>
<p style="top:465.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing classi&#xfb01;ed as a combination of the fundamental under-</span></p>
<p style="top:476.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lying topics, and adapted to accordingly. In [77], this type</span></p>
<p style="top:488.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of adaptation is combined with an</span></p>
<p style="top:492.3pt;left:452.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> -gram, and a perplex-</span></p>
<p style="top:500.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ity reduction of 30% over a trigram baseline is reported. In</span></p>
<p style="top:512.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[79], the technique is further developed and is found to also</span></p>
<p style="top:524.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">reduce recognition errors by 16% over a trigram baseline.</span></p>
<p style="top:553.6pt;left:309.1pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">4.3. Whole sentence models</span></b></p>
<p style="top:573.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">All language models described so far use the chain rule</span></p>
<p style="top:585.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to decompose the probability of a sentence into a product</span></p>
<p style="top:597.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of conditional probabilities of the type Pr</span></p>
<p style="top:598.1pt;left:482.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;h</span></p>
<p style="top:604.9pt;left:493.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xa;</span></p>
<p style="top:604.9pt;left:496.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">l</span></p>
<p style="top:598.1pt;left:501.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> . Histori-</span></p>
<p style="top:609.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cally, this has been done to facilitate estimation by relative</span></p>
<p style="top:621.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">counts. The decomposition is ostensibly harmless: after all,</span></p>
<p style="top:632.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">it is not an approximation but an exact equality. However,</span></p>
<p style="top:644.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">as a result, language modeling by and large has been re-</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">duced to modeling the distribution of a single word. This</span></p>
<p style="top:668.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in turn may be a signi&#xfb01;cant hindrance to modeling linguis-</span></p>
<p style="top:680.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tic structure: some linguistic phenomena are impossible or</span></p>
<p style="top:692.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">at best awkward to think about, let alone encode, in a con-</span></p>
<p style="top:704.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ditional framework. These include sentence-level features</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">such as person and number agreement, semantic coherence,</span></p>
<p style="top:85.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">parsability, and even length. Furthermore, external in&#xfb02;u-</span></p>
<p style="top:97.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ences on the sentence (e.g. previous sentences, topic) must</span></p>
<p style="top:109.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">be factored into the prediction of every word, which can</span></p>
<p style="top:121.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cause small biases to compound.</span></p>
<p style="top:133.8pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">To address these issues, [53] proposed a whole sentence</span></p>
<p style="top:145.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">exponential model:</span></p>
<p style="top:172.1pt;left:89.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;&#xfffd;&#xd;&#x12;&#x9a;;</span></p>
<img style="position:absolute;transform:matrix(10.240001,0,-0,-.64000007,169.568,235.16797)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:178.9pt;left:123.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">|</span></p>
<p style="top:172.1pt;left:135.3pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">%p&#xfffd;</span></p>
<p style="top:172.2pt;left:146.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">w</span></p>
<p style="top:172.1pt;left:151.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;^%</span></p>
<p style="top:174.5pt;left:169.9pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">W</span></p>
<p style="top:172.1pt;left:174.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#x1d;</span></p>
<p style="top:174.5pt;left:179.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">V&#x9b;}</span></p>
<p style="top:181.5pt;left:193.6pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">=&#x9c;?</span></p>
<p style="top:181.4pt;left:209.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">~</span></p>
<p style="top:171.2pt;left:215.2pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:171.1pt;left:218.8pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt"></span></p>
<p style="top:171.2pt;left:223.4pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">?</span></p>
<p style="top:172.1pt;left:227.5pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&apos;&#xfffd;$&#xfffd;&#xd;&#x82;</span></p>
<p style="top:171.0pt;left:269.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">(15)</span></p>
<p style="top:200.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Compared with the conditional exponential model of equa-</span></p>
<p style="top:212.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion 12,</span></p>
<p style="top:213.7pt;left:83.0pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">|</span></p>
<p style="top:212.6pt;left:92.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is now a true constant, which eliminates the seri-</span></p>
<p style="top:224.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ous burden of normalization. Most importantly, the features</span></p>
<p style="top:236.7pt;left:50.1pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">$?</span></p>
<p style="top:237.7pt;left:58.7pt;line-height:0.1pt"><span style="font-family:A,sans-serif;font-size:0.1pt">&#xfffd;&#xfffd;&#xfffd;</span><span style="font-family:Times New Roman,serif;font-size:10.0pt"> can capture arbitrary properties of the entire sentence.</span></p>
<p style="top:248.3pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Training this model requires sampling from an exponen-</span></p>
<p style="top:260.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tial distribution, a non-trivial task. The use of Monte Carlo</span></p>
<p style="top:272.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Markov Chain and other sampling methods for language</span></p>
<p style="top:284.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is studied in [80]. Sampling ef&#xfb01;ciency is crucial. Conse-</span></p>
<p style="top:296.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">quently, the bottleneck in this model is not the number of</span></p>
<p style="top:308.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">features or amount of data, but rather how rare the features</span></p>
<p style="top:320.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">are, and how accurately they need to be modeled. Inter-</span></p>
<p style="top:332.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">estingly, it has been shown [81] that most of the bene&#xfb01;t is</span></p>
<p style="top:344.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">likely to come from the more common features.</span></p>
<p style="top:356.1pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Parse-based features have been tried in [81], and seman-</span></p>
<p style="top:368.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tic features are discussed in [53]. An interactive methodol-</span></p>
<p style="top:379.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ogy for feature induction was also proposed in [53]. This</span></p>
<p style="top:391.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">methodology leads to a formulation of the training problem</span></p>
<p style="top:403.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">as logistic regression, with signi&#xfb01;cant practical bene&#xfb01;ts over</span></p>
<p style="top:415.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ML training.</span></p>
<p style="top:443.2pt;left:126.7pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">5. CHALLENGES</span></b></p>
<p style="top:465.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Perhaps the most frustrating aspect of statistical language</span></p>
<p style="top:477.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">modeling is the contrast between our intuition as speakers of</span></p>
<p style="top:489.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">natural language and the over-simplistic nature of our most</span></p>
<p style="top:501.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">successful models.</span></p>
<p style="top:513.3pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">As native speakers, we feel strongly that language has</span></p>
<p style="top:525.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">a deep structure. Yet we are not sure how to articulate that</span></p>
<p style="top:537.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">structure, let alone encode it, in a probabilistic framework.</span></p>
<p style="top:549.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Established linguistic theories have been of surprisingly lit-</span></p>
<p style="top:561.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tle help here, probably because their goal is to draw a line</span></p>
<p style="top:573.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">between what is properly in the language and what isn&#x2019;t,</span></p>
<p style="top:585.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">whereas SLM&#x2019;s goals are quite different.</span></p>
<p style="top:597.0pt;left:64.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">As an example, consider the problem of clustering the</span></p>
<p style="top:609.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">vocabulary words which was discussed in section 3.1. As</span></p>
<p style="top:621.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mentioned there, several automatic iterative methods have</span></p>
<p style="top:632.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">been proposed (e.g. [33, 34]). Table 1 lists example word</span></p>
<p style="top:644.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">classes derived by such a method [82]. While most words&#x2019;</span></p>
<p style="top:656.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">placement appear satisfactory, a few of the words seem out</span></p>
<p style="top:668.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of place.</span></p>
<p style="top:668.8pt;left:93.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Not surprisingly, these are often words whose</span></p>
<p style="top:680.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">count in the corpus was insuf&#xfb01;cient for reliable assignment.</span></p>
<p style="top:692.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Ironically, it is exactly these words which stood to bene&#xfb01;t</span></p>
<p style="top:704.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the most from clustering. In general, the more reliably a</span></p>
<p style="top:80.8pt;left:358.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Table 1: Data driven word classes.</span></p>
<img style="position:absolute;transform:matrix(322.24003,0,-0,-.64000007,573.24807,119.96796)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,412.12803,128.28801)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:92.2pt;left:315.1pt;line-height:8.0pt"><tt><span style="font-family:Courier,monospace;font-size:8.0pt">COMMITTEE COMMISSION PANEL SUBCOMMITTEE WONK</span></tt></p>
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,734.04806,128.28801)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,412.12803,144.28801)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:104.2pt;left:315.1pt;line-height:8.0pt"><tt><span style="font-family:Courier,monospace;font-size:8.0pt">THEMSELVES MYSELF YOURSELF UNBECOMING ...</span></tt></p>
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,734.04806,144.28801)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,412.12803,160.28801)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:116.2pt;left:315.1pt;line-height:8.0pt"><tt><span style="font-family:Courier,monospace;font-size:8.0pt">ATTORNEY SURGEON RUKEYSER CONSUL RICKEY ...</span></tt></p>
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,734.04806,160.28801)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,412.12803,175.96794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:128.0pt;left:315.1pt;line-height:8.0pt"><tt><span style="font-family:Courier,monospace;font-size:8.0pt">ACTION ACTIVITY INTERVENTION ATTACHE WARFARE ...</span></tt></p>
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,734.04806,175.96794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,412.12803,191.96794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:140.0pt;left:315.1pt;line-height:8.0pt"><tt><span style="font-family:Courier,monospace;font-size:8.0pt">CENTER ASSOCIATION FACETED INSTITUTE GUILD ...</span></tt></p>
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,734.04806,191.96794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,412.12803,207.96794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:152.0pt;left:315.1pt;line-height:8.0pt"><tt><span style="font-family:Courier,monospace;font-size:8.0pt">PARTICULAR YEAR&#x2019;S NIGHT&#x2019;S MORNING&#x2019;S FATEFUL ...</span></tt></p>
<img style="position:absolute;transform:matrix(.64000007,0,-0,-16,734.04806,207.96794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<img style="position:absolute;transform:matrix(322.24003,0,-0,-.64000007,573.24807,216.28794)" src="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAAAAAA6fptVAAAACXBIWXMAAA7EAAAO
xAGVKw4bAAAACklEQVR4nGP4DwABAQEAsTj2FAAAAABJRU5ErkJggg==">
<p style="top:194.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">word can be assigned to a class, the less it will bene&#xfb01;t from</span></p>
<p style="top:206.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">that assignment. How then is vocabulary clustering to be-</span></p>
<p style="top:218.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">come effective?</span></p>
<p style="top:235.4pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">I believe that the solution to this problem, and others</span></p>
<p style="top:247.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">like it, is to inject human knowledge of language into the</span></p>
<p style="top:259.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">process. This can take the following forms:</span></p>
<p style="top:276.2pt;left:324.0pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">Interactive modeling.</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Data-driven optimization and hu-</span></p>
<p style="top:288.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">man knowledge and decision making can play complemen-</span></p>
<p style="top:300.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tary roles in an intertwined iterative process. For the vocab-</span></p>
<p style="top:311.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ulary clustering problem, this means that a human is put in</span></p>
<p style="top:323.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the loop, to arbitrate some borderline decisions and override</span></p>
<p style="top:335.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">others. For example, a human can decide that &#x2019;TUESDAY&#x2019;</span></p>
<p style="top:347.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">belongs in the same cluster as &#x2019;MONDAY&#x2019;, &#x2019;WEDNES-</span></p>
<p style="top:359.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">DAY&#x2019;, &#x2019;THURSDAY&#x2019; and &#x2019;FRIDAY&#x2019;, even if it did not oc-</span></p>
<p style="top:371.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cur enough times to be placed there automatically, and even</span></p>
<p style="top:383.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">if it did not occur at all. Another example of this approach</span></p>
<p style="top:395.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">is the interactive feature induction methodology described</span></p>
<p style="top:407.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in [53].</span></p>
<p style="top:424.7pt;left:324.0pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">Encoding knowledge as priors.</span></b><span style="font-family:Times New Roman,serif;font-size:10.0pt"> One of the perils of</span></p>
<p style="top:436.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">using human knowledge is that it is often overstated, and</span></p>
<p style="top:448.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sometimes wrong. Thus a better solution might be to encode</span></p>
<p style="top:460.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">such knowledge as a prior in a Bayesian updating scheme.</span></p>
<p style="top:472.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">After training, whatever phenomena are not suf&#xfb01;ciently rep-</span></p>
<p style="top:484.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">resented in the training corpus will continue to be captured</span></p>
<p style="top:496.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">thanks to the prior. Whenever enough data exist, however,</span></p>
<p style="top:508.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">they will override the prior. For the vocabulary clustering</span></p>
<p style="top:520.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">problem, experts&#x2019; beliefs about the relationships between</span></p>
<p style="top:532.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">vocabulary entries must be suitably encoded, and the clus-</span></p>
<p style="top:544.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tering paradigm must be changed to optimize an appropri-</span></p>
<p style="top:556.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ate posterior measure. Thus, in the example above, enough</span></p>
<p style="top:568.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">data may exist to separate out &#x2019;FRIDAY&#x2019; because of its use</span></p>
<p style="top:580.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">in phrases like &#x201c;Thank God It&#x2019;s Friday&#x201d;.</span></p>
<p style="top:597.0pt;left:324.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Encoding linguistic knowledge as a prior is an exciting</span></p>
<p style="top:609.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">challenge which has yet to be seriously attempted. This</span></p>
<p style="top:621.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">will likely include de&#xfb01;ning a distance metric over words</span></p>
<p style="top:632.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and phrases, and a stochastic version of structured word on-</span></p>
<p style="top:644.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tologies like WordNet [83]. At the syntactic level, it could</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">include Bayesian versions of manually created lexicalized</span></p>
<p style="top:668.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">grammars. In practice, the Bayesian framework and the in-</span></p>
<p style="top:680.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">teractive process may be combined, taking advantage of the</span></p>
<p style="top:692.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">superior theoretical foundation of the former and the com-</span></p>
<p style="top:704.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">putational advantages of the latter.</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:70.6pt;left:49.7pt;line-height:14.4pt"><b><span style="font-family:Times New Roman,serif;font-size:14.4pt">Acknowledgements</span></b></p>
<p style="top:97.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">I am grateful to Stanly Chen, Sanjeev Khudanpur, John Laf-</span></p>
<p style="top:109.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ferty and Bob Moore for helpful comments.</span></p>
<p style="top:140.8pt;left:127.9pt;line-height:10.0pt"><b><span style="font-family:Times New Roman,serif;font-size:10.0pt">6. REFERENCES</span></b></p>
<p style="top:164.3pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[1] Peter F. Brown, John Cocke, Stephen A. Della Pietra,</span></p>
<p style="top:176.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-</span></p>
<p style="top:188.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ferty, Robert L. Mercer, and Paul S. Roossin. A statis-</span></p>
<p style="top:200.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tical approach to machine translation.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computational</span></i></p>
<p style="top:212.1pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 16(2):79&#x2013;85, June 1990.</span></p>
<p style="top:234.6pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[2] Ralf brown and Robert &amp; Frederking. Applying sta-</span></p>
<p style="top:246.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tistical English language modeling to symbolic ma-</span></p>
<p style="top:258.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">chine translation. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the 6th Interna-</span></i></p>
<p style="top:270.4pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">tional Conference on Theoretical and Methodological</span></i></p>
<p style="top:282.4pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Issues in Machine Translation (TMI&#x2019;95)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 221&#x2013;</span></p>
<p style="top:294.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">239, July 1995.</span></p>
<p style="top:317.0pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[3] J. Ponte and W. Bruce. Croft. A language modeling</span></p>
<p style="top:329.0pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">approach to information retrieval. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of</span></i></p>
<p style="top:340.7pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">the 21st international conference on research and de-</span></i></p>
<p style="top:352.7pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">velopment in information retrieval (SIGIR&#x2019;98)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages</span></p>
<p style="top:364.7pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">275&#x2013;281, 1998.</span></p>
<p style="top:387.3pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[4] Adam Berger and John Lafferty. Information retrieval</span></p>
<p style="top:399.3pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">as statistical translation. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the 22nd</span></i></p>
<p style="top:411.3pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">annual conference on research and development in in-</span></i></p>
<p style="top:423.0pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">formation retrieval (SIGIR&#x2019;99)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 222&#x2013;229, 1999.</span></p>
<p style="top:445.6pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[5] Fred Jelinek. The 1995 language modeling summer</span></p>
<p style="top:457.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">workshop at Johns Hopkins University. Closing re-</span></p>
<p style="top:469.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">marks.</span></p>
<p style="top:492.2pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[6] Lalit R. Bahl, Jim K. Baker, Frederick Jelinek, and</span></p>
<p style="top:504.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Robert L. Mercer. Perplexity - a measure of the dif-</span></p>
<p style="top:516.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">&#xfb01;culty of speech recognition tasks.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Program of the</span></i></p>
<p style="top:527.9pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">94th Meeting of the Acoustical Society of America J.</span></i></p>
<p style="top:539.9pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Acoust. Soc. Am.</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 62:S63, 1977. Suppl. no. 1.</span></p>
<p style="top:562.5pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[7] Stanley F. Chen, Douglas Beeferman, and Ronald</span></p>
<p style="top:574.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Rosenfeld. Evaluation metrics for language models.</span></p>
<p style="top:586.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the DARPA Broadcast News Tran-</span></i></p>
<p style="top:598.5pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">scription and Understanding Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 275&#x2013;</span></p>
<p style="top:610.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">280, 1998.</span></p>
<p style="top:632.8pt;left:54.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[8] Ronald Rosenfeld.</span></p>
<p style="top:632.8pt;left:157.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A maximum entropy approach</span></p>
<p style="top:644.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">to adaptive statistical language modeling.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computer</span></i></p>
<p style="top:656.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Speech and Language</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 10:187&#x2013;228, 1996.</span></p>
<p style="top:656.8pt;left:260.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">longer</span></p>
<p style="top:668.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">version published as &#x201c;Adaptive Statistical Language</span></p>
<p style="top:680.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Modeling: A Maximum Entropy Approach,&#x201d; Ph.D.</span></p>
<p style="top:692.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">thesis, Computer Science Department, Carnegie Mel-</span></p>
<p style="top:704.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">lon University, TR CMU-CS-94-138, April 1994.</span></p>
<p style="top:74.1pt;left:313.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[9] C.E. Shannon.</span></p>
<p style="top:74.1pt;left:400.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A mathematical theory of commu-</span></p>
<p style="top:85.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nication.</span></p>
<p style="top:85.8pt;left:375.4pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Bell Systems Technical Journal</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 27:379&#x2013;</span></p>
<p style="top:97.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">423,623&#x2013;656, 1948.</span></p>
<p style="top:117.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[10] C.E. Shannon. Prediction and entropy of printed En-</span></p>
<p style="top:129.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">glish.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Bell Systems Technical Journal</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 30:50&#x2013;64, Jan-</span></p>
<p style="top:141.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">uary 1951.</span></p>
<p style="top:160.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[11] T.M. Cover and R.C. King. A convergent gambling</span></p>
<p style="top:172.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">estimate of the entropy of English.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Transactions</span></i></p>
<p style="top:184.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">on Information Theory</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 24(4):413&#x2013;421, 1978.</span></p>
<p style="top:203.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[12] E. Brill, R. Florian, C. Henderson, and L. Mangu. Be-</span></p>
<p style="top:215.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">yond n-grams: Can linguistic sophistication improve</span></p>
<p style="top:227.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">language modeling? In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the 36th An-</span></i></p>
<p style="top:239.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">nual Meeting of the ACL</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 1998.</span></p>
<p style="top:258.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[13] Frederick Jelinek.</span></p>
<p style="top:258.6pt;left:416.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Statistical Methods for Speech</span></i></p>
<p style="top:270.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Recognition</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">. MIT Press, Cambridge, Massachusetts,</span></p>
<p style="top:282.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1997.</span></p>
<p style="top:301.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[14] I.J. Good. The population frequencies of species and</span></p>
<p style="top:313.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the estimation of population parameters.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Biometrika</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:325.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">40(3 and 4):237&#x2013;264, 1953.</span></p>
<p style="top:345.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[15] Ian H. Witten and Timothy C. Bell.</span></p>
<p style="top:345.0pt;left:504.0pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The zero-</span></p>
<p style="top:357.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">frequency problem: Estimating the probabilities of</span></p>
<p style="top:369.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">novel events in adaptive text compression.</span></p>
<p style="top:369.0pt;left:524.2pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">IEEE</span></i></p>
<p style="top:381.0pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Transactions on Information Theory</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 37(4):1085&#x2013;</span></p>
<p style="top:393.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1094, July 1991.</span></p>
<p style="top:412.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[16] Slava M. Katz. Estimation of probabilities from sparse</span></p>
<p style="top:424.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">data for the language model component of a speech</span></p>
<p style="top:436.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">recognizer.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Transactions on Acoustics, Speech</span></i></p>
<p style="top:448.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">and Signal Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 35(3):400&#x2013;401, March 1987.</span></p>
<p style="top:467.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[17] Hermann Ney, Ute Essen, and Reinhard Kneser. On</span></p>
<p style="top:479.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">structuring probabilistic dependences in stochastic</span></p>
<p style="top:491.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">language modeling.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computer Speech and Language</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:503.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">8:1&#x2013;38, 1994.</span></p>
<p style="top:522.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[18] Reinhard Kneser and Hermann Ney.</span></p>
<p style="top:522.6pt;left:507.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Improved</span></p>
<p style="top:534.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">backing-off for m-gram language modeling. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pro-</span></i></p>
<p style="top:546.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ceedings of the IEEE International Conference on</span></i></p>
<p style="top:558.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Acoustics, Speech and Signal Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, volume I,</span></p>
<p style="top:570.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">pages 181&#x2013;184, Detroit, Michigan, May 1995.</span></p>
<p style="top:589.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[19] Frederick Jelinek and Robert L. Mercer. Interpolated</span></p>
<p style="top:601.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">estimation of Markov source parameters from sparse</span></p>
<p style="top:613.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">data.</span></p>
<p style="top:613.6pt;left:360.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the Workshop on Pattern</span></i></p>
<p style="top:625.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Recognition in Practice</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 381&#x2013;397, Amsterdam,</span></p>
<p style="top:637.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The Netherlands: North-Holland, May 1980.</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[20] D. Ron, Y. Singer, and N. Tishby. The power of amne-</span></p>
<p style="top:668.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sia. In J. Cowan, G. Tesauro, and J. Alspector, editors,</span></p>
<p style="top:680.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Advances in Neural Information Processing Systems</span></i></p>
<p style="top:692.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">6</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 176&#x2013;183. Morgam Kaufmann, San Mateo,</span></p>
<p style="top:704.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">CA, 1994.</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[21] I. Guyon and F. Pereira. Design of a linguistic post-</span></p>
<p style="top:85.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">processor using variable memory length Markov mod-</span></p>
<p style="top:97.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">els. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the 3rd ICDAR</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 454&#x2013;457,</span></p>
<p style="top:109.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1995.</span></p>
<p style="top:131.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[22] Reinhard Kneser.</span></p>
<p style="top:131.2pt;left:149.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Statistical language modeling us-</span></p>
<p style="top:143.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing a variable context length. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of IC-</span></i></p>
<p style="top:155.2pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">SLP</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, volume 1, pages 494&#x2013;497, Philadelphia, October</span></p>
<p style="top:167.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1996.</span></p>
<p style="top:188.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[23] Thomas Niesler and Philip Woodland. Variable-length</span></p>
<p style="top:200.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">category n-gram language models.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computer Speech</span></i></p>
<p style="top:212.6pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">and Language</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 21:1&#x2013;26, 1999.</span></p>
<p style="top:233.9pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[24] Man-Hung Siu and Mari Ostendorf. Variable n-gram</span></p>
<p style="top:245.9pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and extensions for conversational speech language</span></p>
<p style="top:257.9pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">modeling.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Transactions on Speech and Audio</span></i></p>
<p style="top:269.9pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 8(1):63&#x2013;75, 2000.</span></p>
<p style="top:291.3pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[25] Pierre Dupont and Ronald Rosenfeld. Lattice based</span></p>
<p style="top:303.3pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">language models. Technical Report CMU-CS-97-173,</span></p>
<p style="top:315.3pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Carnegie Mellon University, Department of Computer</span></p>
<p style="top:327.3pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Science, September 1997.</span></p>
<p style="top:348.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[26] Stanley F. Chen and Joshua Goodman. An empirical</span></p>
<p style="top:360.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">study of smoothing techniques for language modeling.</span></p>
<p style="top:372.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the 34th Annual Meeting of the ACL</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:384.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">pages 310&#x2013;318, Santa Cruz, California, June 1996.</span></p>
<p style="top:406.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[27] Ronald Rosenfeld.</span></p>
<p style="top:406.0pt;left:159.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The CMU statistical language</span></p>
<p style="top:418.0pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">modeling toolkit and its use in the 1994 ARPA CSR</span></p>
<p style="top:430.0pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">evaluation. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the Spoken Language</span></i></p>
<p style="top:441.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Systems Technology Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 47&#x2013;50, Austin,</span></p>
<p style="top:453.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Texas, January 1995.</span></p>
<p style="top:475.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[28] Philip Clarkson and Ronald Rosenfeld. Statistical lan-</span></p>
<p style="top:487.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage modeling using the CMU-Cambridge toolkit. In</span></p>
<p style="top:499.1pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Proceedings of the European Conference on Speech</span></i></p>
<p style="top:511.1pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Communication and Technology (Eurospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 1997.</span></p>
<p style="top:532.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[29] Andreas Stolcke. SRILM&#x2014;the SRI language model-</span></p>
<p style="top:544.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing toolkit. http://www.speech.sri.com/projects/srilm/,</span></p>
<p style="top:556.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1999.</span></p>
<p style="top:578.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[30] Stanley F. Chen. Language model tools (v0.1) user&#x2019;s</span></p>
<p style="top:590.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guide. http://www.cs.cmu.edu/ sfc/manuals/h015c.ps,</span></p>
<p style="top:601.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">December 1998.</span></p>
<p style="top:623.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[31] Patti J. Price. Evaluation of spoken language systems:</span></p>
<p style="top:635.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the atis domain. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the DARPA Speech</span></i></p>
<p style="top:647.2pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">and Natural Language Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, June 1990.</span></p>
<p style="top:668.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[32] Wayne H. Ward. The cmu air travel information ser-</span></p>
<p style="top:680.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">vice: understanding spontaneous speech.</span></p>
<p style="top:680.8pt;left:255.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pro-</span></i></p>
<p style="top:692.6pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ceedings of the DARPA Speech and Natural Language</span></i></p>
<p style="top:704.6pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 127&#x2013;129, June 1990.</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[33] Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-</span></p>
<p style="top:85.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ouza, Jennifer C. Lai, and Robert L. Mercer. Class-</span></p>
<p style="top:97.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">based n-gram models of natural language.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computa-</span></i></p>
<p style="top:109.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">tional Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 18(4):467&#x2013;479, December 1992.</span></p>
<p style="top:129.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[34] Reinhard Kneser and Hermann Ney. Improved clus-</span></p>
<p style="top:141.3pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tering techniques for class-based statistical language</span></p>
<p style="top:153.3pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">modeling. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the European Confer-</span></i></p>
<p style="top:165.3pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ence on Speech Communication and Technology (Eu-</span></i></p>
<p style="top:177.3pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">rospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 1993.</span></p>
<p style="top:197.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[35] Leo Breiman, Jerome H. Friedman, Richard A. Ol-</span></p>
<p style="top:208.7pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">shen, and Charles J. Stone.</span></p>
<p style="top:208.7pt;left:453.8pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Classi&#xfb01;cation and Re-</span></i></p>
<p style="top:220.7pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">gression Trees</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">. Wadsworth &amp; Brooks/Cole Advanced</span></p>
<p style="top:232.7pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Books &amp; Software, Monterey, California, 1984.</span></p>
<p style="top:252.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[36] Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and</span></p>
<p style="top:264.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Robert L. Mercer.</span></p>
<p style="top:264.2pt;left:412.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A tree-based statistical language</span></p>
<p style="top:276.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">model for natural language speech recognition.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE</span></i></p>
<p style="top:288.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Transactions on Acoustics, Speech and Signal Pro-</span></i></p>
<p style="top:300.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">cessing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 37:1001&#x2013;1008, July 1989.</span></p>
<p style="top:319.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[37] Arthur N&#xb4;adas, David Nahamoo, Michael A. Picheny,</span></p>
<p style="top:331.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and Jeffrey Powell. An iterative &#x201c;&#xfb02;ip-&#xfb02;op&#x201d; approxima-</span></p>
<p style="top:343.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion of the most informative split in the construction</span></p>
<p style="top:355.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of decision trees. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the IEEE Inter-</span></i></p>
<p style="top:367.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">national Conference on Acoustics, Speech and Signal</span></i></p>
<p style="top:379.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, Toronto, Canada, May 1991.</span></p>
<p style="top:399.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[38] Peter F. Brown, Steven A. Della Pietra, Vincent J.</span></p>
<p style="top:411.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Della Pietra, Robert L. Mercer, and Philip S. Resnik.</span></p>
<p style="top:423.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Language modeling using decision trees. research re-</span></p>
<p style="top:435.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">port, I.B.M. Research, Yorktown Heights, NY, 1991.</span></p>
<p style="top:454.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[39] M. Marcus, B. Santorini, and M. Marcinkiewicz.</span></p>
<p style="top:466.5pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Building a large annotated corpus of English: the Penn</span></p>
<p style="top:478.5pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Treeback.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computational Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 19(2), 1993.</span></p>
<p style="top:498.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[40] James K. Baker.</span></p>
<p style="top:498.2pt;left:413.5pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Trainable grammars for speech</span></p>
<p style="top:509.9pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">recognition. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the Spring Conference</span></i></p>
<p style="top:521.9pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">of the Acoustical Society of America</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 547&#x2013;550,</span></p>
<p style="top:533.9pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Boston, MA, June 1979.</span></p>
<p style="top:553.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[41] Frederick Jelinek, John D. Lafferty, and Robert L.</span></p>
<p style="top:565.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Mercer.</span></p>
<p style="top:565.6pt;left:375.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Basic methods of probabilistic context-</span></p>
<p style="top:577.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">free grammars.</span></p>
<p style="top:577.4pt;left:401.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In P. Laface and R. De Mori, edi-</span></p>
<p style="top:589.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tors,</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Speech Recognition and Understanding: Recent</span></i></p>
<p style="top:601.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Advances, Trends, and Applications</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, volume 75 of</span></p>
<p style="top:613.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">F: Computer and Systems Sciences</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 345&#x2013;360.</span></p>
<p style="top:625.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Springer Verlag, 1992.</span></p>
<p style="top:644.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[42] R. Moore, D. Appelt, J. Dowding, J. M. Gawron, and</span></p>
<p style="top:656.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">D. Moran. Combining linguistic and statistical knowl-</span></p>
<p style="top:668.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">edge sources in natural-language processing for atis.</span></p>
<p style="top:680.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Spoken Language Systems Technology Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:692.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">pages 261&#x2013;264, Austin, Texas, February 1995. Mor-</span></p>
<p style="top:704.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">gan Kaufmann Publishers, Inc.</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[43] Danny Sleator and Davy Temperley. Parsing English</span></p>
<p style="top:85.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">with a link grammar. Technical Report CMU-CS-91-</span></p>
<p style="top:97.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">196, Computer Science Department, Carnegie Mellon</span></p>
<p style="top:109.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">University, Pittsburgh, PA, October 1991.</span></p>
<p style="top:132.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[44] John D. Lafferty, Danny Sleator, and Davy Temper-</span></p>
<p style="top:144.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ley. Grammatical trigrams: a probabilistic model of</span></p>
<p style="top:156.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">link grammar. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the AAAI Fall Sym-</span></i></p>
<p style="top:168.2pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">posium on Probabilistic Approaches to Natural lan-</span></i></p>
<p style="top:180.2pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, Cambridge, MA, October 1992.</span></p>
<p style="top:202.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[45] E.T. Jaynes.</span></p>
<p style="top:202.5pt;left:129.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Information theory and statistical me-</span></p>
<p style="top:214.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">chanics.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Physics Reviews</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 106:620&#x2013;630, 1957.</span></p>
<p style="top:236.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[46] J.N. Darroch and D. Ratcliff.</span></p>
<p style="top:236.8pt;left:201.8pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Generalized iterative</span></p>
<p style="top:248.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">scaling for log-linear models.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> The Annals of Mathe-</span></i></p>
<p style="top:260.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">matical Statistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 43:1470&#x2013;1480, 1972.</span></p>
<p style="top:283.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[47] S. Della Pietra, V. Della Pietra, and J. Lafferty. In-</span></p>
<p style="top:295.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ducing features of random &#xfb01;elds.</span></p>
<p style="top:295.1pt;left:224.4pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">IEEE Transac-</span></i></p>
<p style="top:307.1pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">tions on Pattern Analysis and Machine Intelligence</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:319.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">19(4):380&#x2013;393, April 1997.</span></p>
<p style="top:341.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[48] S. Della Pietra, V. Della Pietra, R.L. Mercer, and</span></p>
<p style="top:353.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">S. Roukos. Adaptive language modeling using min-</span></p>
<p style="top:365.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">imum discriminant estimation.</span></p>
<p style="top:365.4pt;left:210.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of</span></i></p>
<p style="top:377.4pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">the Speech and Natural Language DARPA Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:389.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">February 1992.</span></p>
<p style="top:411.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[49] Raymond Lau, Ronald Rosenfeld, and Salim Roukos.</span></p>
<p style="top:423.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Trigger-based language models: A maximum entropy</span></p>
<p style="top:435.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">approach. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of ICASSP-93</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages II&#x2013;45</span></p>
<p style="top:447.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">&#x2013; II&#x2013;48, April 1993.</span></p>
<p style="top:470.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[50] Adam Berger, Stephen Della Pietra, and Vincent</span></p>
<p style="top:481.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Della Pietra. A maximum entropy approach to nat-</span></p>
<p style="top:493.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ural language processing.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Computational Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:505.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">22(1):39&#x2013;71, 1996.</span></p>
<p style="top:528.4pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[51] Stanley F. Chen, Kristie Seymore, and Ronald Rosen-</span></p>
<p style="top:540.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">feld.</span></p>
<p style="top:540.2pt;left:100.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Topic adaptation for language modeling us-</span></p>
<p style="top:552.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ing unnormalized exponential models. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ICASSP-98</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:564.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Seattle, Washington, 1998.</span></p>
<p style="top:586.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[52] Stan F. Chen and Ronald Rosenfeld.</span></p>
<p style="top:586.5pt;left:235.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A survey of</span></p>
<p style="top:598.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">smoothing techniques for me models.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Trans-</span></i></p>
<p style="top:610.5pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">actions on Speech and Audio Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 8(1):37&#x2013;50,</span></p>
<p style="top:622.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">2000.</span></p>
<p style="top:644.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[53] Ronald Rosenfeld, Larry Wasserman, Can Cai, and</span></p>
<p style="top:656.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Xiaojin Zhu. Interactive feature induction and logis-</span></p>
<p style="top:668.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tic regression for whole sentence exponential language</span></p>
<p style="top:680.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">models. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the IEEE Workshop on Au-</span></i></p>
<p style="top:692.6pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">tomatic Speech Recognition and Understanding</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, Key-</span></p>
<p style="top:704.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">stone, CO, December 1999.</span></p>
<p style="top:74.1pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[54] Doug Beeferman, Adam Berger, and John Lafferty. A</span></p>
<p style="top:85.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">model of lexical attraction and repulsion. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceed-</span></i></p>
<p style="top:97.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ings of the 35th Annual Meeting of the Association for</span></i></p>
<p style="top:109.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Computational Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 373&#x2013;380, Madrid,</span></p>
<p style="top:121.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Spain, 1997.</span></p>
<p style="top:141.3pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[55] John D. Lafferty and Bernard Suhm. Cluster expan-</span></p>
<p style="top:153.3pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sions and iterative scaling for maximum entropy lan-</span></p>
<p style="top:165.3pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage models. In K. Hanson and R. Silver, editors,</span></p>
<p style="top:177.3pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Maximum Entropy and Bayesian Methods</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 195&#x2013;</span></p>
<p style="top:189.3pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">202. Kluwer Academic Publishers, 1995.</span></p>
<p style="top:208.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[56] Jochen Peters and Dietrich Klakow. Compact maxi-</span></p>
<p style="top:220.7pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mum entropy language models. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the</span></i></p>
<p style="top:232.7pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">IEEE Workshop on Automatic Speech Recognition and</span></i></p>
<p style="top:244.7pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Understanding</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, Keystone, CO, December 1999.</span></p>
<p style="top:264.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[57] Sanjeev Khudanpur and Jun Wu.</span></p>
<p style="top:264.2pt;left:477.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A maximum en-</span></p>
<p style="top:276.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tropy language model integrating n-grams and topic</span></p>
<p style="top:288.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">dependencies for conversational speech recognition.</span></p>
<p style="top:300.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the IEEE International Conference</span></i></p>
<p style="top:312.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">on Acoustics, Speech and Signal Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, Phoenix,</span></p>
<p style="top:323.9pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">AZ, 1999.</span></p>
<p style="top:343.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[58] Jun Wu and Sanjeev Khudanpur. Combining nonlocal,</span></p>
<p style="top:355.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">syntactic and n-gram dependencies in language mod-</span></p>
<p style="top:367.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">eling. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the European Conference on</span></i></p>
<p style="top:379.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Speech Communication and Technology (Eurospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:391.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Budapest, Hungary, 1999.</span></p>
<p style="top:411.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[59] Roland Kuhn. Speech recognition and the frequency</span></p>
<p style="top:423.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">of recently used words: A modi&#xfb01;ed markov model for</span></p>
<p style="top:435.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">natural language. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> 12th International Conference on</span></i></p>
<p style="top:446.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Computational Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 348&#x2013;350, Budapest,</span></p>
<p style="top:458.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">August 1988.</span></p>
<p style="top:478.5pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[60] Julian Kupiec. Probabilistic models of short and long</span></p>
<p style="top:490.5pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">distance word dependencies in running text. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pro-</span></i></p>
<p style="top:502.5pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ceedings of the DARPA Workshop on Speech and Nat-</span></i></p>
<p style="top:514.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ural Language</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 290&#x2013;295, February 1989.</span></p>
<p style="top:533.9pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[61] Roland Kuhn and Renato De Mori. A cache-based nat-</span></p>
<p style="top:545.9pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ural language model for speech reproduction.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE</span></i></p>
<p style="top:557.9pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Transactions on Pattern Analysis and Machine Intelli-</span></i></p>
<p style="top:569.7pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">gence</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, PAMI-12(6):570&#x2013;583, 1990.</span></p>
<p style="top:589.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[62] Roland Kuhn and Renato De Mori.</span></p>
<p style="top:589.4pt;left:488.4pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Correction to:</span></p>
<p style="top:601.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">A cache-based natural language model for speech re-</span></p>
<p style="top:613.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">production.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Transactions on Pattern Analysis</span></i></p>
<p style="top:625.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">and Machine Intelligence</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, PAMI-14(6):691&#x2013;692, June</span></p>
<p style="top:637.1pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1992.</span></p>
<p style="top:656.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[63] Fred Jelinek, Salim Roukos Bernard Merialdo, and</span></p>
<p style="top:668.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">M. Strauss. A dynamic language model for speech</span></p>
<p style="top:680.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">recognition. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the DARPA Workshop</span></i></p>
<p style="top:692.6pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">on Speech and Natural Language</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 293&#x2013;295,</span></p>
<p style="top:704.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">February 1991.</span></p>
</div>
<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:74.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[64] Reinhard Kneser and Volker Steinbiss.</span></p>
<p style="top:74.1pt;left:240.5pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">On the dy-</span></p>
<p style="top:85.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">namic adaptation of stochastic language models. In</span></p>
<p style="top:97.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Proceedings of the IEEE conference on acoustics,</span></i></p>
<p style="top:109.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">speech and signal processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 586&#x2013;589, Min-</span></p>
<p style="top:121.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">neapolis, MN, 1993. volume II.</span></p>
<p style="top:141.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[65] Rukmini Iyer and Mari Ostendorf. Modeling long dis-</span></p>
<p style="top:153.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tance dependence in language: Topic mixture vs. dy-</span></p>
<p style="top:165.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">namic cache models.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Transactions on Speech</span></i></p>
<p style="top:177.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">and Audio Processing IEEE-SAP</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 7:30&#x2013;39, 1999.</span></p>
<p style="top:197.7pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[66] Kristie Seymore and Ronald Rosenfeld. Using story</span></p>
<p style="top:209.7pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">topics for language model adaptation. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings</span></i></p>
<p style="top:221.7pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">of the European Conference on Speech Communica-</span></i></p>
<p style="top:233.7pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion and Technology (Eurospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 1997.</span></p>
<p style="top:253.6pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[67] Kristie Seymore, Stanley Chen, and Ronald Rosen-</span></p>
<p style="top:265.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">feld. Nonlinear interpolation of topic models for lan-</span></p>
<p style="top:277.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">guage model adaptation. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of ICSLP-98</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:289.4pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1998.</span></p>
<p style="top:309.5pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[68] J.J. Godfrey,</span></p>
<p style="top:309.5pt;left:134.9pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">E.C. Holliman,</span></p>
<p style="top:309.5pt;left:208.6pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">and J. McDaniel.</span></p>
<p style="top:321.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">SWITCHBOARD: Telephone speech corpus for re-</span></p>
<p style="top:333.5pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">search and development. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the IEEE</span></i></p>
<p style="top:345.3pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">International Conference on Acoustics, Speech and</span></i></p>
<p style="top:357.3pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Signal Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, volume I, pages 517&#x2013;520, March</span></p>
<p style="top:369.3pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1992.</span></p>
<p style="top:389.2pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[69] Douglas B. Paul and Janet M. Baker. The design for</span></p>
<p style="top:401.2pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">the Wall Street Journal-based CSR corpus. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Pro-</span></i></p>
<p style="top:413.2pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ceedings of the DARPA Speech and Natural Language</span></i></p>
<p style="top:425.2pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Workshop</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 357&#x2013;362, February 1992.</span></p>
<p style="top:445.1pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[70] David Graff.</span></p>
<p style="top:445.1pt;left:131.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">The 1996 broadcast news speech and</span></p>
<p style="top:457.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">language model corpus. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the DARPA</span></i></p>
<p style="top:469.1pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Workshop on Spoken Language technology</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 11&#x2013;</span></p>
<p style="top:481.1pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">14, 1997.</span></p>
<p style="top:501.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[71] Ronald Rosenfeld, Rajeev Agarwal, Bill Byrne, Ruk-</span></p>
<p style="top:513.0pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mini Iyer, Mark Liberman, Elizabeth Shriberg, Jack</span></p>
<p style="top:525.0pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Unverferth, Dimitra Vergyri, and Enrique Vidal. Error</span></p>
<p style="top:537.0pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">analysis and dis&#xfb02;uency modeling in the switchbboard</span></p>
<p style="top:548.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">domain. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the International Confer-</span></i></p>
<p style="top:560.8pt;left:71.3pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">ence on Speech and Language Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 1996.</span></p>
<p style="top:581.0pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[72] http://ufal.mff.cuni.cz/dg-bib2.html.</span></p>
<p style="top:600.9pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[73] Glenn Carrol and Eugene Charniak. Two experiments</span></p>
<p style="top:612.9pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">on learning probabilistic dependency grammars from</span></p>
<p style="top:624.9pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">corpora. Technical Report TR 92-16, Computer Sci-</span></p>
<p style="top:636.9pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">ence Department, Brown University, 1992.</span></p>
<p style="top:656.8pt;left:49.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[74] Ciprian Chelba, David Engle, frederick Jelinek, Vic-</span></p>
<p style="top:668.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tor Jimenaz, Sanjeev Khudanpur, Lidia Mangu, Harry</span></p>
<p style="top:680.8pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-</span></p>
<p style="top:692.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cke, and Dekai Wu. Structure and performance of a</span></p>
<p style="top:704.6pt;left:71.3pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">dependency language model. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the</span></i></p>
<p style="top:74.1pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">European Conference on Speech Communication and</span></i></p>
<p style="top:85.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Technology (Eurospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 2775&#x2013;2778, 1997.</span></p>
<p style="top:97.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">volume 5.</span></p>
<p style="top:117.8pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[75] Michael Collins. A new statistical parser based on bi-</span></p>
<p style="top:129.8pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">gram lexical dependencies. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the 34th</span></i></p>
<p style="top:141.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">annual meeting of the association for Computational</span></i></p>
<p style="top:153.8pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Linguistics</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 184&#x2013;191, May 1996.</span></p>
<p style="top:173.7pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[76] Ciprian Chelba and Fred Jelinek. Recognition perfor-</span></p>
<p style="top:185.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">mance of a structured language model. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings</span></i></p>
<p style="top:197.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">of the European Conference on Speech Communica-</span></i></p>
<p style="top:209.4pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion and Technology (Eurospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, pages 1567&#x2013;1570,</span></p>
<p style="top:221.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1999. volume 4.</span></p>
<p style="top:241.4pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[77] Jerome R. Bellegarda. A multi-span language mod-</span></p>
<p style="top:253.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">eling framework for large vocabulary speech recogni-</span></p>
<p style="top:265.4pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tion.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> IEEE Transactions on Speech and Audio Pro-</span></i></p>
<p style="top:277.1pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">cessing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 6:456&#x2013;467, 1998.</span></p>
<p style="top:297.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[78] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-</span></p>
<p style="top:309.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">dauer, and R. Harshman. Indexing by latent semantic</span></p>
<p style="top:321.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">analysis.</span></p>
<p style="top:321.0pt;left:373.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">J. Am. Soc. Inform. Science</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, 41:391&#x2013;407,</span></p>
<p style="top:333.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">1990.</span></p>
<p style="top:353.0pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[79] Jerome R. Bellegarda. Large vocabulary speech recog-</span></p>
<p style="top:365.0pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">nition with multi-span statistical language models.</span></p>
<p style="top:376.7pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">IEEE Transactions on Speech and Audio Processing</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:388.7pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">8(1):76&#x2013;84, 2000.</span></p>
<p style="top:408.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[80] Stanley F. Chen and Ronald Rosenfeld.</span></p>
<p style="top:408.6pt;left:511.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Ef&#xfb01;cient</span></p>
<p style="top:420.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">sampling and feature selection in whole sentence</span></p>
<p style="top:432.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">maximum entropy language models. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> ICASSP-99</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">,</span></p>
<p style="top:444.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Phoenix, Arizona, 1999.</span></p>
<p style="top:464.6pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[81] Xiaojin Zhu, Stanley F. Chen, and Ronald Rosenfeld.</span></p>
<p style="top:476.6pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">Linguistic features for whole sentence maximum en-</span></p>
<p style="top:488.3pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">tropy language models. In</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> Proceedings of the Euro-</span></i></p>
<p style="top:500.3pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">pean Conference on Speech Communication and Tech-</span></i></p>
<p style="top:512.3pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">nology (Eurospeech)</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">, Budapest, Hungary, 1999.</span></p>
<p style="top:532.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[82] Stanley F. Chen. Unpublished work. 1998.</span></p>
<p style="top:552.2pt;left:309.1pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">[83] Christiane FellBaum, editor.</span><i><span style="font-family:Times New Roman,serif;font-size:10.0pt"> WordNet: An Electronic</span></i></p>
<p style="top:564.2pt;left:330.7pt;line-height:10.0pt"><i><span style="font-family:Times New Roman,serif;font-size:10.0pt">Lexical Database</span></i><span style="font-family:Times New Roman,serif;font-size:10.0pt">. Language, Speech and Communi-</span></p>
<p style="top:576.2pt;left:330.7pt;line-height:10.0pt"><span style="font-family:Times New Roman,serif;font-size:10.0pt">cation. MIT Press, 1998.</span></p>
</div>
</body></html>